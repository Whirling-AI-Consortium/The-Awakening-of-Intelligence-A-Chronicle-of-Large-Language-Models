---
sidebar_position: 2
---

## 从链条到网络的思维革命

如果说Chain-of-Thought是一条笔直的思维之路，那么Tree of Thoughts就是对这条路的根本性反思。2023年的春天，一个看似理所当然的问题在学术界引发了连锁反应：为什么我们要假定推理的过程必须是线性的？为什么模型不能在某个中间步骤尝试多种不同的路径，然后选择最有前景的那一条继续深入？

这个问题来自于对人类思维方式的观察。当我们面对一个复杂的、多步骤的问题时，我们的思考通常不是一条直线。我们会在某些关键节点上停下来，考虑多个可能的方向，评估每个方向的可行性，然后选择看起来最有希望的方向继续推进。有时候，我们会发现某条路走到了死胡同，然后回溯到之前的分叉点，尝试另一条路。这个过程是树状的、探索性的，而不是线性的。

由Shunyu Yao、Dian Yu等研究者在论文《Tree of Thoughts: Deliberate Problem Solving with Language Models》中提出的框架，正是试图将这种直观的、树状的推理过程形式化。ToT框架的核心思想是：将推理问题建模为一个树搜索问题，其中每个节点代表一个思考状态（thought state），每条边代表一个思考步骤（thought step）。模型不再是单纯地生成一条线性的推理链，而是探索这个思想树中的多条路径，通过某种搜索策略（如广度优先搜索、深度优先搜索或启发式搜索）找到最优的解决方案。

这个框架之所以激进，在于它对CoT的直接挑战。CoT告诉我们："让模型展示推理过程吧，这会很有帮助。"但ToT问的是："为什么只让它展示一个推理过程？为什么不让它尝试多个？"这个转变看似微小，实际上触及了如何最充分地利用大语言模型能力的根本问题。

## 从线性链到结构化树

为了理解ToT相对于CoT的优势，我们需要先理解两者的本质区别。考虑一个经典的推理任务：Game of 24，这是一个出现在许多推理数据集中的问题。给定四个数字，比如3、8、3和8，你需要通过加减乘除的操作将它们组合起来，得到24。这个问题看似简单，但其搜索空间是指数级的——每一步操作的选择都会分支出多种可能的下一步。

在Chain-of-Thought的框架下，模型会尝试生成一条完整的、线性的解决方案路径。但问题是，在这个高度组合的搜索空间中，线性地尝试每一步可能会导致模型陷入死胡同。模型可能会生成看起来合理的中间步骤，但最终无法到达正确答案。一旦陷入死胡同，CoT框架中没有机制让模型回溯到之前的分叉点重新尝试。

Tree of Thoughts通过完全不同的方式处理这个问题。它允许模型在每个中间思考状态上生成多个可能的下一步思考。这样就形成了一个树状的结构：

```
                    初始状态
                   /  |  \
                  /   |   \
            思考1  思考2  思考3
            /  \    /  \    / \
           /    \  /    \  /   \
        思考1.1 1.2 2.1 2.2 3.1 3.2
         / \     / \     / \
        ...    ...    ...
```

在这个树中，搜索算法的任务就是找到从根节点到某个目标节点的最优路径。最简单的方法是广度优先搜索（BFS）——在每一层同时探索所有候选节点，然后选择最有前景的节点进行下一层的扩展。更复杂的方法是使用启发式搜索，比如A*算法，通过评估每个节点的"启发值"（heuristic value）来决定优先探索哪些节点。

论文中讨论了三种主要的搜索策略，每一种都代表了不同的计算成本与性能的权衡。

**广度优先搜索（BFS）** 是最直观的选择。在每个思考层级，模型生成多个候选思考，然后评估它们的质量。最有前景的候选被保留下来进行下一层的展开。这种方法的优点是系统性很强，不容易错过最优解。但缺点也很明显：计算成本随着树的宽度指数增长。如果你在每层生成三个候选思考，树的深度是四，那么你需要评估大约3^4=81个节点。

**深度优先搜索（DFS）** 则沿着单一路径走到底部，只有在碰到死胡同时才回溯。这种方法的计算成本相对较低，但容易被局部最优迷惑，错过全局的最优解。

**启发式搜索** 试图在两者之间找到平衡。通过设计一个评估函数（evaluation function），模型可以估计某个中间思考状态距离正确答案有多远。基于这个启发值，搜索算法可以优先探索看起来最有前景的分支，从而减少需要探索的节点总数。


## 现实中的性能与成本

论文在多个任务上测试了ToT框架，结果展现了显著的改进。在Game of 24任务上，使用标准CoT的GPT-3.5只能达到4%的成功率——这意味着模型几乎无法解决这个问题。但使用BFS搜索的ToT框架，成功率直接跳升到74%。这是一个质的飞跃，从"基本无能"到"相当可靠"。

类似的改进也出现在其他任务上。在创意写作任务（需要生成创意但符合约束的故事）上，ToT的成功率达到了64%，而CoT只有10%。甚至在相对简单的任务上，ToT也通常比CoT有10%-20%的性能优势。

然而，这些性能的提升是有代价的。BFS搜索导致了显著的计算成本增加。Game of 24问题上，使用ToT需要更多的API调用——如果BFS在每层生成3个候选思考，深度为4，那么你需要进行大约1+3+9+27=40次模型调用，而CoT只需要1次。这意味着使用ToT的成本大约是CoT的40倍。

这个成本-收益的权衡引发了有趣的问题。什么时候值得支付这样的计算成本来换取更好的推理性能？论文的暗示是：当问题的复杂度足够高、线性推理容易陷入死胡同时，ToT的额外成本是合理的。但对于相对简单、线性推理就能解决的任务，CoT的简洁性更优。

ToT框架中一个关键但常被忽视的部分是评估函数（evaluator）的设计。这个函数需要评判一个中间思考状态有多接近最终答案，从而指导搜索算法的决策。

在论文中，研究者探索了多种评估方法。最简单的是让模型自己判断：给定一个中间状态，模型生成一个评分，表示这个状态"看起来有多正确"或"有多大可能导向正确答案"。但这种自评方法有个明显的问题：模型可能会系统性地高估某类思考，或者在某些情况下过度自信。

更可靠的方法是使用一个独立的验证模型或启发函数。比如在Game of 24中，可以实时检查某个中间状态是否合法（数字是否被正确使用、运算是否有效等）。在数学推理任务中，可以通过符号验证来检查中间步骤的正确性。这种硬启发（hard heuristics）往往比软启发（模型生成的评分）更可靠。

但设计好的启发函数本身就是一门艺术。对于某些任务，启发函数很容易设计（比如Game of 24有明确的规则）。但对于更开放的推理任务，设计启发值就变成了一个挑战。过于乐观的启发值会导致算法早早陷入局部最优；过于悲观的启发值则会导致算法探索不必要的分支，浪费计算资源。

## 从学术框架到实际应用的鸿沟

虽然ToT在论文中展现了令人印象深刻的成绩，但在实际应用中推广它的道路并不平坦。主要的困难包括几个方面。

首先是可伸缩性问题。BFS的计算成本随着问题复杂度指数增长。对于某些复杂的现实问题，即使是三层BFS也可能导致成本爆炸。而且，随着branching factor和树深度的增加，缓存管理变成了一个实际的工程问题。

其次是任务适配性。ToT框架对那些有明确中间步骤和可评估状态的问题工作很好（比如数学、逻辑谜题、代码生成）。但对于那些目标模糊、难以评估进度的开放式任务（比如创意写作、对话），ToT的优势就没那么明显了。

第三是评估函数的获取。好的启发函数通常需要领域知识或额外的标注工作。在没有这些的情况下，依赖模型自评的启发值往往不够可靠。

正因为这些原因，尽管ToT在学术上很优雅，但在工业应用中却没有得到广泛采用。更常见的是研究者使用ToT的思想——多路径探索、启发式评估——但以简化的、工程化的方式集成到系统中，而不是严格按照论文中的完整框架。

尽管如此，ToT的核心思想——搜索空间的显式建模和多路径探索——已经深刻影响了后续的研究。我们看到了多个变体的出现：

Self-consistency框架虽然早于ToT，但在概念上是互补的：生成多条推理路径，然后通过多数投票确定答案。这可以看作是ToT的一个简化版本，省去了评估和搜索的复杂性，代之以简单的并行采样。

Decomposition方法则将ToT的思想应用于任务分解：大问题被分解为多个子问题，子问题之间形成树状或图状的依赖关系。通过这种方式，模型可以系统地解决更复杂的问题。

当Reinforcement Learning技术与ToT相结合时，研究者们可以学习出最优的搜索策略，而不是依赖固定的BFS或DFS。这打开了将传统的搜索和规划算法与深度学习结合的可能性。

## 推理的未来方向

如果说Chain-of-Thought回答的是"思考如何帮助推理"，ReAct回答的是"行动如何辅助推理"，那么Tree of Thoughts则问的是"搜索如何指导推理"。这三个问题分别代表了推理模型发展的三个不同维度。

从某种意义上说，ToT代表了一个重要的认识：大语言模型的能力不仅取决于参数大小和训练数据，还取决于我们如何让这些模型进行推理。给予模型足够的"思考空间"——让它尝试多条路径、回溯、重新探索——可以显著扩展其问题解决能力。

但ToT也揭示了一个本质的权衡：更好的推理通常需要更多的计算资源。这不是一个可以轻松解决的问题。它暗示，在追求更强推理能力的道路上，我们最终可能需要在速度和准确性之间做出选择，或者投入更多的计算资源。在资源有限的场景下，如何在这个权衡中找到最优点，成为了一个持续的研究问题。