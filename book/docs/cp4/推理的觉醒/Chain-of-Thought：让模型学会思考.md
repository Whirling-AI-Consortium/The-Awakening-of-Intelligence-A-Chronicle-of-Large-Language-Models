---
sidebar_position: 0
---

## 那个改变游戏规则的"想想"

2022年的春天，谷歌大脑团队的研究者们正在研究一个令人沮丧的现象。他们观察到，即使是规模庞大的语言模型，在面对需要多步推理的数学应用题时，仍然经常给出完全错误的答案——而最令人困惑的是，模型生成的第一个token往往就已经错了。这不是一个细微的改进空间，而是一个根本性的障碍：模型似乎缺少某种"思考"的能力。

此前，研究者们的直觉是模型需要更多的参数、更多的数据、更强的架构。但是谷歌团队的Jason Wei、Xuezhi Wang等人却发现了一种不同的可能性。他们在一篇题为《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》的论文中记录了一个简单而优雅的发现：仅仅通过改变提示的方式，让模型在给出最终答案前先展示出推理的中间步骤，就能显著提升其在复杂推理任务上的性能。

更戏剧化的是，这个发现极其简洁：只需要在示例中加入一句"Let's think step by step"，或者在少样本演示中包含解题的过程，模型的性能就会产生质的飞跃。这看起来很像是某种"魔法"，但其背后却包含了对语言模型本质的深刻洞察。

## 从"一步跳跃"到"阶梯上升"

理解Chain-of-Thought（CoT）的重要性，首先需要明确传统的语言模型输出方式为什么会失败。在没有引入CoT之前，当你问一个模型"如果一个篮子里有8个苹果，Tom吃掉了3个，后来他又拿回了5个，现在篮子里有多少个苹果？"时，模型倾向于直接预测最后一个数字——或者说，它试图通过单次的、原子性的前向传播来完成整个推理链条。这个过程就像是要求某人在不写下任何中间步骤的情况下直接计算256×137的结果，虽然对于足够聪慧的大脑来说不是不可能，但成功的概率会大幅下降。

Chain-of-Thought的洞察正在于此：通过强制模型生成中间推理步骤，我们不仅让模型有机会修正可能的计算错误，更重要的是，我们改变了模型的内部计算动力学。当模型被要求逐步生成推理过程时，它实际上在执行一种"自我验证"的机制。每一个生成的中间步骤都提供了后续推理的约束条件，使得模型在构建最终答案时，不得不维持推理过程中的内部一致性。

谷歌论文中的实验数据展示了这种效应的幅度。在GSM8K数据集（包含8.5K个小学数学应用题）上，没有CoT的GPT-3仅能获得36%的准确率。但当使用8个包含推理步骤的Few-shot示例时，同一模型的准确率直接跃升到79%。这不是一个10%或20%的改进，而是一个翻倍的飞跃——它充分说明了这个看似微小的改变实际上触及了大模型推理能力的根本结构。

更令人惊喜的是，这个效应在不同的推理任务上是普遍适用的。不论是数学应用题、常识推理、逻辑谜题还是符号操作任务，CoT都展现出了一致的性能提升。这种跨任务的有效性暗示，我们可能发现的不仅仅是某个技巧的有效性，而是对语言模型推理机制的某种通用规律。

要理解CoT的工作原理，我们可以从一个简化的实现开始。其核心思想可以用以下伪代码表示：

```python
# 不带CoT的标准方法（性能较差）
prompt_direct = "Q: If a basket has 8 apples and Tom eats 3, then gets 5 back, how many are left?\nA:"
answer = model.generate(prompt_direct)
# 可能输出：错误的数字

# 带CoT的方法（性能改进）
prompt_cot = """Q: If a basket has 8 apples and Tom eats 3, then gets 5 back, how many are left?
Let's think step by step.
Starting apples: 8
After Tom eats 3: 8 - 3 = 5
After Tom gets 5 back: 5 + 5 = 10
A: 10"""

# Few-shot CoT示例
few_shot_cot = """
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. 
How many tennis balls does he have now?
A: Roger started with 5 tennis balls. 2 cans × 3 balls per can = 6 balls. 5 + 6 = 11 balls.

Q: If a basket has 8 apples and Tom eats 3, then gets 5 back, how many are left?
A: Let's think step by step.
Starting apples: 8
After Tom eats 3: 8 - 3 = 5
After Tom gets 5 back: 5 + 5 = 10
Final answer: 10
"""

answer = model.generate(few_shot_cot)
```

这段代码看起来简朴得几乎有些俗气，但它触及了一个至关重要的计算机科学原理：程序的中间状态表达。在传统的编译器设计中，我们知道一个程序可以通过不同的方式执行：可以是直接的、原子的操作，也可以是一系列可观察的、可验证的中间步骤。Chain-of-Thought本质上是在要求语言模型采用后者——它强制模型"显示工作过程"，就像数学老师要求学生写出计算步骤一样。

谷歌的论文进一步分析了为什么Zero-shot CoT（仅在问题后添加"Let's think step by step"而无需示例）也能产生效果。这个现象尤其令人费解，因为模型并未从任何示例中学习CoT的格式。答案在于，这种提示实际上激活了模型内部某种潜在的推理能力。模型在其训练数据中已经见过大量包含推理过程的文本（比如教科书、论坛讨论、代码注释等），这个简单的提示词语就足以"激活"这种模式。换句话说，CoT不是教会模型新的能力，而是唤醒它已有的但默认未被使用的能力。

## 从一个令人尴尬的问题到一个开放的研究方向

Chain-of-Thought的成功引发了一个整个领域都感到尴尬的问题：为什么模型在看到提示词变化后的行为会有如此巨大的差异？这不应该发生。或者说，这种发生暗示了我们对语言模型内部机制的理解远未完整。

随后的研究开始深入探讨CoT背后的机制。Wei等人在后续工作中发现，并非所有的中间步骤都等价。例如，生成的步骤顺序很重要——如果你打乱推理步骤的顺序，模型的性能就会下降。这表明模型确实在理解这些步骤之间的因果关系。另一方面，研究者们也发现了一个有趣的负面现象：即使CoT生成了错误的中间步骤，只要最终答案正确，模型有时仍然会坚持这个答案。这提示我们，CoT的价值并不单纯来自于逻辑的正确性，而是来自于这种"展示思考过程"的行为本身对模型输出分布的改变。

Automatic Chain-of-Thought（Auto-CoT）的出现进一步扩展了这个思想。研究者们不再需要手工制作CoT示例，而是可以让模型自动为每个问题生成相应的推理过程。这似乎是一个小的技术改进，但它意味着CoT从一个需要人工设计的技巧升级为一个可自动化的、可扩展的系统。

CoT的另一个重要变体是Self-Consistency，这个方法认为与其依赖单一的推理链，不如生成多条推理路径，然后通过多数投票来确定最终答案。这就像是要求一个学生用不同的方法解同一道题，然后比较这些方法是否都指向相同的答案。在复杂推理任务上，Self-Consistency往往能比单一CoT带来另外10%-20%的性能提升。

从历史的角度看，Chain-of-Thought这篇论文的意义远超其技术本身。它标志着一个时代的开始，这个时代的特征是：我们发现大语言模型的能力不仅取决于模型的大小和数据的质量，还在很大程度上取决于我们如何与它们交互。

这个发现是对传统深度学习范式的一个深刻的挑战。在计算机视觉或其他领域中，我们习惯于认为模型的性能由其架构和训练过程决定，而推理/提示方式几乎无关。但CoT证明了在语言模型领域，对话方式本身可以是改变游戏规则的变量。

这个洞察随后被扩展到了Prompt Engineering的整个领域。如果仅仅改变提示词就能产生如此大的性能差异，那么一定存在某种"最优"的提示方式。这个问题激发了一股研究热潮：如何系统地设计提示词？如何自动化提示词的优化？如何理解提示词为何有效？这些问题的答案最终汇聚成了现代大模型应用的核心技术——Prompt Engineering。

而对于大模型的开发者和应用者而言，CoT带来的启示更加实际：当你的模型在某个任务上表现不佳时，在花费时间和金钱来重新训练模型之前，值得先尝试改变提示方式。这个建议听起来平凡，但它代表了一种根本不同的思考方式。它将问题从"模型够不够聪明"转变为"我问得够不够清楚"。

时至今日，Chain-of-Thought已经成为了几乎所有复杂推理应用的标准做法。无论是医学诊断、法律分析还是科学论证，当我们需要模型提供可解释的、基于步骤的答案时，CoT都是一个默认的选择。而这个看似简单的方法论，已经成为了连接GPT-3时代的"黑盒智能"和现代可解释AI的桥梁。