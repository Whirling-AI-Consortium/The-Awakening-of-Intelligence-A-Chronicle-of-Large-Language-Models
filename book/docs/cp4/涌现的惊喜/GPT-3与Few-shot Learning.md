---
sidebar_position: 0
---

在2020年的春天，新冠疫情正在全球肆虐。在家隔离的研究者们通过Zoom进行协作，科学界的节奏被打乱，但OpenAI却在这个时刻宣布了一个突破性的成果。

2020年5月28日，OpenAI发表了《Language Models are Few-Shot Learners》，随之而来的是一个数字，足以让整个AI社区为之震动：175B。

一百七十五亿参数。这比GPT-2的1.5B大了足足116倍。在此之前，业界最大的公开模型是Google的T5-11B，也就是110亿参数。而GPT-3一举跃升到了一个前所未有的规模。

但这不仅仅是一个数字的突破。随着论文和代码示例的发布，人们意识到，规模不只是带来了性能的线性提升，而是触发了什么东西的质变。

## 1200万美元的训练成本

要训练GPT-3，OpenAI投入了令人难以置信的资源。论文中披露的数字是：训练需要3650 PetaFLOP/s-days的计算。翻译成人类能理解的语言，就是大约1200万美元的计算成本。

这个成本包括什么？首先是大规模的硬件。OpenAI需要数千块高端GPU或TPU同时运行，进行分布式训练。其次是电费。一个这样规模的训练过程会持续数周，耗电量以兆瓦时计算。第三是人力成本——开发、调试、监控这样一个庞大的训练过程需要一个专业团队。

1200万美元这个数字在2020年是天文数字。大多数AI研究者的年度研究经费也才几十万美元。这意味着，只有像OpenAI、Google、Meta这样资金充足的机构才能进行这样的尝试。

但从另一个角度看，1200万美元对于一个革命性的工具来说，也并不算太贵。比起药物开发（通常需要数十亿美元）或航天项目（需要数百亿美元），这是相对便宜的。而且，一旦GPT-3被训练出来，就可以被无数次地使用，摊平了平均成本。

## 45TB文本的语言宇宙

训练数据的规模同样惊人。GPT-3使用了45TB的文本数据。这来自于多个来源的混合：

Common Crawl的爬虫数据（占多数），WebText2（GPT-2使用的数据集的升级版），Books1和Books2（来自互联网上的大量电子书），以及Wikipedia和其他高质量的公开资源。

45TB是什么概念？如果一部平均长度的小说是1MB，那45TB就相当于4500万部小说。如果每部小说有10万个单词，那这是4.5万亿个词。即使去除重复和垃圾内容，也足以覆盖人类书面语言的大部分模式。

更重要的是，这个数据集的多样性。它包含新闻文章、学术论文、代码、诗歌、故事、对话、技术文档等等。这种多样性使得GPT-3能够学到语言在各种不同上下文中的使用方式。

## In-Context Learning的发现

但GPT-3最令人惊讶的能力，并不是它的规模或数据，而是一个现象：它能从很少的示例中学习。

在机器学习中，"学习"通常意味着更新模型的参数。一个模型通过反向传播、梯度下降等方式调整其内部权重。但GPT-3展现出的是一种不同的学习方式。给它几个示例，只改变输入，不改变任何参数，模型就能适应新的任务。

这个现象被称为In-Context Learning，或者说Few-Shot Learning（少样本学习）。

举个例子。假设我们想让模型学会一个新的任务：情感反转。给定一个正面的句子，生成一个表达相反情感的句子。用传统的方法，我们需要收集几千个这样的正面-负面句子对，然后对模型进行微调。但用GPT-3，我们只需要在Prompt中提供2-3个例子：

```
正面句子 -> 负面句子的转换：

积极的例子：
"这部电影太棒了！" -> "这部电影太糟了"
"我喜欢这个地方" -> "我讨厌这个地方"
"天气很好" -> "天气很差"

现在转换：
"我非常高兴" -> 
```

GPT-3能够从这几个例子中推断出规律，生成："我非常伤心" 或类似的反义表达。

这不是微调。模型的参数没有改变。发生的事情更微妙：模型学会了从Prompt中提取上下文（Context），理解所要求的任务，然后应用这个理解来生成答案。

## 零样本、一样本、少样本的光谱

GPT-3的Few-Shot能力可以沿着一个光谱分布：

**零样本（Zero-shot）**：不提供任何例子，只提供任务描述。

```
Q: 以下是一个积极的电影评论吗？
"这部电影很无聊"

A: 不是
```

**一样本（One-shot）**：提供一个例子。

```
正面评论的例子：
"这部电影太棒了！"

现在判断：
"这部电影很无聊" 是正面评论吗？

A: 不是
```

**少样本（Few-shot）**：提供多个例子，通常2-5个。

```
正面评论的例子：
1. "这部电影太棒了！"
2. "我喜欢这个故事情节"

负面评论的例子：
1. "这部电影很无聊"
2. "表演太差了"

现在判断：
"这部电影很长但很有趣" 是正面还是负面？

A: 正面
```

论文展示的结果令人瞩目。在许多任务上，Few-shot的GPT-3优于之前最佳的微调模型。有时候，零样本的GPT-3就已经能做得很好。这颠覆了深度学习的一个基本假设：学习需要大量的标注数据。

## 论文的核心主张

《Language Models are Few-Shot Learners》论文的标题本身就是一个大胆的声明。它声称，语言模型——通过其在海量文本上的预训练——本质上已经学会了如何学习。

论文提出的解释是：当一个模型在这么多的语言中看到这么多的模式时，它实际上已经学会了各种各样的"任务解决策略"。当面对新任务时，模型可以通过上下文识别出应该使用哪一种策略。

但这个解释仍然相当神秘。为什么一个单纯为了最小化下一个词的预测损失而训练的模型，会学会这种通用的问题解决能力？这个问题至今仍然是开放的。

有一个研究方向试图从梯度的角度理解In-Context Learning。论文《In-Context Learning and Induction Heads》提出，模型的某些注意力头可能在执行"内在的微调"——它们在执行过程中（In-Context）学习序列中的模式，这有点像是在推理时进行参数更新。但这仍然是一个初步的解释。

## 代码与实验的示例

让我们看看如何用API的形式与GPT-3交互：

```python
import openai

openai.api_key = "your-api-key"

# Few-shot prompt for arithmetic
prompt = """
Q: 两个质数的和是9，这两个数是什么？
A: 2和7

Q: 一个数的两倍减去5等于11，这个数是什么？
A: 8

Q: 如果一个矩形的长是宽的2倍，周长是30，那么宽是多少？
A: 
"""

response = openai.Completion.create(
    model="text-davinci-003",  # GPT-3 的高级版本
    prompt=prompt,
    temperature=0.7,
    max_tokens=50
)

print(response.choices[0].text)  # 可能输出: 5

# 演示 In-Context Learning 的演技检测
sentiment_prompt = """
这些电影评论的情感是积极(正)还是消极(负)？

评论: "这是我看过最好的电影！"
情感: 正

评论: "非常令人失望的经历"
情感: 负

评论: "一部杰作，绝对推荐！"
情感: 正

评论: "非常平庸，浪费时间"
情感: 负

评论: "我看了三遍，每次都很享受"
情感:
"""

response = openai.Completion.create(
    model="text-davinci-003",
    prompt=sentiment_prompt,
    temperature=0,
    max_tokens=10
)

print(response.choices[0].text)  # 输出: 正
```

这些代码示例展示了GPT-3如何通过简单的Prompt就能完成各种任务，而无需额外的训练。

## 学习的本质：内部发生了什么？

GPT-3的Few-Shot学习能力引发了一个深刻的问题：它到底是真的在"学习"吗？

传统的观点是，学习必须涉及参数的改变。但GPT-3的参数在Few-Shot学习时并没有改变。那么，什么被改变了？

一个可能的答案是：它改变的是计算过程。在一个标准的前向传播中，模型接收输入，通过多个层的计算，生成输出。但在Few-Shot设置中，模型实际上在进行更复杂的计算——它在"思考"如何处理新任务。

另一个角度是信息论的。在Prompt中提供的示例，包含了关于新任务的信息。模型能够提取这些信息，并将其应用于生成答案。从这个角度，In-Context Learning就是一种信息提取和应用的过程。

还有一个角度是从注意力机制的角度。注意力机制使得后面的token可以"查看"之前的token。在Few-Shot设置中，模型可能在使用注意力来检索相关的示例，并基于这些示例来生成答案。

## 与微调的对比

虽然Few-Shot非常强大，但它并不总是比微调更好。论文中显示了一个有趣的现象：

对于某些任务，提供更多的示例会帮助GPT-3提升性能。但这种改进有一个极限。一旦超过了这个极限（通常在10-50个示例之后），进一步增加示例的作用就会减弱。

另一方面，如果有成千上万的标注数据可用，进行一个针对特定任务的微调仍然会带来性能的进一步提升。

这导致了一个实际的建议：对于标注数据有限的场景，Few-Shot是最好的选择。但如果有足够的数据，微调仍然是值得的。

## 成本与民主化

GPT-3的发布方式也很特殊。OpenAI不是直接发布模型权重（虽然后来为少数研究机构提供了访问），而是提供了一个API接口。用户可以通过调用API来使用GPT-3，按使用量付费。

这个决定有几个含义。首先，它确保了OpenAI能够控制模型的使用，防止被用于有害目的。其次，它使得没有大型GPU集群的个人和小公司也能使用GPT-3。第三，它为OpenAI创造了一个商业模式。

但这也意味着，GPT-3的能力被"锁定"在OpenAI的服务器中。开源社区无法获得完整的模型权重来进行研究或改进。这引发了关于开源vs.闭源的辩论。

有意思的是，这个辩论很快得到了解答。到了2023年，Meta、Stability AI等机构发布了自己的大规模开源模型（LLaMA、Mistral等），使用了与GPT-3类似的规模和方法。这表明，GPT-3的方法不是专属于OpenAI的，而是深度学习发展的一个自然阶段。

## 涌现能力的第一个标志

也许GPT-3最重要的贡献，不是它在具体任务上的性能，而是它第一次展现了所谓的"涌现能力"（Emergent Ability）。

涌现是指，当一个系统（在这里是神经网络）达到某个规模或复杂度时，出现了在更小的版本中完全不存在的新能力。就像一个分子足够多的物质达到某个温度后，物理性质会突然改变一样。

以Few-Shot Learning为例。GPT-2虽然也有一定的Few-Shot能力，但远不如GPT-3。GPT-3在规模上的跨越似乎触发了这个能力的量变到质变。

论文中展示的其他涌现能力包括：算术推理、常识推理、甚至简单的代码生成。在这些任务上，规模的增加似乎不是线性的，而是在某个点之后产生了显著的跳跃。

这个发现至关重要，因为它改变了我们对AI能力的理解。如果能力是通过规模涌现的，那么只要有足够的计算资源和数据，我们也许可以期待更强大的AI系统。这也就是为什么在GPT-3之后，AI研究的竞争变成了一场"规模军备竞赛"。

## 对后续的启示

GPT-3对整个AI领域的影响是深远的：

首先，它验证了Scaling Laws不仅仅是一个学术观察，而是一个可行的商业策略。如果规模确实能带来性能提升，那么投入更多资源来训练更大的模型就是合理的。

其次，它打开了In-Context Learning作为一个研究方向的大门。后续的工作（包括我们下一节将讨论的Prompt Engineering）都建立在对In-Context Learning的深入理解之上。

第三，它激发了对涌现能力的研究热情。为什么会出现涌现？什么时候会出现？涌现的边界在哪里？这些问题成为了后续工作的核心。

最后，它改变了NLP研究的范式。从关注特定任务的微调，转向关注通用模型的能力。从精心设计的Prompt工程，转向理解如何让模型理解任务指令。

## 局限与批评

尽管GPT-3的成就是显著的，但它也有明显的局限：

首先是知识的时效性。GPT-3的知识截止于2021年左右。对于需要最新信息的任务，它会产生不准确的答案。

其次是推理能力的局限。虽然GPT-3能做一些简单的算术，但在复杂推理上仍然容易出错。它有时会"编造"答案而非承认不知道。

第三是缺乏长期记忆。每次对话都是独立的。它无法记住跨越不同会话的信息。

第四是成本问题。虽然API使用相对便宜，但大规模应用仍然需要考虑成本。

这些局限推动了后续研究的方向，也为改进留下了空间。

## 遗产

2020年5月的GPT-3发布，标志着一个时代的开始。它不仅是一个更大的模型，更重要的是，它展示了规模可以带来质变。它证明了Few-Shot Learning的可行性。它触发了涌现能力的发现。

也正是因为GPT-3的成功，ChatGPT三年后才有了足够的技术基础和商业论证。175B参数的模型经过适当的对齐和微调，就成为了改变世界的工具。

有人说，GPT-3是AI走向通用的第一步。虽然我们现在知道这还远不是终点，但GPT-3确实是那个标志性的时刻：人们开始相信，通过规模和学习，AI可能真的能做很多事情。那种"如果规模足够，也许一切都是可能的"的信念，从GPT-3开始，就深深地嵌入了整个AI社区的集体意识中。