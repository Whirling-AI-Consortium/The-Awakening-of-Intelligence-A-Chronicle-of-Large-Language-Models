---
sidebar_position: 3
---

## 稀疏激活的洞察

在深度学习的发展中，一个根本的趋势是模型规模的持续增长。从一百万参数到十亿参数再到万亿参数，规模的增加带来了能力的提升。但规模的增加也带来了成本的激增：训练时间、计算资源、推理延迟都随之增加。

一个关键的观察来自Shazeer在2017年的论文《Outrageously Large Neural Networks for Efficient Conditional Computation》。这个观察很简单：**我们是否需要在每个输入上都激活模型的所有参数？**

在传统的密集（Dense）神经网络中，答案是肯定的。每个输入都会通过所有的层和所有的权重。但也许对于不同的输入，模型的不同部分会变得更有相关性。例如，在一个多语言模型中，处理中文输入时，负责中文处理的部分应该被激活；处理英文时，英文处理的部分应该被激活。

这个想法导致了**混合专家（Mixture of Experts, MoE）**架构。在MoE中，模型包含多个专家（Expert）——每个都是一个相对较小的子网络。给定一个输入，一个门控网络（Gating Network）决定哪些专家应该被激活。只有被选中的专家会处理这个输入。结果是，尽管模型的总参数数量很大，但对于任何单个输入，只有一小部分参数被激活（Sparse Activation）。

这个想法在理论上很吸引人，但在实践中实现MoE一直很困难。长期以来，MoE主要停留在研究阶段，在工业应用中很少被采用。直到最近几年，随着计算基础设施的改进和新的训练技术的出现，MoE才开始在大规模模型中被有效地应用。

## MoE架构与路由机制

标准的MoE架构包含几个关键组件。最简单的形式是在Transformer的前馈网络层中添加MoE。对于标准Transformer中的每个前馈层（通常是两个线性层加一个非线性激活），可以用一个MoE层替代。这个MoE层包含 $N$ 个专家，每个是一个前馈网络。

路由机制是MoE的核心。给定一个输入 $x$，门控网络计算一个分数向量：

$$s = W_g \cdot x$$

其中 $W_g$ 是一个可学习的权重矩阵。然后，这个分数向量通过softmax得到一个概率分布：

$$p = \text{softmax}(s)$$

或者使用一个稀疏的Top-K操作，只选择概率最高的K个专家：

$$r_i = \begin{cases} 1 & \text{if } i \in \text{Top-K}(s) \\ 0 & \text{otherwise} \end{cases}$$

然后，输入被路由到被选中的专家，输出被加权求和：

$$y = \sum_{i=1}^{N} r_i \cdot p_i \cdot \text{Expert}_i(x)$$

其中 $p_i$ 是第 $i$ 个专家的权重（从softmax输出或经过其他归一化）。

最常用的实现是**Top-1或Top-2路由**。在Top-1中，每个输入被发送到概率最高的单个专家。在Top-2中，发送到概率最高的两个专家。Top-1最简单且计算最高效，但Top-2或更高的K值通常能获得更好的精度，因为多个专家的协作提供了冗余性和更好的表示。

## Switch Transformer与稀疏激活的规模化

Switch Transformer是Google在2021年发表的一项重要工作，它展示了如何在大规模上有效地使用MoE。论文题目《Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparse Mixture-of-Experts》暗示了其野心：构建有万亿参数的模型。

Switch Transformer使用了极端的稀疏性——**Top-1路由**，每个输入只被发送到单个专家。虽然这听起来过于激进（丧失了多专家协作的益处），但论文通过大量实验证明了，对于足够大的模型和充足的数据，即使是Top-1路由也能工作良好。

关键的创新是对训练稳定性的关注。MoE训练历史上遭遇的一个主要问题是**路由崩溃（Router Collapse）**——某些专家获得所有的流量，而其他专家变成死亡的（从不被激活）。这导致：第一，这些专家的梯度为零，无法学习。第二，活跃的专家过载，性能下降。第三，模型的表示能力受限。

为了解决这个问题，Switch Transformer引入了**辅助损失（Auxiliary Loss）**。这个额外的损失项鼓励负载平衡——所有专家应该大约平等地接收流量。具体来说，对于每个batch，计算每个专家接收的token数量的方差。较高的方差表示不平衡，辅助损失会惩罚这个不平衡：

$$L_{\text{aux}} = \alpha \cdot (\text{CV}(\text{expert\_tokens}))^2$$

其中CV是变异系数（标准差/平均值），$\alpha$ 是加权因子。这个简单的添加显著改进了训练稳定性。

## 不同MoE方案与其权衡

虽然Switch Transformer的Top-1方案简单有效，但不同的应用场景催生了多种MoE变体。

**GShard与GLaM**是Google在超大规模模型训练中使用的MoE变体。GLaM（Generalist Language Model）是一个1.2万亿参数的模型，包含64个专家。与Switch Transformer的Top-1不同，GLaM使用了更复杂的负载平衡和专家分配策略，包括**分组（Grouping）**——tokens被分组，每组被路由到一个专家。这增加了通信的局部性和效率。

**Mixtral 8x7B**是一个最近的、也是迄今为止最成功的开源MoE模型。由Mistral AI发布，它包含8个7B参数的专家和一个路由层。推理时，每个token被路由到2个专家（Top-2）。虽然模型有56B的总参数，但由于稀疏激活，每个token只激活14B参数。这使得Mixtral在推理效率上与单个13B密集模型相当，但能力更强。

**CMoE（Custom Mixture of Experts）**和其他方案关注于**专家的多样性**。而不是让所有专家是相同的前馈网络，不同的专家可以有不同的架构、参数化或初始化。这增加了模型容量和多样性。

**条件计算**框架更广泛地考虑了哪些部分应该被激活。除了前馈网络外，MoE也可以应用于Transformer的其他部分——注意力层、嵌入层等。虽然这增加了灵活性，但也增加了复杂性。

## MoE的收益与成本分析

MoE的主要优点是什么？最明显的是**参数效率**。一个有N个专家的MoE模型可以有N倍于单个专家的参数，同时在相同的计算成本下运行（因为只有一小部分参数被激活）。这意味着对于固定的计算预算，MoE模型可以比密集模型大得多。

论文实验表明，一个MoE模型可以用相同的FLOPs比密集模型快数倍。例如，一个switch Transformer可以用相同的训练成本达到比密集Transformer更好的性能。

但MoE也有显著的成本。**通信开销**是首要的。在分布式训练中，tokens需要被发送到分配给它们的专家，这些专家可能在不同的设备上。这需要大量的跨设备通信，可能成为瓶颈。

**不均衡的负载**仍然是一个问题。尽管有辅助损失，某些输入类型或batch仍然可能导致某些专家过载。例如，某些语言的tokens可能被路由到相同的专家集合。

**推理的复杂性**也增加了。虽然推理时的总激活参数更少，但路由决策和多专家的并行协调增加了系统复杂性。在某些硬件上（特别是较小的推理集群），MoE的推理速度甚至可能不如密集模型。

**内存使用**虽然对单个token更低，但如果所有专家需要保留在内存中（对于推理），总的内存需求仍然很高。只有在可以在专家之间动态卸载或有分布式推理基础设施时，这个优势才能实现。

## 负载均衡与训练稳定性

MoE的成功很大程度上取决于有效的负载均衡。多个技术被提出来解决这个问题。

**辅助损失**是最直接的方法，之前已经讨论过了。但它不是没有缺陷。辅助损失的强度参数 $\alpha$ 需要仔细调整。太强会干扰主要的学习目标，太弱则无法防止路由崩溃。

**专家容量限制（Expert Capacity）**是另一个策略。为每个专家设置一个最大容量——比如，每个专家最多处理来自batch的X%的tokens。超过容量的tokens被丢弃或重新路由。虽然丢弃tokens听起来很糟糕，但在实践中，适度的丢弃（通常$<1%$）不会显著影响性能，同时可以强制负载均衡。

**具有历史的路由（Routing with History）**考虑的是过去几个step的路由决策。这可以帮助稳定路由，防止剧烈的波动。

**多头路由（Multi-Head Routing）**使用多个路由函数，每个产生不同的路由决策。这增加了多样性，减少了单个路由决策导致崩溃的风险。

## MoE与其他效率技术的结合

MoE可以与上述讨论的其他效率技术结合，创建更强大的系统。

**MoE + 量化**：由于只有子集的参数被激活，量化的挑战可能更容易。某些非活跃的专家可以被量化得更激进，因为它们不经常被使用。

**MoE + LoRA**：LoRA适配可以应用于MoE专家，在保持原始专家的同时进行任务特定的调整。这对多任务学习特别有用。

**MoE + 知识蒸馏**：一个更大的MoE模型可以蒸馏成一个更小的模型（密集或MoE），权衡参数效率与性能。

**MoE + 长文本处理**：结合FlashAttention等效率改进，MoE可以处理更长的序列，同时保持计算效率。

## 部署的挑战

尽管MoE在理论上有吸引力，实际部署仍然面临多个挑战。

首先是**框架支持**。虽然主要框架（PyTorch、TensorFlow）都有MoE实现，但不是所有框架都同样成熟或高效。某些特定的路由策略或负载均衡技术可能没有优化的实现。

其次是**硬件考量**。MoE最适合在大规模分布式设置中运行，专家分散在多个GPU或TPU上。在单个GPU上运行MoE可能不如密集模型高效，因为路由开销相对于计算量更显著。

第三是**可调试性**。MoE模型的行为更复杂，有多个相互作用的路由决策。调试问题（如某个专家始终未被使用或性能下降）需要特定的诊断工具。

第四是**再现性**。由于路由的随机性和通信的复杂性，MoE训练的再现性可能很困难。结果可能因运行而略有不同。

## 开源MoE的兴起

最近，开源社区开始采用MoE。Mistral的Mixtral是一个里程碑，展示了MoE可以在开源设置中有效工作。随后，许多社区项目探索了MoE变体——从不同的专家数量到不同的路由策略。

这推动了对MoE的理解和工具的进步。优化库变得更加可用。关于如何有效训练和部署MoE的最佳实践被共享。这反过来推动了更多的采用和创新。

## 结语

MoE代表了一个关于计算效率的深刻洞察：我们不需要在每个输入上激活模型的所有参数。通过条件激活和聪明的路由，我们可以构建远大于密集模型但计算成本类似的模型。

虽然MoE的概念已经存在多年，但最近的突破——特别是Switch Transformer和Mixtral——展示了它的实际潜力。当与其他效率技术结合时，MoE成为了构建可扩展AI系统的强大工具。

但MoE也不是解决所有问题的银弹。它引入了复杂性，需要仔细的负载平衡和分布式训练基础设施。对于许多应用，密集模型可能仍然是更实用的选择。

然而，在追求更强大AI系统的过程中，MoE提供了一个有前景的方向：通过允许模型在不同情况下使用不同的"专家"，我们可以获得规模的益处，同时保持计算效率。这是效率与能力之间的精妙平衡。