---
sidebar_position: 0
---


## 2020年的困惑

在2020年初的OpenAI办公室里，研究者们面临着一个既兴奋又困扰的局面。GPT-2的成功已经证明了自回归语言模型的潜力，但随之而来的问题更加棘手：如果我们想要构建一个更强大的模型，到底应该怎么做？

这不仅仅是一个工程问题。在机器学习的历史上，模型的改进往往来自于巧妙的架构设计或创新的训练方法。但现在，研究者们隐隐感觉到，也许还有更基本的东西被忽视了——模型规模本身。一个朴素的直觉在酝酿：如果我们投入更多的计算资源、使用更多的数据、训练更大的模型，会发生什么？

但这个问题没有明确的答案。深度学习的理论还远远不够成熟。有的论文提出过规模会导致过拟合，有的则声称增加模型大小带来的收益会逐渐消失。在这种不确定性中，没人敢大举押注规模。

2020年年中，Jared Kaplan领导的OpenAI团队决定系统地回答这个问题。他们没有选择在某个具体的任务上做出最好的模型，而是做了一项看似不太"酷"但却深刻得多的工作：用数学的方式理解规模与性能的关系。

## 寻找隐藏的规律

论文《Scaling Laws for Neural Language Models》发表于2020年年中，它做的事情非常直截了当却又极其系统。团队训练了一系列参数量从800万到160亿不等的GPT-2架构模型，每一个都用不同大小的数据集进行训练。他们不追求任何具体任务的最佳性能，而是纯粹地观察一个指标：在验证集上的语言模型损失函数（Language Model Loss）。

所有的实验数据被绘制到图表上时，一个令人震惊的规律浮现了出来。不管是模型参数数量 $N$、训练数据的token数 $D$，还是实际使用的计算资源 $C$，它们与最终损失函数之间的关系都可以用幂律函数描述：

$$L(N) \propto N^{-\alpha}$$

$$L(D) \propto D^{-\beta}$$

$$L(C) \propto C^{-\gamma}$$

其中 $\alpha \approx 0.074$，$\beta \approx 0.076$，$\gamma \approx 0.076$。这意味着什么？意味着这些关系不是模型或数据的具体特性，而是更深层的、几乎普遍的规律。

论文的影响是震撼性的。在当时，这样的定量结果几乎没有先例。通常，机器学习论文会展示一个新方法如何击败基准，但很少有人试图找到跨越所有方法、所有模型大小的通用规律。Jared Kaplan和团队做的正是这个——他们在寻找的是深度学习本身的法则。

## 论文的核心发现

为了获得这些规律，团队进行了计算量巨大的消融实验。他们分别改变以下变量：

**参数量的影响**：固定数据量和计算预算，逐倍增加模型大小。他们发现损失函数随着参数量的增加而平稳下降，最小的模型远不如最大的模型，但增长的曲线很少出现崩溃的情况。

**数据量的影响**：固定模型大小，用不同大小的数据集进行训练。有趣的是，数据的幂律指数与参数的指数几乎相同，这意味着模型和数据的贡献大致是对称的。

**计算量的影响**：这是最实用的一个。无论通过增加参数还是数据来消耗计算资源，最终的效果遵循同样的规律。这对于有固定计算预算的组织至关重要。

论文提出了一个优化建议：假设你有固定的计算预算 $C$，应该如何分配给模型大小和数据量？最优的选择是模型和数据的规模应该大致成比例增长。换句话说，当你有足够的资源训练一个1000亿参数的模型时，你应该准备至少几千亿个token的数据，否则就是浪费计算资源。

这听起来像是常识，但在2020年并非如此。很多实践中，研究者们会固定数据量，不断增加模型大小，直到模型过拟合。Scaling Laws论文表明，这种做法是对资源的浪费。

## 幂律：大自然的笔迹

为什么是幂律？这个问题本身就很有趣。在物理学中，幂律几乎无处不在。从地震的里氏震级分布到城市人口的Zipf定律，从复杂网络的度数分布到神经网络的权重分布，幂律似乎是复杂系统的签名。

有一种理论认为，当一个系统由大量相互作用的组件组成，并且系统在不同尺度上表现出自相似性时，幂律就会出现。深度学习模型正是这样的系统：它由数十亿个参数构成，每一个都通过反向传播相互影响。当模型足够大、数据足够多时，这个系统的性能可能就自然地遵循幂律。

论文作者也承认，他们还没有完全理解为什么会是这样。但他们有一个重要的推论：既然这些规律如此稳定和通用，也许可以用少量的小实验来外推大实验的结果。换句话说，通过训练一个小模型，观察损失曲线的衰减趋势，我们可以相当准确地预测一个大模型会有多好。

这个洞察开启了一个新的研究方向：用小模型的实验数据来预测大模型的性能，从而在浪费大量计算资源前发现某个想法是否可行。

## 代码的启示

让我们用一个简化的例子来看看Scaling Laws如何在实践中工作：

```python
import numpy as np
import matplotlib.pyplot as plt

# 模拟幂律关系
# Loss = a * N^(-alpha)
def scaling_law_loss(N, alpha=0.074, a=1.0):
    return a * (N ** (-alpha))

# 模型大小（参数数量，单位：百万）
model_sizes = np.logspace(3, 11, 50)  # 从10M到100B

# 计算对应的损失
losses = scaling_law_loss(model_sizes)

# 给定计算预算C，预测最优的模型和数据分配
def optimal_allocation(C, alpha=0.074, beta=0.076):
    """
    假设计算预算为C，根据Scaling Laws建议最优的N和D分配
    C ≈ N * D (简化假设，不考虑常数项)
    最优分配：N_opt ≈ C / D_opt, 其中D_opt满足最小化总损失
    """
    # 实际上，最优分配应该是N和D大致相等（在对数空间）
    # 即：log(N) ≈ log(D)，所以N ≈ D
    # 如果C = N * D，那么N ≈ sqrt(C)，D ≈ sqrt(C)
    optimal_N = np.sqrt(C)
    optimal_D = np.sqrt(C)
    
    loss = scaling_law_loss(optimal_N, alpha=alpha)
    return optimal_N, optimal_D, loss

# 场景：如果有1e13的计算预算
C = 1e13
opt_N, opt_D, opt_loss = optimal_allocation(C)

print(f"给定计算预算 {C:.2e}:")
print(f"  推荐模型参数量: {opt_N:.2e}")
print(f"  推荐训练token数: {opt_D:.2e}")
print(f"  预期损失: {opt_loss:.4f}")
```

这段代码展示的就是Scaling Laws论文的核心思想的实际应用。假设一个组织要构建一个语言模型，知道自己有多少计算预算，就可以用这个规律来指导决策。

## Chinchilla的修正

2022年，DeepMind发表了一篇论文《Training Compute-Optimal Large Language Models》，作者包括著名的Kaplan和Hoffmann。这篇论文在某种意义上是对Scaling Laws论文的修正和深化。

Chinchilla论文指出，OpenAI的原始建议可能过于保守。他们通过更多的实验发现，对于给定的计算预算 $C$，最优的策略应该是训练一个较小的模型（参数量为 $N_{opt}$）使用较大的数据集（$D_{opt} ≈ 20 \times N_{opt}$）。这与OpenAI原来建议的大致相等的规模不同。

换句话说，Chinchilla建议：与其训练一个3000亿参数的模型用3000亿个token的数据，不如训练一个700亿参数的模型用1.4万亿个token。后者会用相同的计算成本达到更好的效果。

Chinchilla论文的发表引发了业界的一些反思。但这也展示了Scaling Laws研究的活力——即使在核心规律框架确立之后，仍然有细节可以探讨和改进。

## 规律改变了什么

如果说Scaling Laws论文只是一篇学术工作，那就太小看它了。这篇论文实际上改变了整个AI行业的商业逻辑。

在2020年之前，AI投资的决策基于相当模糊的直觉：一个有才华的团队、一个创新的想法、对未来的乐观展望。但Scaling Laws给了投资者一个量化的论证。如果性能与规模之间存在可预测的幂律关系，那么问题就变成了一个单纯的经济学问题：给定初始投资，能在多长时间内通过更多的计算达到更好的性能？

这个转变非常关键。正因为有了Scaling Laws，OpenAI、DeepMind和Google等公司才敢于投入数十亿美元来构建超大规模模型。没有这种理论基础，董事会可能不会批准这些预算。

同时，Scaling Laws也改变了研究的优先级。在此之前，论文会关注在某个特定数据集上的微小改进——比如在SQuAD上提高0.5%。但Scaling Laws表明，这些微小的架构改进相比模型本身的规模来说几乎微不足道。基于这个认识，研究的重点逐渐从架构创新转向规模探索。

## 未解的谜团

尽管Scaling Laws论文给出了令人信服的经验规律，但理论上的理解仍然不充分。为什么幂律指数会是这样的数值？是否存在某个根本的信息论下界来解释这个指数？在数据更多或模型更大的情况下，这个规律是否会破裂？

还有一个更深层的问题：Scaling Laws描述的是损失函数与规模的关系，但损失与真实的智能或能力之间的关系是什么？一个损失下降一半的模型，真的就聪明一倍吗？有些研究表明，损失的改进和在某些任务上性能的改进之间的关系并不是线性的。在大语言模型中，有时候一个看似微小的损失改进，可能带来能力上的质的飞跃。

这就引出了我们后面将要讨论的主题：涌现（Emergence）。当模型规模达到某个临界点时，会出现在小模型中完全看不到的能力。这个现象无法完全用Scaling Laws来解释，但却是理解现代大语言模型的关键。


2020年的Scaling Laws论文如今已经成为了深度学习的经典。任何有志于构建大规模语言模型的机构都需要理解这些规律。初创公司根据这个框架来估算他们需要多少资金。研究机构用这个框架来规划他们的GPU集群采购。甚至政策制定者也开始用这个框架来评估AI的发展轨迹。

也许更重要的是，Scaling Laws代表了一种科学态度的转变：从追求单点的突破，转向理解系统的法则。在深度学习的狂热年代，提出通用规律需要相当的自信和远见。Jared Kaplan团队做了这件事，并因此改变了整个领域。

今天，当我们谈论"大力出奇迹"这个行业俚语时，我们其实是在引用一个经过数学证明的深层规律。那些投资数千亿美元的决定，本质上都建立在2020年那些精确的实验曲线之上。