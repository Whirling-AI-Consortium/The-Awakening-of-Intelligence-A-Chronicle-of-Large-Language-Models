---
sidebar_position: 2
---



## 一场无声的军备竞赛

2022年初，大语言模型的上下文长度还停留在一个令人尴尬的水平。OpenAI的GPT-3支持2048个tokens，看起来勉强能处理一篇短新闻，但面对任何稍微长一些的文档就捉襟见肘。当时一个常见的使用方式是将长文本分割成多个片段，分别喂给模型，然后试图整合结果。这不仅低效，更切割了文本的连贯性。

但到了2023年，整个局面发生了戏剧性的转变。这一年成为了上下文长度的"奇点"：Claude从原始的9K扩展到100K tokens，GPT-4 Turbo支持128K tokens，而Gemini Pro在功能演进中也开始支持更长的上下文。到了2024年中期，Gemini 1.5 Pro宣布了一个让人瞠目结舌的数字——百万tokens。从2048到1000000，短短两年间上升了500倍。

这场竞赛的背后不仅仅是企业之间的品牌较量，更代表了一次深刻的技术突破。长上下文能力直接决定了大模型能做什么和不能做什么。一个支持4K tokens的模型只能处理约15页的文档；一个支持100K的模型可以一次处理整部小说；而一个百万token的模型可以同时容纳数百本书籍的信息。这不是简单的量变，而是质变——它重新定义了大模型应用的边界。

## 从瓶颈到突破：技术的五重门

要理解为什么从2K跨越到百万token如此困难，必须理解长上下文面临的五个核心瓶颈。

**第一道门：计算复杂度。** 如前所述，标准的Attention机制是$O(n^2)$。我们已经通过Sliding Window和稀疏Attention部分解决了这个问题，但代价是舍弃了某些全局信息。支持百万token意味着即使采用稀疏化，仍需处理数百亿的计算操作。

**第二道门：内存压力。** 一个百万token的序列，即使只以INT8精度存储激活值，也需要超过1GB的显存仅用于KV缓存。在生成阶段，KV缓存成为了最严重的瓶颈。当批处理多个请求时，内存需求会成倍增长。

**第三道门：位置编码的外推。** 我们在前一章讨论过，大多数预训练使用的位置编码（包括RoPE）都是在有限长度下训练的。当推理长度超过训练长度时，位置编码会出现失效。虽然内插方法（如NTK-Aware缩放）能缓解这个问题，但仍会导致性能下降。

**第四道门："Lost in the Middle"问题。** 这个看似反直觉但真实存在的现象表明，当上下文足够长时，模型对中间的信息会产生关注不足。在被要求从一个很长的文本中提取特定信息时，模型往往能记住开始和结尾的部分，但对中间部分的检索能力会下降。这不是因为模型不够聪明，而是因为Attention的学习动态——在长上下文下，注意力权重的分布会产生某种偏差。

**第五道门：训练的不稳定性。** 在极长序列上进行预训练涉及到极其复杂的数值稳定性问题。梯度可能会出现爆炸或消失，学习率的设置变得极其敏感。此外，长序列训练对硬件同步和通信的要求也指数级增长。

## 分布式的突破：Ring Attention与超越

Anthropic在其100K token Claude模型中采取的方案之一是Ring Attention，这是一个在2023年由领先的研究团队提出的分布式注意力机制。Ring Attention的核心洞察是：与其在单个GPU上存储完整的KV缓存，不如在多个GPU间分布式地管理这些缓存。

Ring Attention的工作原理是这样的：假设我们有8个GPU，序列被分成8个段，每个GPU负责一个段。在计算Attention时，GPU 0上的查询需要与所有的Key和Value交互。Ring Attention采用了一个"流水线"的思路：GPU 0先与自己的KV计算，然后接收GPU 1的KV，计算完后将自己的KV传递给GPU 2，同时接收GPU 3的KV。这样形成了一个"环"——数据在GPU间流动，而不是集中存储。

```python
def ring_attention(Q, K, V, num_gpus):
    """
    Ring Attention的简化伪代码
    Q, K, V: (seq_len//num_gpus, dim)，每个GPU上的本地段
    """
    seq_len_local = Q.shape[0]
    output = torch.zeros_like(Q)
    
    # 初始化：本地KV
    KV_remote = (K, V)
    
    for step in range(num_gpus):
        # 1. 使用当前KV计算局部Attention
        scores = torch.matmul(Q, KV_remote[0].transpose(0, 1))
        scores = scores / math.sqrt(Q.shape[-1])
        attn = torch.softmax(scores, dim=-1)
        output += torch.matmul(attn, KV_remote[1])
        
        # 2. 循环传递KV到下一个GPU（环形拓扑）
        if step < num_gpus - 1:
            KV_remote = rotate_to_next_gpu(KV_remote)
    
    return output
```

这个设计的妙处在于它将分布式计算与Attention的全局依赖性巧妙地协调起来。通过8个GPU的Ring Attention，可以支持单GPU的8倍序列长度，同时计算时间几乎没有增加。这个技术成为了支持超长序列的关键基础设施。

但Ring Attention本身还不足以跨越到百万token。Google在Gemini 1.5的技术报告中暗示，他们采用了多层级的策略。对于最长的层，可能使用了更激进的稀疏化方案；对于中间层，使用Ring Attention；对于浅层，甚至可能使用某种动态的上下文管理——模型在解码过程中动态决定应该关注序列的哪些部分。

## 长上下文检索：从被动到主动

支持长上下文的另一个关键突破来自于检索增强（Retrieval Augmentation）的演进。最初的想法很简单：不要试图让模型一次性处理所有信息，而是在需要时动态检索相关部分。但这引发了一个新的问题：模型如何知道应该检索什么？

2023年的一个重要发现是，大模型本身可以学会"元认知"——即对自己知识的认知。论文《Self-RAG: Learning to Retrieve, Generate, and Reflect Critically》展示了一个有趣的想法：让模型在生成过程中，根据需要插入检索步骤。模型会学到在什么时候需要外部信息，然后显式地调用检索工具，获取相关内容后继续生成。

这个想法的优雅之处在于它不需要复杂的路由算法。模型通过训练自然地学会了何时需要检索。在某些任务上，Self-RAG的性能甚至超过了直接微调的模型，因为模型能够更精准地控制信息流——关键信息被及时检索，噪声被最小化。

结合这个思想，Claude和其他模型采取了一个混合策略：短期上下文（比如过去100个tokens）完全存储在KV缓存中以确保实时性，中期上下文通过压缩或摘要的形式保存，远期上下文则通过检索动态获取。这样可以在支持长序列和保持计算效率之间达到平衡。

## "Lost in the Middle"的诅咒与救赎

当研究者们在2023年仔细分析长上下文模型的行为时，发现了一个令人困扰的规律：给定一个100K token的上下文和一个检索任务，模型对出现在开头或结尾的信息检索效果很好，但对中间部分的检索能力显著下降——即使这些信息在上下文中完全清晰可见。

这个现象的根源是Attention的梯度流动。在长序列上，Attention权重的学习动态导致模型偏向于"关注"序列的边界。论文《Lost in the Middle: How Language Models Use Long Contexts》的作者们测试了多个模型，包括GPT-3.5、Text-davinci-003等，都展现出了这种偏差。有趣的是，较小的模型（70B以下）的问题更严重，而更大的模型虽然有所缓解但仍存在。

解决这个问题的方法包括：第一，在训练阶段显式地使用随机长度的上下文，让模型学会在任意位置寻找信息。第二，在Attention设计中加入位置相关的权重调整，确保模型不会自动忽视中间部分。Anthropic在Claude中采取的策略可能包括两者的结合，加上某种显式的信息标记机制——让重要信息通过特殊的token被标记，从而吸引模型的注意力。

有趣的是，最新的研究表明，这个问题可能会在模型规模进一步扩大时自动缓解。OpenAI的研究暗示，当模型参数量达到某个阈值时，模型的全局定位能力会出现涌现——它能够更好地处理长上下文中的任意位置检索。这再次印证了我们在《规模的魔法》章节中讨论的缩放定律的威力。

## 百万token的现实与愿景

当Gemini 1.5在2024年中期宣布百万token支持时，技术社区为之震动。但这个成就到底意味着什么？它是如何实现的？

从Google发布的有限信息来看，百万token的实现可能涉及以下几个层面的创新：首先，该模型采用了某种多层级的表示——长上下文被压缩或层级化表示，不是所有token都以相同精度存储。其次，Attention可能被进一步稀疏化，或者采用了某种条件计算的方案——模型能够动态决定哪些部分需要精细的注意力计算。再次，KV缓存的管理被彻底重新设计，很可能采用了某种激进的量化或者分页式的内存管理。最后，位置编码被显著改进，可能不再是简单的RoPE或ALiBi，而是某种自适应的位置表示。

但即使有这些技术突破，百万token也面临一个根本的限制：实际的有用性。一个百万token的上下文相当于大约300本200页的书籍，但人工智能是否真的能够在这么庞大的信息中进行有意义的推理？这不仅涉及技术问题，更涉及认知科学的基本问题。

初步的实验结果令人惊讶。用百万token上下文的Gemini 1.5在某些信息检索任务上表现可靠，甚至在处理超长视频转录（百万token相当于约100小时的转录文本）时也表现出色。但在需要复杂推理的任务上，性能提升的幅度相对有限。这启发了一个思考：超长上下文的真正价值可能不在于复杂推理，而在于信息容纳和表面层面的检索——这本身就足以开启许多新的应用。

## 未来：无限长度的可能性

现在的问题是：我们是否能够实现真正的"无限"上下文？

从技术上看，瓶颈逐渐转向了硬件而非算法。如果我们有无限的GPU资源，Ring Attention之类的技术理论上可以支持任意长度的序列。但在实际中，通信开销会成为新的瓶颈——在成百上千个GPU间协调长序列的计算变成了一个配置和网络工程的问题，而不仅仅是算法问题。

另一个方向是彻底放弃"处理完整上下文"的想法，转向某种动态的、流式的上下文管理。想象一个模型能够以流式方式接收输入，动态地更新其内部状态，而不需要存储完整的历史。这种想法与某些人脑的工作方式更接近——我们不是同时处理所有记忆，而是动态地激活相关的信息。

论文《StreamingLLM: Efficient Streaming Language Models with Attention Sinks》探索了这个方向。关键的洞察是：在长推理过程中，模型的Attention权重会高度集中在最近的少数几个token（称为"Attention Sinks"）。StreamingLLM提议只保留这些关键token，加上最新的token，其他旧token则被逐渐遗忘。这样可以用常数内存支持无限长序列的流式处理。

还有一个根本性的问题需要思考：在百万token或更长的上下文中，模型是否真的在"理解"，还是在进行某种复杂的表面层面匹配？当Claude声称能够处理整部小说并回答深层问题时，它是真的在"阅读"小说，还是在执行某种高级的模式识别？这是一个哲学问题，但也是技术问题——因为答案决定了长上下文技术的真正价值上限。

## 收官

从2022年的2048 tokens到2024年的百万 tokens，我们见证了一次技术的飞跃。这个飞跃不是单一技术的胜利，而是多项创新的叠加：位置编码的进化（RoPE）、稀疏Attention的成熟（Sliding Window）、分布式算法的突破（Ring Attention）、模型架构的优化，以及对长上下文问题本质的更深理解。

每一步的背后都是研究者对问题的持续追问：为什么全局Attention是必要的？为什么位置信息会失效？为什么模型会忽视中间的信息？通过不断地问和答，我们逐渐从"让模型处理更长序列"这个工程问题，演进到"模型如何理解长文本"这个认知问题。

长上下文的军备竞赛还远未结束。即使实现了百万token，仍有更多的问题等待解答。但可以确定的是，长上下文能力已经成为了大模型的一项基本要求。它不再是一个边缘的能力指标，而是与模型的智能程度一样重要的维度。当下一代模型诞生时，我们可能会期待它不仅更聪慧，更重要的是能够记住更多。