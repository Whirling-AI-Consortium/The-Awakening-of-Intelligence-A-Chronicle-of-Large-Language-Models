---
sidebar_position: 0
---


## 失踪的位置信息

2018年当《Attention Is All You Need》问世时，Transformer架构展现了令人瞩目的并行化能力——相比递归神经网络逐步处理序列，Transformer可以同时处理整个序列中的所有token。但这份优雅背后隐藏着一个根本的问题：没有了RNN的循环结构，模型如何知道每个单词在序列中的位置？

原论文提出的解决方案是正弦位置编码（Sinusoidal Positional Encoding），即对每个位置$p$和维度$d$，分别使用$\sin(\frac{p}{10000^{2i/d}})$和$\cos(\frac{p}{10000^{2i/d}})$来编码。这个方案虽然优雅且能够外推到更长的序列，但存在一个微妙的缺陷：它是绝对的。两个不同位置的token即使相距相同，其位置编码的关系也会因为它们的绝对位置而改变。换句话说，模型难以天然地捕捉相对位置关系——这恰好是推理和理解的关键。

在接下来的五年里，研究者们意识到这个看似微小的设计决策可能成为制约模型长上下文能力的瓶颈。当模型参数从10亿增长到1000亿量级，当应用场景从摘要总结延伸到长文档处理，位置编码的局限性逐渐显露。到了2023年初，当大语言模型开始竞相支持更长的上下文窗口时，位置编码已然成为所有工程师心中的一块心病。

## 相对位置的觉醒

2018年末，Google的研究团队在开发T5模型时意识到相对位置编码可能更加有效。他们的洞察很直接：与其让模型记住"这是第47个token"，不如让模型理解"这个token距离上一个重要信息隔了3个位置"。这个想法看似简单，却需要在Self-Attention机制中优雅地实现。

T5采取的方案是在Attention的分数计算中直接添加相对位置偏置。具体来说，当计算query和key的Attention权重时，除了标准的$\text{score} = \frac{Q K^T}{\sqrt{d_k}}$，T5为每一对位置$(i, j)$添加了一个可学习的偏置项$b_{i-j}$。这个简化但有效的设计让模型能够根据相对距离来调整Attention权重，而不必依赖绝对位置信息。

然而T5的方案仍存在限制：可学习的相对位置偏置数量有限，很难直接推广到远超训练长度的序列。当sequence length突破8K、16K时，这套方案的外推能力捉襟见肘。

真正的转折出现在2021年。Facebook（现Meta）的研究者提出了ALiBi（Attention with Linear Biases），这个看似平凡的想法却蕴含着深刻的洞察。ALiBi不再使用可学习的参数，而是直接在Attention分数中加入一个固定的线性偏置：对于相对距离$|i-j|$，简单地减去$m \cdot |i-j|$，其中$m$是一个预设的斜率系数。这意味着距离越远的token对，其Attention权重就会被衰减得越多。

ALiBi的美妙之处在于它的参数完全为零——甚至不需要学习。这个固定的线性衰减足以让模型理解相对位置，而且由于衰减规则完全独立于序列长度，模型可以自然地外推到任意长度的序列。在实验中，用ALiBi预训练的模型在从2K外推到16K token时性能下降微乎其微，这在当时是革命性的突破。

## 旋转的智慧：RoPE的几何观点

就在ALiBi获得关注的同一时期，另一条技术线路正在成熟。2021年底，北京大学的研究者在论文《RoFormer: Enhanced Transformer with Rotary Position Embedding》中提出了一个几何上更为优雅的方案——旋转位置嵌入（Rotary Position Embedding，RoPE）。

RoPE的核心思想源于复数几何。每个token的查询向量$\mathbf{q}$和键向量$\mathbf{k}$不再简单地与位置编码向量相加，而是经历一个与位置相关的旋转变换。具体地，对于位置$m$，定义旋转矩阵为：

$$R_\theta^m = \begin{pmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{pmatrix}$$

其中$\theta$是频率参数。虽然这个公式看似复杂，但其妙处在于它能被高效地应用于高维向量——只需对每对相邻维度进行旋转。更重要的是，当两个位置的向量经过RoPE变换后计算点积时，相对位置$m-n$的信息被巧妙地编码进了结果。

换言之，RoPE让Attention自然地"看到"相对位置，而不是绝对位置。这在数学上是严密的：两个经过RoPE变换的向量之间的点积天然地反映了它们的相对距离。RoPE还具有一个特殊的性质——它对旋转不变性的完美处理，使得模型的Attention权重与绝对坐标系统无关，完全取决于相对关系。

从工程实现的角度，RoPE相当优雅。假设向量维度为$d$，接着将其分解为$d/2$对维度，对每一对应用旋转：

```python
def apply_rotary_pos_emb(x, freqs):
    # x: (batch, seq_len, dim)
    # freqs: (seq_len, dim//2) 预计算的频率
    
    # 将向量分组为(x1,x2), (x3,x4), ...
    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]
    
    # 应用旋转：[x1*cos - x2*sin, x1*sin + x2*cos]
    cos_vals, sin_vals = freqs.cos(), freqs.sin()
    out1 = x1 * cos_vals - x2 * sin_vals
    out2 = x1 * sin_vals + x2 * cos_vals
    
    return torch.cat([out1, out2], dim=-1)
```

这个实现轻量而高效。与绝对位置编码的加法不同，RoPE通过乘法变换深度地改变了向量的方向，使得位置信息被牢牢地焙进了Attention机制的几何结构中。

## 三种方案的抉择

到了2023年，三种位置编码方案已成为业界的三条主要路线。Meta在LLaMA中采用了RoPE，其175亿参数的模型在长上下文任务上表现突出。Anthropic在Claude中结合使用了RoPE和其他技术，支持100K token的上下文。而一些研究机构则坚持ALiBi的简洁性和可证明的外推能力。

选择哪一种方案往往取决于具体的应用场景和工程权衡。RoPE提供了最强的理论基础和实验验证，特别是在需要强大相对位置建模时。论文《NTK-Aware Scaled RoPE》进一步展示了如何通过调整频率参数来改进RoPE的外推性能，研究者们发现原始论文中的频率设定$\theta_i = 10000^{-2i/d}$可以被优化以支持到64倍长度的上下文。

ALiBi则因其参数零开销和实现的简洁性而受到关注。Mosaic ML的研究表明，用ALiBi训练的模型参数量可以更少地用于相同长度的序列处理。此外，ALiBi的线性衰减规则有更强的可解释性——Attention权重随距离线性递减，就像某种注意力的"视距限制"。

与此同时，原始的绝对位置编码（特别是可学习版本）并未完全退出舞台。在某些需要绝对定位的任务中，如代码生成中的行号定位，或结构化数据处理中的位置标记，绝对位置仍有其价值。但在通用语言模型的主流设计中，相对位置编码已成为共识。

## 前沿：超越固定位置

2023年的最新研究开始探索更激进的想法。论文《Transformer Positional Encoding Considered Harmful》和《Extrapolation and Generalization of Neural Networks with Changing Activation Functions》质疑了现有位置编码的某些假设。一些团队尝试了完全避免显式位置编码的方案，转而依赖模型自己学习隐式的位置表示。其他研究则提出了相对位置和绝对位置的混合方案。

Claude 3系列和GPT-4在支持128K上下文时采用的技术细节尚未完全公开，但从公开论文的进展来看，RoPE与ALiBi的某种变体很可能是核心组件。当Gemini 1.5宣称支持百万token时，技术社区普遍认为这背后涉及了多项位置编码的突破，包括频率参数的自适应调整、分组Attention机制的配合，乃至某种动态的位置缩放策略。

位置编码看似是Transformer架构中的边角细节，却在推向极限时成为了临界瓶颈。从绝对到相对、从可学习到固定、从加法到乘法的演进路径，反映了深度学习研究中一个普遍的智慧：最好的方案往往来自于对问题几何结构的深刻理解。RoPE的成功在于它用复数旋转优雅地表达了相对位置的本质，而ALiBi的成功则在于它发现了一个简单的规则足以捕捉位置关系的本质。当我们谈论大模型如何理解长文本时，这些看不见的位置编码正在后台默默地为每个token指引方向。