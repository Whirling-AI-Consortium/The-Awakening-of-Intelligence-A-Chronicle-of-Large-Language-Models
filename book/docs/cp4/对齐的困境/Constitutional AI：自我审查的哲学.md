---
sidebar_position: 2
---

## 人类反馈的瓶颈

到了2022年中期，RLHF已经被证明是有效的。InstructGPT展示了通过人类反馈来对齐模型的价值。但一个新的问题逐渐变得明显：随着模型变得更大、更复杂、能力范围更广，我们需要的人类反馈量呈指数级增长。

收集人类反馈不仅成本高昂，而且存在根本的可扩展性限制。要对齐一个175B的模型，需要数百万个人类判断。要对齐一个更大的模型，需要更多。而且，不是所有的标注员都有能力评估所有类型的输出。在某些技术或专业领域，可能没有足够的合格标注员。

更深层的问题是关于**标注员共识的本质**。当我们收集大量标注员的反馈时，我们实际上是在对一个多样化群体的偏好进行某种民主投票。但这种方法有根本的局限性。首先，标注员群体可能不能代表更广泛的用户群体。其次，即使他们能代表，也不清楚应该如何聚合不同的、可能相互冲突的偏好。第三，这个过程没有明确的原则——我们只是在平均不同人的意见。

Anthropic在2022年底提出了一个激进的想法来解决这个问题：与其依赖人类来评估模型的输出，不如给模型一套明确的原则，然后让模型自己评估其输出是否遵循这些原则？这就是Constitutional AI的核心思想。

## 从人类反馈到AI反馈

Constitutional AI的核心创新是用**AI反馈替代人类反馈**。这听起来很激进，甚至可能有些讽刺：用AI来对齐AI。但这个想法有其优雅之处。

首先，考虑一下"什么是好的行为"这个问题。在RLHF中，我们试图从人类判断中推断这一点。但也许有一个更直接的方式：明确地说出我们想要的原则，然后让系统遵循这些原则。

Anthropic提出了一组称为**Constitution**的原则。这个Constitution包含16条规则，涵盖了模型应该如何行动的各个方面。例如：
- 模型应该是有帮助的、诚实的、无害的
- 模型应该尊重人类的自主权
- 模型应该拒绝参与非法活动
- 模型应该避免重复或冗长
- 模型应该承认其局限和不确定性

这些原则不是凭空而来的。它们来自于多个来源：人类价值的哲学原则、AI安全研究的最佳实践、以及在实际使用中学到的经验教训。选择这些特定的原则本身就是一个价值判断，但至少是一个透明的、可以被讨论和修改的值判断。

Constitutional AI的训练过程有两个主要阶段。**第一阶段称为Critique and Revision（评判与修正）**。首先，用一个基础模型生成一些输出。然后，用相同的模型（现在扮演"评论者"的角色）根据Constitution来评判这个输出。评论者会被提示说"根据以下原则，这个输出有什么问题？"。基础模型会生成一个评判。然后，再次使用模型来修正原始输出，使其更好地符合Constitution。

这个过程看起来很神奇：一个模型在评判自己，然后改进自己。但这在实际中是可行的，因为模型在预训练时已经学会了如何进行评判和改进的概念。

伪代码大致如下：

```python
def constitutional_ai_phase1(model, constitution, num_iterations):
    """Critique and Revision Phase"""
    improved_outputs = []
    
    for _ in range(num_iterations):
        # 生成初始输出
        prompt = sample_prompt()
        initial_output = model.generate(prompt)
        
        # 评判阶段
        critique_prompt = f"""
        根据以下Constitution，评判这个输出：
        {constitution}
        
        输出: {initial_output}
        
        这个输出有什么问题或可以改进的地方？
        """
        critique = model.generate(critique_prompt)
        
        # 修正阶段
        revision_prompt = f"""
        原始输出: {initial_output}
        
        反馈: {critique}
        
        请提供一个改进的版本，更好地遵循上述原则。
        """
        revised_output = model.generate(revision_prompt)
        
        improved_outputs.append(revised_output)
    
    return improved_outputs
```

这个第一阶段的输出是一大批改进的、更符合Constitutional原则的示例。这些成为了后续监督学习的数据。

**第二阶段称为Supervised Learning from Revised Outputs（从修正输出进行监督学习）**。使用第一阶段生成的改进输出对模型进行微调，使其学会直接生成符合Constitutional原则的输出，而无需经过明确的评判和修正步骤。

这个方法相比传统的RLHF有几个优点。首先，它完全避免了对人类标注员的大规模依赖。虽然仍然可以用少量的人类反馈来进一步改进，但基本的对齐可以通过AI反馈来实现。其次，它更加可解释——原则是明确陈述的，而不是隐含在标注员的集体判断中。第三，如果想改变行为，只需改变Constitution，而不需要重新进行昂贵的人类标注。

## RLAIF与RLHF的对比

Constitutional AI的第一阶段产生了一个想法的自然扩展：**RLAIF（Reinforcement Learning from AI Feedback）**，相对于RLHF（Reinforcement Learning from Human Feedback）。

在RLAIF中，不是用人类标注员来生成偏好对，而是用一个评估模型（通常是一个经过Constitutional AI微调的模型）来生成。给定两个输出，评估模型会根据Constitutional原则判断哪个更好。然后，这个偏好对被用来训练一个奖励模型，奖励模型再用来通过RL对原始模型进行优化。

这听起来很复杂，但实际上是RLHF流程的一个直接变体，只是用AI替代了人类。

RLAIF与RLHF的对比很有启发性。RLHF的优点是：人类的判断能够捕捉微妙的、难以明确表达的偏好。人类可以处理道德上的灰色地带和价值判断的复杂性。RLHF与广泛的人类社会接触，可能更好地反映社会的多样价值观。但RLHF的缺点是：成本高昂，可扩展性受限，过程不透明，人类标注员的偏见会被编码进来。

相比之下，RLAIF的优点是：成本低廉，完全可扩展，过程透明（原则是明确的），可以轻松修改（改变Constitution），不受人类工作量限制。但RLAIF的缺点是：原则可能过于僵化，无法捕捉人类价值的全部复杂性。AI可能会过度解释或误解Constitution。可能出现新的对齐问题——AI正在教导AI遵循某些原则，这可能放大初始偏差。

在实践中，Anthropic的方法是结合两者。使用Constitutional AI和RLAIF作为主要的对齐方法，但仍然在某些步骤中使用人类反馈，特别是在评估和改进Constitutional原则本身时。这是一个务实的方法，试图获取两种方法的优点。

## 可解释性与透明性的进步

Constitutional AI的一个重要优点是它增加了模型对齐过程的透明性。在RLHF中，奖励模型学习的东西通常是隐含的。我们知道它优化了某个目标，但我们不知道确切是什么。但在Constitutional AI中，原则是明确陈述的。任何人都可以看到Claude（Anthropic的模型）被设计用来遵循哪些原则。

这个透明性有多个层面的价值。首先，它允许外部审查和批评。如果人们不同意某个原则，他们可以明确地说出来，而不是对一个黑盒奖励模型进行抽象的批评。其次，它允许科学共同体进行关于对齐的有意义的讨论。关于"什么是好的AI行为"的问题可以变成一个关于"Constitution应该包含哪些原则"的具体问题。第三，它为监管和政策创造了可能性。如果一个公司必须发布其AI系统的Constitutional原则，那么监管机构就可以审查并确保符合法律和社会标准。

但这种透明性也带来了挑战。首先，就是**原则选择的合法性**问题。谁决定Constitution是什么？在Anthropic的情况下，这是由公司的研究团队决定的。这是否民主？是否代表了广泛的利益相关者？有人争论，AI系统的Constitution应该由社会共识而不是单一公司决定。

第二是**原则的不完整性**问题。无论有多聪明，一组原则无法完全捕捉人类价值的复杂性。可能总会有一些边界情况，原则给出的指导不清楚或不适当。

第三是**可解释性的局限性**。虽然Constitutional AI比RLHF更透明，但底层的神经网络仍然是一个黑盒。我们知道模型应该遵循什么原则，但我们不一定理解它是如何遵循这些原则的，或者在遇到冲突时它如何权衡。

## 从规则到学习的另一种观点

从另一个角度看，Constitutional AI代表了AI对齐中从**规则系统**到**学习系统**的一个有趣的混合。

传统的规则系统，比如专家系统或逻辑程序，有明确的规则和推理过程。系统的行为完全由这些规则决定，因此是可预测和可解释的。但规则系统很脆弱，难以处理现实世界的复杂性和歧义。

另一方面，现代的深度学习系统，比如神经网络，从数据中学习。它们很灵活，可以处理复杂的、未见过的情况。但它们是黑盒，其行为难以解释。

Constitutional AI试图结合两者的优点。它有明确的规则（Constitution），但使用学习来实现这些规则。模型不是通过硬编码的逻辑来检查是否遵循原则，而是通过学习一个满足原则的输出分布。

这个方法的优雅之处在于它的灵活性。Constitution可以包含相对高级的、有时模糊的原则，比如"要尊重人类自主权"。模型通过学习来理解这个原则的含义，并在各种情况下适用它。这比试图用硬规则来编码每一个可能的场景要优越得多。

## 自我改进的潜力与风险

Constitutional AI打开了一个新的可能性：**模型的自我改进**。如果一个模型可以根据原则来评判和改进自己的输出，那么是否可以让它在没有外部干预的情况下继续改进？

这个想法有着深远的含义。从积极的一面，它可能导致无需人类参与的持续改进。从消极的一面，它引发了对失控的担忧。如果一个模型开始自我改进，我们还能控制它吗？它会坚守其原始的原则，还是会"进化"出新的、我们没有预见的目标？

这回到了AI安全研究中的一个经典问题，有时被称为**值锁定**（Value Lock）问题。一旦我们确定了一个模型应该遵循的值，我们如何确保它在自我改进时不会改变这些值？

Anthropic的立场是谨慎但乐观的。他们认为，只要原则是明确的并且被正确学习，模型就应该保持忠实于这些原则。但他们也承认这是一个开放的研究问题，需要继续的工作来完全理解和验证。

在实践中，这意味着Constitutional AI，虽然减少了人类反馈的需求，但并不意味着完全消除了监督。模型的输出仍然需要被评估，仍然需要进行人类监督，特别是在部署到现实世界之前。自我改进可能是一个长期的目标，但在当前阶段，它更多地是一个概念而非现实。

## 道德与哲学的维度

Constitutional AI的名称本身就暗示了其灵感的来源：国家的宪法。一个宪法是一个政治体系的基本法律和原则。类似地，一个AI系统的Constitution是其行为的基本原则。

但这个类比也揭示了Constitutional AI所面临的深层问题。制定国家宪法是一个政治过程，涉及广泛的公众参与和辩论。而Constitutional AI中的Constitution通常是由技术公司的研究团队单方面决定的。这是否民主？是否正当？

这回到了我们在RLHF讨论中提到的问题：**谁的价值观被嵌入到AI系统中？**无论是通过标注员的偏好还是通过明确的原则，这个问题都是关键的。

有人争论，既然AI系统会影响广泛的社会，其对齐原则应该由民主过程决定，而不是由技术公司决定。这可能意味着进行广泛的咨询、建立公众评论期、甚至进行投票。但这样的过程也很困难——如何在数百万人中达成共识？如何处理不同价值观的冲突？

另一个观点是，明确的Constitutional原则实际上提高了透明性和民主性。与其隐形地嵌入人类标注员的中位数偏见，不如明确地陈述原则，这样社会可以审视并批评。这鼓励了关于AI对齐的公开对话。

Constitutional AI也引发了关于**自主性**的哲学问题。当我们强制一个模型遵循一组原则时，我们是否在限制其"自由"或"自主"？或者这个问题对AI是否甚至适用？一个模型是否可以有真实的自由意志，还是它的所有行为，无论是通过原则还是通过学习，都是完全由其训练和架构决定的？

这些问题没有简单的答案。但Constitutional AI的出现强制我们面对它们。它不再是可能忽视的抽象哲学问题，而是影响数百万用户的真实系统的设计中的具体决策。

## 现实中的应用

Anthropic使用Constitutional AI来开发Claude，他们的大型语言模型助手。Claude已经被用于各种应用，从客户服务到内容生成。

在实践中，Claude根据其Constitution表现出了什么样的行为？据Anthropic的报告和公开测试，Claude表现出：明确拒绝参与非法或有害活动。对其知识的局限进行诚实的讨论。尽力提供有帮助和准确的信息。以对话性和尊重的方式与用户交互。

但也有批评者指出，即使有Constitution，Claude仍然不完美。有时候，它可能过度谨慎，拒绝回答看似无害的问题。有时候，它的拒绝可能不够一致。有时候，对话可能显得不自然或过度道歉。

这些观察表明，即使是Constitutional AI，也不是一个完美的解决方案。它改进了许多东西，但也引入了新的问题。这是对齐研究的常态——每一个新方法都有其优点和缺点。

## 前景与局限

Constitutional AI代表了对齐研究的一个有前景的方向。通过明确化原则和减少对人类反馈的依赖，它为构建可扩展的、透明的、可理解的对齐系统开辟了道路。

但它也有明显的局限。首先，原则的选择仍然涉及价值判断，这些判断可能不被所有人接受。第二，即使是明确的原则，模型对其的解释和应用仍然可能存在差异和错误。第三，Constitutional AI并不能完全解决AI安全的所有方面——即使一个模型遵循好的道德原则，如果其能力被用于有害目的，仍然可能造成伤害。

此外，还有一个关于**规模的问题**。Constitutional AI在当前规模的模型上看起来是有效的。但如果模型变得显著更大或更强大，情况会怎样？更强大的模型可能能够更好地理解和操纵其Constitutional原则。它可能找到遵循原则字面意思但违反其精神的方式。或者，它可能能够论证为什么应该改变这些原则。这些都是open research问题。

尽管有这些局限，Constitutional AI仍然代表了一个重要的进展。它展示了一个明确、可解释、可扩展的方式来思考和实现对齐。它转移了讨论的焦点，从"如何收集足够的人类反馈"转向"什么是好的AI行为的首要原则"。这个焦点的转移，可能与问题本身的解决方案一样重要。

