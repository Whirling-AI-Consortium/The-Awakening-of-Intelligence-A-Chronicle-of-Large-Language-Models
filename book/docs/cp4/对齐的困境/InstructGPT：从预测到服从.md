---
sidebar_position: 1
---



## 2022年3月的转折

2022年3月4日，OpenAI发布了一篇论文《Training language models to follow instructions with human feedback》。这篇论文描述了InstructGPT的开发过程，以及它如何通过RLHF从一个能力强大但不够有用的基础模型，转变成一个更小但更有帮助的模型。

这个发布的时间和方式很值得注意。在GPT-3发布一年半之后，OpenAI并没有急于宣布一个更大的模型。相反，他们发布了一个更小的、经过对齐的模型，并明确说明了为什么这很重要。这种策略转变本身就说明了什么。整个AI研究领域正在从对规模的痴迷，转向对对齐和实用性的关注。

InstructGPT实际上是多个大小的模型：1.3B、6B和175B参数的版本。最大的版本与原始的GPT-3大小相同。但最有趣的对比不是在大小之间，而是在能力之间。OpenAI在论文中展示的数据表明，一个6B参数的InstructGPT在人类评估中往往被认为优于175B的原始GPT-3。这个结果对整个行业产生了深远的影响。

这个对比背后的含义是深刻的。它表明，在某个点之后，单纯增加参数和计算不再是改进模型的最佳方式。相反，正确的训练方法、对人类偏好的对齐、可以带来更好的结果。这验证了RLHF不仅仅是一个有趣的研究方向，而是一个改变游戏规则的方法。

## 三阶段的对齐之旅

InstructGPT的训练过程被分为三个不同的阶段，每个阶段都有明确的目标和技术方法。这个三阶段流程后来成为了对齐研究的标准范式，并被采用到其他大模型的开发中。

**第一阶段是监督微调（Supervised Fine-Tuning, SFT）**。这个阶段相对直接：从OpenAI的API日志和内部数据中收集用户输入和专业标注员撰写的高质量输出。然后，用标准的监督学习来微调GPT-3，使其学会模仿这些高质量的输出。这个过程被称为行为克隆（Behavioral Cloning）——模型学会了复制人类示范的行为。

SFT数据集包含大约13,000个示例。这个数字看起来不多——与GPT-3的45TB预训练数据相比，这是微不足道的。但这恰好说明了一个重要的点：一旦模型已经通过大规模预训练获得了基础能力，进一步对齐它所需的数据量是相对较少的。SFT阶段通常需要进行几个epoch，使用相对较大的学习率。最终的结果是一个模型，其行为开始更接近人类期望的样子。

从技术角度，SFT看起来像标准的语言模型微调。但从目标角度，它是完全不同的。我们不再试图最小化在任意网络文本上的困惑度，而是试图学会遵循指令。这个转变虽然看似微妙，但实际上反映了AI从"通用语言模型"向"指令遵循助手"的转变。

**第二阶段是奖励模型（Reward Model, RM）的训练**。这个阶段最复杂，也是RLHF的核心。在这个阶段，使用第一阶段得到的SFT模型来生成对各种提示的多个响应。然后，让人类标注员对这些响应进行比较和排序。标注员不需要写出"正确"的答案，只需要判断哪个响应更好。

这个过程产生了成千上万的比较数据点。每个数据点都是一个三元组：（提示，较好的响应，较差的响应）。然后，用这些比较数据来训练一个奖励模型。奖励模型本质上是一个Transformer，其输出头被修改为输出单个标量分数，而不是next-token预测。目标是让这个模型学会预测人类偏好——给定任何模型的输出，奖励模型应该能给出一个分数，该分数与人类认为这个输出有多好的程度相关。

在OpenAI的论文中，他们收集了大约33,000个比较。这些比较来自于约40名人类标注员。这个规模足够大，使得奖励模型能学到稳健的人类偏好信号，但又足够小，不需要大规模的标注基础设施。

奖励模型的训练本身是一个有趣的问题。由于我们处理的是比较而不是绝对评分，一个常见的方法是使用Bradley-Terry模型，一个来自统计学的经典模型，用于从配对比较推断偏好。具体来说，给定两个输出 $y_w$ 和 $y_l$（其中w代表更好，l代表更差），我们希望最大化：

$$\log \sigma(r(y_w) - r(y_l))$$

其中 $r$ 是奖励函数，$\sigma$ 是sigmoid函数。这个目标函数鼓励模型为更好的输出分配更高的分数。

**第三阶段是强化学习（Reinforcement Learning）阶段**，这是应用RLHF的地方。在这个阶段，使用第二阶段训练好的奖励模型，来优化第一阶段的SFT模型。具体来说，我们生成新的模型输出，用奖励模型给这些输出评分，然后用PPO算法更新模型参数以最大化这个奖励。

这个阶段需要仔细平衡。如我们在RLHF部分讨论的，一个关键的技术是KL散度惩罚，防止模型偏离其SFT版本太远。OpenAI在论文中报告说，他们使用了 $\beta = 0.02$ 的KL系数，这意味着他们给予对齐的约束相对较高的权重。

整个三阶段过程看起来像这样：基础GPT-3 → 监督微调 → 生成与比较 → 奖励模型训练 → RL优化 → InstructGPT。每个阶段都为下一个阶段建立基础。

## 数据的质量与多样性

InstructGPT的成功很大程度上归功于用于训练它的数据的质量。这不是一个可以通过简单地增加数据量来解决的问题。相反，关键在于数据的精心选择和准备。

首先，**SFT数据的构成**。这些数据来自两个主要来源。一部分是从OpenAI API的使用日志中采样的真实用户查询。这确保了模型接触到真实的、多样的用户意图。另一部分是由标注员撰写的。这些"人工编造"的数据用来填补真实数据中的空白，并确保覆盖特定的能力和边界情况。

第二，**标注过程的指导**。OpenAI为标注员提供了详细的指导，说明什么是"好的"输出。这些指导包括：帮助性（模型应该尽力帮助用户），真实性（模型应该避免编造事实），和安全性（模型应该拒绝某些有害的请求）。值得注意的是，这些指导本身体现了对齐目标的价值判断。"好"不是由技术指标定义的，而是由这些社会价值定义的。

第三，**多样性与代表性**。InstructGPT团队意识到，不同的用户群体可能有不同的需求和偏好。为了确保模型不会偏向某个特定群体的偏好，他们试图在数据中包含各种用途和语言风格。这包括从简单的事实问答到创意写作，从编程帮助到生活建议。

但多样性也带来了挑战。当你有这么多不同类型的任务和偏好时，如何确保一个单一的奖励模型能够准确捕捉所有这些？答案是不完美的。论文中提到，即使经过了所有的努力，奖励模型仍然有明显的局限。有时候，它可能无法正确评估某些类型的输出。有时候，它会捕捉到一些虚假的相关性。

## 从基础模型到对齐模型的转变

InstructGPT代表了一个重要的范式转变。在此之前，大多数NLP研究的关注点是基础模型的能力——模型在各种基准上的表现如何。但InstructGPT强调的是对齐模型的用处——模型对用户有多有帮助。

这个转变的含义很广泛。首先，它改变了我们如何评估模型。传统的自动化指标（如BLEU、ROUGE、困惑度）变得不那么重要。相反，人类评估变得更加重要。OpenAI在论文中进行了大规模的人类评估，让多个评估者判断InstructGPT和原始GPT-3的输出。结果显示，InstructGPT在大多数任务上都被更高比例的评估者偏好。

其次，它改变了我们如何思考模型的"好坏"。一个基础模型的"好坏"可能取决于它在下游任务上的微调性能。但一个对齐模型的"好坏"取决于用户的满意度。这两者之间可能有很大的差异。

第三，它为后续的应用打开了大门。当一个模型被对齐后，就更容易将其部署到实际的用户产品中。不需要花费大量工程努力来处理模型的怪癖或不希望的行为。从某种意义上说，对齐是模型从研究实验室走向产品的必要一步。

## 对安全与有帮助的权衡

InstructGPT的训练过程中，一个不断出现的主题是安全与有帮助性之间的权衡。这两个目标有时是相容的，但有时会冲突。

**有帮助性**指的是模型应该尽力满足用户的请求，提供相关、准确和有用的信息。这是一个模型的核心目的。

**安全性**指的是模型应该避免生成有害的内容。这可能包括拒绝非法的请求、避免生成偏见的内容、避免帮助进行有害活动。

在许多情况下，这两个目标是一致的。一个模型既有帮助又安全是最好的。但在边界情况下，它们可能会冲突。例如，如果用户问一个可能被用于有害目的的技术问题，模型应该如何回应？完全回答会很有帮助，但可能不安全。拒绝回答会很安全，但可能不有帮助。

OpenAI的方法是给予这两个目标一定的权重，并在训练过程中平衡它们。在标注指导中，标注员被告知应该同时考虑有帮助和安全。在某些情况下，安全会被给予更高的权重。在其他情况下，有帮助可能会被给予更高的权重。

这种平衡的选择本身是一种价值判断。不同的社会或组织可能会做出不同的选择。有些可能更强调安全，有些可能更强调帮助。这正是对齐的一个关键挑战——没有单一的"正确"答案，而是需要在多个竞争的目标之间进行有意识的权衡。

## 从InstructGPT到ChatGPT

虽然InstructGPT本身主要通过OpenAI的API提供给有限的用户，但它的影响远远超出了API的范围。InstructGPT的成功直接导致了ChatGPT的开发。

ChatGPT在本质上是InstructGPT的思想的推广和改进。它使用了类似的三阶段训练过程，经过对齐，并针对对话交互进行了特别优化。但ChatGPT在几个重要方面进行了改进。首先，它可能使用了更多的数据和更多的计算资源。其次，它引入了更新的优化技术和架构改进。第三，它被设计为一个通用的、易于使用的对话界面，而不仅仅是一个API。

ChatGPT的发布在2022年11月，不到一年之后。它迅速成为了历史上增长最快的应用，达到了一百万用户的速度远快于任何之前的应用。从某种意义上说，ChatGPT的成功证实了InstructGPT所展示的：用户渴望一个有帮助、对齐的AI助手。

这个转变也标志着整个AI行业的一个转折点。在ChatGPT之前，大型语言模型主要是一个学术和技术兴趣。在ChatGPT之后，它们成为了一个广泛的社会现象。这个转变不是因为有一个更大的模型或一个更聪明的想法——虽然ChatGPT确实有技术上的改进——而是因为一个对齐的、易于使用的、对用户有帮助的模型最终被创造出来。

## 反思与局限

虽然InstructGPT在许多方面是成功的，但它也揭示了对齐的一些持久的挑战。

**价值的传达与学习**。InstructGPT是通过标注员的偏好来学习人类价值的。但这意味着，奖励模型本质上是在学习这一特定标注员群体的偏好。这个群体可能不能代表所有用户，更不能代表全球多样的价值观。因此，虽然InstructGPT可能非常适合某些用户，但对其他用户来说可能不那么合适。

**对齐的稳定性**。对齐是通过持续的优化过程实现的。但一旦优化停止，模型是否会保持对齐？在面对新的、未见过的情况时，模型是否仍然会表现得好？这些问题仍然没有完全的答案。

**可解释性与可控性**。虽然我们知道InstructGPT的行为与用户偏好更加一致，但我们仍然不能完全解释为什么它的行为是这样的。这种缺乏可解释性可能会在高风险应用中导致问题。

**扩展到更复杂的任务**。InstructGPT在相对直接的任务上表现良好，比如回答问题或撰写内容。但对于更复杂的任务，比如长期规划或处理有歧义的需求，对齐变得更困难。

尽管有这些局限，InstructGPT仍然代表了对齐研究的一个重要里程碑。它不仅证明了对齐在原则上是可能的，而且证明了它在实践中是有价值的。它为后续的研究指出了方向：对齐不是一个可选的特性，而是构建真正有用的AI系统的必要条件。

## 更广泛的影响

InstructGPT的发布对整个AI行业产生了深远的影响，超出了语言模型的范围。

首先，它改变了公司的策略。在InstructGPT发布之前，许多AI公司的策略是不断增加模型的规模，相信规模会导致能力的提升。但InstructGPT清楚地表明，对齐和指令遵循可能与规模一样重要，甚至更重要。这导致了Google、Meta等公司对他们自己的大模型进行更多的对齐工作。

其次，它提高了社会对AI对齐的认识。在InstructGPT发布之前，对齐还主要是一个AI安全社区的关注点。但当OpenAI明确指出对齐如何改进了模型的实用性时，更广泛的AI社区开始注意到这个问题。

第三，它为一个新的AI子领域建立了基础——指令微调和对齐。许多后续的研究工作都是基于InstructGPT所建立的框架。学术论文开始研究更好的对齐方法，不同的模型发布时都包含了对齐的步骤。

## 结论

InstructGPT代表了一个转折点。在这一点之前，AI研究主要关注能力——模型能做什么。在这一点之后，关注转向了对齐——模型应该怎么做。这个转变不仅改变了研究的方向，也改变了AI如何被部署和使用。

从技术角度，InstructGPT展示了一个清晰的三阶段流程来实现对齐。从商业角度，它证明了对齐的价值——一个对齐的小模型可能比一个未对齐的大模型更有用。从社会角度，它为一个更加人道的、有帮助的AI开辟了道路。

最终，InstructGPT不仅仅是一个更好的GPT-3。它是一个关于AI未来方向的声明——未来的AI系统应该被设计成不仅强大，而且有帮助、诚实、安全。这个声明，通过InstructGPT的成功和随后ChatGPT的爆炸性增长，最终得到了整个社会的响应。