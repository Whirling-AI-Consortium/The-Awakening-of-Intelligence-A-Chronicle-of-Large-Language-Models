---
sidebar_position: 4
---

### 固定窗口的局限

Word2Vec的成功是巨大的，但它也揭示了一个根本性的限制。所有这些漂亮的词向量，无论是通过Skip-gram、CBOW还是其他方法学到的，都有同一个致命缺陷：它们是**固定的**。

每一个词对应一个向量，不管这个词出现在什么上下文。"bank"在"river bank"（河岸）和"savings bank"（储蓄银行）中获得的是同一个向量。一个固定的词向量必须在这两个完全不同的含义之间进行某种妥协，最终学到的是一个模糊的、介于两者之间的表示。

更根本的问题是，固定的词向量无法捕捉**长距离的依赖关系**。考虑这样一个句子："The bank that I opened with my friend five years ago was very successful." 在这里，"bank"的含义不仅取决于它周围的几个词，还取决于整个句子的背景。一个固定的词向量，特别是那些只在有限窗口（比如±2个词）上训练的词向量，根本无法获得这样的全局上下文。

在Bengio和Mikolov的模型中，虽然他们创新的地方在于用神经网络学习词的分布式表示，但他们仍然受困于一个硬约束：上下文窗口的大小。在Bengio的模型中，输入必须是固定的N个词。这使得这些方法只能看到有限的上下文，无论N设置为多大。

在2010年代初期，一些研究者尝试了各种方法来突破这个限制。有人用卷积神经网络处理句子，试图用更大的感受野来捕捉更广的上下文。但这些方法都有一个共同的问题：它们最终学到的表示仍然无法有效地处理任意长的序列，无法捕捉真正的长距离依赖关系。

### 记忆中的循环

解决这个问题的关键想法并不新颖。事实上，它的根源可以追溯到神经网络理论的早期。

在1990年，Jeffrey Elman发表了一篇论文《在时间中寻找结构》（Finding Structure in Time）。这篇论文提出了一个看似简单但深刻的想法：如果一个神经网络的输出可以反馈到它自己的输入，形成一个循环，会怎样？这样的网络可以利用这些反馈连接来维持一种"记忆"或"内部状态"，这个状态可以随着时间序列的处理而改变。

这就是**循环神经网络**（Recurrent Neural Network，RNN）的起源。Elman的想法很快受到其他人的关注。在1997年，Sepp Hochreiter和Jürgen Schmidhuber发表了一篇后来被证明极其重要的论文《长短期记忆》（Long Short-Term Memory），提出了LSTM单元。

为什么LSTM如此重要？因为它解决了RNN的一个根本性问题：**梯度消失问题**（Vanishing Gradient Problem）。这是一个复杂的技术细节，但核心思想是这样的：当你用反向传播算法训练RNN时，误差信号需要从输出一路反向传播回输入。但在很长的序列中，这个误差信号会变得越来越小，最终变成接近零，就像一个极小的数不断被乘以介于0和1之间的数。经过足够多的乘法后，这个数会变得不可检测地小。这意味着，RNN很难学到对长距离的依赖关系——因为用于学习这些依赖关系的梯度已经消失了。

LSTM通过引入一个巧妙的结构来解决这个问题。它使用了**记忆单元**（Memory Cell）和**门机制**（Gate Mechanism）。记忆单元可以在时间步之间保持信息，而门机制则控制信息的流动。最重要的是，这个结构使得误差信号可以更有效地反向传播，梯度不会像在普通RNN中那样急速消失。

从1990年代末开始，RNN和LSTM就已经被开发出来了。但它们为什么在NLP社区中没有立即产生革命性的影响？最直接的原因是计算成本。训练RNN和LSTM很慢，特别是在处理长序列时。在CPU时代，这意味着训练一个中等规模的RNN模型可能需要数周。

但到了2010年代初，情况开始改变。GPU的计算能力不断提升，深度学习框架逐渐成熟。突然之间，训练RNN变成了可行的。Tomas Mikolov等人在2010年的论文中展示了如何用RNN来构建语言模型。Alex Graves在2013年的工作中展示了RNN可以用来做序列标注和其他任务。人们开始意识到，这些"旧"的循环网络在现代硬件上可以做一些非常强大的事情。

### 从固定到可变：序列到序列模型

但问题仍然存在：RNN虽然可以处理任意长的序列，但它如何处理从一个序列到另一个序列的转换呢？比如说，如何用神经网络实现机器翻译——把英文句子转换成中文句子？

这两个句子可能有不同的长度，不同的结构，甚至在语义上可能有很大的差异。你不能简单地用一个固定大小的向量来表示一个任意长的句子。或者说，你可以，但你需要一个巧妙的方法来做到这一点。

这就是**序列到序列**（Sequence-to-Sequence，简称Seq2Seq）模型的想法。这个想法的核心是这样的：用一个RNN（称为**编码器**）来读取整个输入序列，逐词逐词地处理它，同时维持一个内部状态。这个内部状态在处理完整个输入序列后，就浓缩了输入序列中的所有信息。然后，你用另一个RNN（称为**解码器**）来生成输出序列，这个解码器以第一个RNN的最终状态作为初始化。

形式上，假设输入序列X有Tx个单词，输出序列Y有Ty个单词：

$$X = \{x_1, x_2, ..., x_{T_x}\}, \quad Y = \{y_1, y_2, ..., y_{T_y}\}$$

编码器RNN逐步处理输入，计算隐藏状态序列：

$$h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t)$$

这里h_t是第t时刻的隐藏状态，它包含了从第一个词到第t个词的信息。处理完整个输入序列后，最后一个隐藏状态$h_{T_x}$被作为一个**上下文向量**c，传递给解码器：

$$c = h_{T_x}$$

注意这个上下文向量没有时间索引——它是一个固定的向量，包含了整个输入序列的信息。这个向量就像是编码器对"我看到了什么"的总结。

然后解码器开始工作。它维持自己的隐藏状态序列，这个序列的初始化包含上下文向量c。解码器的循环关系是：

$$s_t = f_{dec}(s_{t-1}, y_{t-1}, c)$$

其中s_t是解码器在第t时刻的隐藏状态，$y_{t-1}$是前一时刻生成的词，c是来自编码器的固定上下文向量。解码器用这个隐藏状态来生成下一个词的概率分布，然后通过采样或贪心选择来生成下一个词。

整个模型通过最大化对数似然来训练：

$$\log p(Y) = \sum_{t=1}^{T_y} \log p(y_t | y_{1:t-1})$$

这正是我们之前看到的自回归目标函数，现在应用到输出序列上。整个模型（包括编码器和解码器的所有权重）通过反向传播算法端到端地训练。

### 从想法到实践

这个想法听起来优雅而合理，但它的实现需要时间。2013年，Kalchbrenner和Blunsom发表了《递归连续翻译模型》（Recurrent Continuous Translation Models），这是第一篇提出完整编码器-解码器架构的论文。他们的编码器使用了卷积神经网络，而解码器使用了RNN。这是一个创新，但有一个明显的改进空间：为什么编码器不也用RNN呢？

第二年，2014年，Cho等人在《使用RNN编码器-解码器进行统计机器翻译的短语表示学习》中提出了一个更对称的架构：两个RNN，一个作为编码器，一个作为解码器。同年，Sutskever等人在《序列到序列学习与神经网络》中发表了类似的模型，但使用了LSTM而不是普通RNN，因为LSTM对处理长距离依赖更有效。

Sutskever在这篇论文中直接提到了Kalchbrenner，但指出了一个关键的改进：

> "我们的工作与Kalchbrenner和Blunsom密切相关，他们是第一个将输入句子映射到向量再映射回句子的人，尽管他们使用卷积神经网络映射句子到向量，这会丢失词的顺序。"

这个简洁的历史陈述清晰地展现了这个领域的进化过程。前人走过的路直接导向了更好的方向。

### 序列到序列的力量

Seq2Seq模型的美妙之处在于它的通用性。一旦你有了这样一个框架，可以处理来自不同长度序列的映射，你就可以解决许多不同的问题。

机器翻译是最直观的应用。输入是一个英文句子，输出是对应的中文翻译。但这个框架同样适用于其他许多问题。文本摘要可以被框架化为：输入一长段文本，输出一个更短的摘要。对话系统可以被框架化为：输入一个用户的问题，输出一个相关的回答。问题回答、风格转换、甚至代码生成——所有这些都可以被视为序列到序列的映射。

这个框架的广泛适用性使得它成为了2014年到2016年间NLP领域的主流方法。许多学术论文、许多工业应用都建立在Seq2Seq的基础上。在这个时期，深度学习真正开始在NLP中占据主导地位，取代了之前基于特征工程的方法。

### 瓶颈：信息的丧失

想象一下，你需要把一个长的英文句子翻译成中文。句子有可能有20个词、30个词，甚至更长。编码器需要把所有这些信息压缩成一个单一的上下文向量c。

这里有个问题：**信息压缩过程中必然会丧失信息**。

考虑一个具体的例子。假设输入是"The cat sat on the mat"（猫坐在垫子上）。编码器逐词处理这个句子，最后的隐藏状态包含了什么？理论上，它应该"记住"所有的词。但实际上，由于RNN的顺序处理方式，最后的隐藏状态更多地编码的是句子的后面部分。前面的信息虽然会通过隐藏状态的递推传递下去，但会逐步被后面的词所影响和覆盖。

这个现象叫做**长距离依赖问题**（Long-range Dependency Problem）。当解码器试图生成翻译时，它是基于这个可能已经"遗忘"了句子开头的上下文向量。结果，长句子的翻译质量往往比短句子差得多。

这个问题在机器翻译中变成了一个著名的挑战。研究者们尝试了各种方法来改进，比如使用双向编码器（Bidirectional Encoder）来同时从前向后处理输入，或者改进隐藏状态的初始化。但根本问题仍然存在：一个固定大小的向量无法完美地编码任意长的序列。

2015年，Bahdanau、Cho和Bengio提出了一个优雅的解决方案：**注意力机制**（Attention Mechanism）。这个想法很简单但深刻：与其在解码时依赖一个单一的上下文向量，不如让解码器可以"看到"编码器的所有隐藏状态，并学习对不同的隐藏状态赋予不同的"注意力"权重。

当解码器生成每个输出词时，它会基于当前的解码状态计算一个权重向量，这个权重向量表示它应该"关注"编码器的哪些隐藏状态。然后，它计算编码器隐藏状态的加权平均作为该时刻的上下文。这样，解码器就可以动态地关注输入序列的不同部分。

注意力机制的出现标志着NLP进入了一个新的阶段。它不仅解决了序列到序列模型的瓶颈问题，更重要的是，它暗示了一个更深刻的真理：**也许完整的语言建模不需要复杂的循环结构，而只需要一个能够在序列中"看到"远距离关系的机制。**

这个观察最终导向了我们故事中最关键的时刻——2017年的Transformer。

但在我们到达那里之前，有一个重要的中间步骤需要跨越。注意力机制虽然优雅，但它仍然嵌入在RNN的循环结构中。循环处理的本质没有改变——你仍然需要一个词一个词地处理序列。这意味着，即使有了注意力，模型仍然无法充分并行化。GPU的威力无法完全释放。

直到有人问了一个革命性的问题：**我们真的需要循环结构吗？**

### 从序列到下一步

到2016年中期，深度学习在NLP中的成功已经无可否认。RNN、LSTM和Seq2Seq模型已经成为主流。许多研究者正在改进这些模型，添加各种机制和优化。但在这个相对的繁荣中，也有一些研究者在思考更根本的问题。

其中一个问题是：如果注意力机制如此有效，它是否可以成为唯一的机制？如果我们有一个完全基于注意力的架构，而不是RNN加上注意力的附加品，会怎样？

另一个问题是：RNN的固有的顺序处理方式是否是必要的？能否设计一个架构，允许全面的并行化，同时仍然能够捕捉序列数据中的长距离依赖？

这两个问题的交点，就是Transformer的诞生地。

在我们继续这个故事之前，让我们停下来回顾一下我们走过的路。从Bengio的固定窗口模型，到Mikolov的Word2Vec破解了廉价词向量的秘密，再到RNN/LSTM展示了如何处理任意长的序列，再到Seq2Seq证明了我们可以用神经网络做复杂的序列到序列映射，最后到注意力机制开始改变我们对序列处理的认识。每一步都解决了前一步的某个问题，但也暴露了新的问题。这就是科学进步的方式——通过不断的迭代、改进和根本性的重新思考。

现在我们终于准备好理解Transformer了。当你看到Transformer的设计时，你会意识到，它不是凭空出现的。它是这十多年来积累的所有洞察、解决方案和技术的一个高潮。而最关键的是，它通过一个激进的想法——"注意力就是你所需的一切"——改变了我们对序列建模的根本理解。