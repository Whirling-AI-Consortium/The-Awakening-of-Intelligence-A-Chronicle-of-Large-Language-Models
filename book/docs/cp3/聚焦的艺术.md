---
sidebar_position: 5
---

### 固定上下文向量的瓶颈

但RNN编码器-解码器模型有一个根本的问题，这个问题在使用它的过程中逐渐被发现。

在2014年，Cho等人发表了一篇关键的分析论文《神经机器翻译的性质：编码器-解码器方法》。在这篇论文中，他们做了一个简单但重要的实验：用RNN编码器-解码器模型翻译不同长度的句子，然后测量翻译质量。结果令人沮丧：

> 我们的分析表明，当源句子的长度增加时，神经机器翻译模型的性能快速下降。

论文中进一步指出：

> 最明显的解释假设是，固定长度的向量表示没有足够的容量来编码具有复杂结构和含义的长句子。

这个观察触及了问题的本质。编码器再怎么强大，都要把整个变长的输入序列压缩成一个固定大小的向量。这个向量要保留源句子的所有重要信息，同时还要足够"紧凑"以便解码器处理。这是一个本质上的矛盾。

当句子很短时，这个压缩还可以接受。但当句子变长时，信息损失就变得不可接受。一个固定大小的向量，就像是一个有限容量的瓶子，你无法把一条河都倒进去。

### 软搜索的想法

2014年，一篇论文的发表改变了这个局面。这篇论文来自Google Brain，标题是《通过联合学习对齐和翻译的神经机器翻译》（Neural machine translation by jointly learning to align and translate），作者是Dzmitry Bahdanau、Kyunghyun Cho和Yoshua Bengio。

有趣的是，这篇论文在标题和正文中几乎没有使用"attention"（注意力）这个词。相反，他们用了一个不同的框架来思考这个问题——他们把它看作一个**搜索问题**：

> 在本文中，我们猜测固定长度向量的使用是改进这个基本编码器-解码器架构性能的瓶颈，我们提议通过允许模型自动进行（软）搜索源句子中与预测目标词相关的部分，来扩展这个架构，而无需将这些部分显式地形成为硬分割。

这个想法有多深刻？让我们想象一个翻译的场景。假设模型正在生成中文翻译的第5个词。在这个时刻，它不应该平等地关注英文句子的所有部分。相反，它应该能够"搜索"到英文句子中最相关的部分，然后基于那个部分来生成翻译。就像你在翻译时，当翻译到某个词时，你会回头看英文原文的相应部分一样。

Bahdanau的论文首次在NLP中系统地实现了这个想法。虽然注意力机制在计算机视觉领域已经被使用过（比如在2010-2014年的视觉追踪论文中），但Bahdanau是第一个将其成功应用到自然语言处理的。

### 注意力机制的核心

让我们深入理解Bahdanau提出的注意力机制是如何工作的。

在有注意力的RNN编码器-解码器中，关键的改变是：**解码器不再只依赖一个固定的上下文向量c，而是在每个解码步骤i都生成一个新的上下文向量c_i**。

这个新的上下文向量是什么？它是编码器所有隐藏状态的**加权组合**：

$$c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_j$$

这里的$α_ij$是一个权重，表示在解码第i个词时，应该对编码器的第j个隐藏状态赋予多少"注意力"。

解码器的递推关系也相应地改变了：

$$s_i = f_{dec}(s_{i-1}, y_{i-1}, c_i)$$

注意这里多了$c_i$这一项。这意味着，解码器在每一步都可以基于当前的解码状态，决定应该关注编码器的哪些部分。

现在的问题是：这些权重$α_{ij}$是如何计算的？

Bahdanau的方法是用一个小的神经网络来计算。首先，计算一个"相似度得分"：

$$z_{ij} = \tanh(W_a s_{i-1} + U_a h_j)$$

然后，用一个向量$v_a$把这个隐藏状态转换为一个标量得分：

$$e_{ij} = v_a^\top z_{ij}$$

最后，对所有的$e_{ij}$进行softmax归一化，得到权重：

$$\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}$$

这整个过程可以看作是一个"软搜索"。模型学会了如何根据当前的解码状态，对编码器的不同部分赋予不同的权重，从而动态地选择关注的内容。这解决了固定上下文向量的瓶颈。

### 为什么叫"对齐"？

Bahdanau的论文标题中有个词特别有意思："对齐"（align）。为什么用这个词？

因为这个权重矩阵α实际上可以被解释为源句子和目标句子之间的对齐方式。如果α_ij很大，这意味着生成目标句子的第i个词时，应该查看源句子的第j个词。通过观察这个对齐矩阵，研究者们可以看到模型是如何把源语言的词对应到目标语言的词的。

这不仅有助于理解模型的行为，也提供了一种评估模型是否学到正确语义的方法。事实上，许多后续的研究都利用这个对齐矩阵来改进翻译质量。

### 注意力机制的类型

Bahdanau的论文发表后，注意力机制在NLP中迅速流行起来。但研究者们很快意识到，有许多不同的方式来实现注意力。

2015年，Minh-Thang Luong等人发表了《有效的基于注意力的神经机器翻译方法》（Effective approaches to attention-based neural machine translation）。这篇论文进行了系统的研究，提出了注意力机制的多个维度。

**第一个维度：全局 vs 局部注意力**

在Bahdanau的原始模型中，解码器的每个词都可以关注编码器的所有词。这叫做**全局注意力**（Global Attention）。但有时候，我们可能只想让解码器关注编码器的一个局部窗口，这叫做**局部注意力**（Local Attention）。比如，在翻译时，翻译第10个词时，我们可能只需要看源句子的第8-12个词，而不需要看整个句子。

局部注意力的优势是计算效率更高。全局注意力需要对所有编码器隐藏状态进行softmax，但局部注意力只需要对一个窗口内的隐藏状态进行。

**第二个维度：得分函数的选择**

给定一个解码器隐藏状态$s_{i-1}$和一个编码器隐藏状态$h_j$，我们如何计算它们的相似度？有多个选择。Luong等人提出了三个主要的得分函数：

1. **点积注意力**（Dot-product Attention）：最简单的选择是直接计算两个向量的点积：
$$e_{ij} = h_j^\top s_{i-1}$$

2. **一般注意力**（General Attention）：用一个学习的权重矩阵W_a：
$$e_{ij} = h_j^\top W_a s_{i-1}$$

3. **加性注意力**（Additive Attention）：这就是Bahdanau原始提出的方法：
$$e_{ij} = v_a^\top \tanh(W_a h_j + U_a s_{i-1})$$

这三种得分函数在经验上表现都不错，没有明显的赢家。不过，在今天的视角看，点积注意力最终成为了主流，因为它计算效率最高，而且有直观的解释——两个向量点积衡量了它们的相似性。

**第三个维度：查询、键和值**

在研究注意力机制时，信息检索的概念非常有用。在一个搜索系统中，有三个关键的要素：

- **查询**（Query）：用户提出的问题，比如"离我近的印度餐厅"
- **键**（Key）：系统用来搜索的元数据，比如餐厅的描述、评分、距离等
- **值**（Value）：系统返回的结果，比如餐厅本身

用这个框架理解Bahdanau的注意力机制：解码器隐藏状态s_i是查询（我们要找什么），编码器隐藏状态h_j既是键也是值（我们从中搜索）。

但这个框架的威力在于，它暗示了一个可能性：**查询、键和值不必来自不同的来源，它们也可以来自同一个来源！**

### 自注意力的出现

这个观察导向了一个深刻的概念：**自注意力**（Self-Attention）。

在跨注意力（Cross-Attention）中，查询来自一个来源（比如解码器），键和值来自另一个来源（比如编码器）。但在自注意力中，查询、键和值都来自同一个序列！

形式上，假设我们有一个隐藏状态序列$h_1$, $h_2$, ..., $h_T$，我们可以定义：

$$q_i = f_q(h_i)$$
$$k_j = f_k(h_j)$$
$$v_j = f_v(h_j)$$

然后，注意力权重可以定义为：

$$\alpha_{ij} = \text{softmax}(\text{score}(q_i, k_j))$$

上下文向量是：

$$c_i = \sum_j \alpha_{ij} v_j$$

这意味着，序列中的每个位置都可以"看到"并关注序列中的其他所有位置（以及自己）。这允许模型在序列内部建立长距离的依赖关系。

据我所知，第一篇在NLP中使用自注意力的论文是2016年的《用于机器阅读的长短期记忆网络》（Long short-term memory-networks for machine reading）。这篇论文相当复杂，但关键的观察是，自注意力可以被应用于序列编码器内部，允许网络发现序列中不同位置之间的关系。

### 2017年的准备

到了2017年中期，注意力机制已经被广泛研究。局部、全局、加性、点积、自注意力、跨注意力——各种形式的注意力都在被探索。论文《结构化自注意力句子嵌入》（A structured self-attentive sentence embedding）和《双向注意力流进行机器阅读理解》（Bidirectional attention flow for machine comprehension）等工作都在使用注意力机制。

但有趣的是，注意力并不被视为深度学习在NLP中的主流方向。2017年3月，就在Transformer发表前几个月，Google Brain发表了《神经机器翻译架构的大规模探索》（Massive exploration of neural machine translation architectures）。这篇论文系统地比较了各种NMT架构。在他们的六个主要结论中，只有一句话提到注意力：

> 参数化加性注意力产生了整体最佳的结果。

注意，论文中提到的是加性注意力，这恰好不是后来Transformer使用的点积注意力！

在2017年中期，许多研究者仍然在努力改进RNN。有的论文在研究如何在规模上训练更大的RNN。有的论文探索不同的循环单元设计。深度学习在NLP中终于证明了自己，但还没有形成清晰的共识关于下一步应该朝哪个方向走。

### 那个特殊的时刻

但是，历史转折即将到来。

有一个细节值得注意：在所有这些关于注意力的工作中，注意力仍然被嵌入在RNN中。序列仍然需要逐步处理。虽然注意力机制解决了长距离依赖的问题，但它没有解决RNN的另一个根本限制：**缺乏并行性**。

RNN天生是顺序的。你必须处理第一个词，然后才能处理第二个词。即使有最好的GPU，这也意味着处理一个100词的句子需要100步。在深度学习时代，当训练数据以数十亿计时，这个顺序处理的瓶颈变得越来越明显。

但没有人知道如何在不放弃序列结构理解的前提下，消除这个顺序处理。或者更准确地说，没有人知道如何做到这一点——直到2017年6月。

一篇论文的发表改变了一切。这篇论文的标题简洁而宏伟：《注意力就是一切所需》（Attention Is All You Need）。

它来自Google Brain和Google的研究者，作者包括Ashish Vaswani、Sharan Katagiri、Lukasz Kaiser和Illia Polosukhin，还有其他人。这篇论文提出了一个完全新的架构，一个既不使用循环也不使用卷积、而是完全基于注意力机制的架构。

他们叫它**Transformer**。

而这，标志着大语言模型时代真正的开始。