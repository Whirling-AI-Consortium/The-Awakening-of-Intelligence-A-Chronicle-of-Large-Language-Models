---
sidebar_position: 3
---


### 简化的力量

AlexNet的成功鼓舞了NLP研究者们。很快，他们开始尝试用神经网络来重新解决语言问题。但他们很快发现，Bengio 2003年提出的神经概率语言模型有一个致命的弱点：**它太贵了**。

让我们更仔细地看看为什么。在Bengio的模型中，输入是一个固定大小的N个词的上下文窗口，比如N=10。每个词用一个D维的向量表示。这些向量被连接起来形成一个ND维的输入向量。然后这个输入通过一个有隐藏层的神经网络：

首先是一个线性层，将输入投影到隐藏层。这涉及一个V×ND的矩阵和一个输入向量的乘法，其中V是词汇表大小——通常100万左右。然后是一个非线性激活函数（tanh）。最后是另一个线性层，从隐藏层投影到V维的输出，代表每个可能词的概率。

看起来还好。但魔鬼在细节中。关键的瓶颈是这一步：计算输出的概率分布。为了得到p(wt|wt-N:t-1)，模型必须对词汇表中的所有100万个词都计算一个分数，然后通过softmax函数将这些分数转换为概率。Softmax函数涉及指数计算，所以复杂度是O(V)，其中V是词汇表大小。

在实际训练中，这意味着什么？假设隐藏层大小H=500，N=10，D=50，词汇表V=100万。对于语料库中的每一个词，你需要执行的计算大约是：

- 输入到隐藏层的计算：O(ND) = O(500)
- 隐藏层到输出层的计算：O(VH) = O(5亿)
- Softmax的正规化：O(V) = O(100万)

支配性的项是VH，这意味着对于每个词，你需要进行数亿次浮点运算。假设你有一个10亿词的训练语料库，这就是10^18次浮点运算。在2010年代初的硬件上，这意味着训练可能需要数周。

这不仅仅是一个学术上的问题。这是一个实际的障碍。在工业界，没有人有时间等待数周去训练一个语言模型，尤其是当更快的N-gram方法已经足够好用的时候。所以，虽然Bengio的想法在概念上是深刻的，但在实践上，它仍然被认为是不切实际的。

2013年，当Tomas Mikolov和他的同事们发表两篇关于Word2Vec的论文时，他们采取了一个完全不同的策略。他们不是试图优化Bengio的完整模型，而是**简化它**。

他们做的第一件事是移除隐藏层。直接从输入词向量计算输出。这听起来像是退步，但实际上这是一次关键的权衡。没有隐藏层意味着模型是"对数线性的"（log-linear），即词的条件概率的对数与输入向量的点积成线性关系：

$$\log p(w_{t+j}|w_t) = \langle u_{I(w_{t+j})}, c_{I(w_t)} \rangle - Z$$

其中Z是一个正规化常数，我们暂时忽略它。这个简单得多。

第二件事是处理softmax的瓶颈。Mikolov和他的同事们使用了两种技巧。第一种叫**分层softmax**（Hierarchical Softmax），它用一个二叉树结构来代替对所有V个词的枚举，将复杂度从O(V)降低到O(log₂V)。对于100万词的词汇表，log₂(1000000)≈20，这是100万的五万分之一。

第二种技巧更激进，叫**负采样**（Negative Sampling）。其基本思想是，与其试图对所有100万个词进行softmax正规化，不如只选择一小部分"负样本"（比如K=10个），然后训练模型区分真实的目标词和这些噪声词。理论证明表明，这种方法会收敛到正确的概率，即使你从不显式计算正规化常数。这就像是在玩"找不同"的游戏：模型学会识别真正的词序列和虚假的词序列，这自动地教会了它语言的规律。

通过这些优化，计算复杂度从O(VH)降低到大约O(K·D)，其中K是你选择的负样本数（比如20）。对于相同的参数设定，这是数百万倍的加速。突然之间，在一台普通的计算机上，在几小时内训练一个词向量模型成为了可能。

### 两个模型的并行演进

Mikolov等人提出的Word2Vec实际上包含两个相似但互补的模型：**连续词袋模型**（CBOW）和**跳跃图模型**（Skip-gram）。

在CBOW中，模型的任务是用周围的词来预测中间的目标词。你给它前两个词和后两个词，它尝试猜测中间的词。这就像是在读一篇文章时，某个词被遮挡了，你根据前后文推测它是什么。

在Skip-gram中，方向相反。模型的任务是用一个中心词来预测它周围的词。你给它一个词，它尝试预测周围的词。这就像是在听一个人讲话时，从一个关键词推测他可能还会说哪些词。

有趣的是，两个模型在实践中都很有效。研究者们原本期望其中一个会明显优于另一个，但事实证明两个都行得通。最终，Skip-gram在后续的研究中被证明更有用，特别是在处理不常见词汇时。但这些区别并不是核心要点。核心要点是：这两个模型都非常简单，都可以极其快速地训练。

而且它们都做同样的事情——学习词的分布式表示。

### 语言的几何规律

Word2Vec最令人惊讶的地方不在于它的速度，而在于它学到的词向量具有某种神奇的性质。

2013年，Mikolov等人发表了第三篇论文，标题是《连续空间词表示中的语言规律》。在这篇论文中，他们做了一个观察：许多语言中的语义和句法关系对应于词向量空间中大约恒定的向量偏移。

换句话说，语言的规律可以用几何关系来表示。

最著名的例子是这样的：

$$\text{vec("king")} - \text{vec("man")} + \text{vec("woman")} ≈ \text{vec("queen")}$$

这个公式说的是，如果你取"king"（国王）的向量，减去"man"（男人）的向量，再加上"woman"（女人）的向量，你会得到一个非常接近"queen"（皇后）向量的向量。

或者用另一个例子：

$$\text{vec("Russia")} + \text{vec("river")} ≈ \text{vec("Volga")}$$

俄罗斯加上河流，你得到伏尔加河。

这听起来像魔法。但更重要的是，这是令人惊讶的。想象一下，如果你在2003年读到Bengio的论文，有人问你："你认为一个训练来进行下一个词预测的简单神经网络会学到这样的关系吗？"大多数人的答案可能是："嗯……也许吧？但我不能确定。"

让我们停下来真正理解这意味着什么。Word2Vec是一个非常简单的模型。它没有深度，没有复杂的架构，没有精心设计的规则。它所做的一切就是看成对的词——一个中心词和它周围的词——然后用点积学习两个向量之间的相似度。这就像是在学习一个座位图：哪些词经常坐在一起？

但从这个简单的任务中，模型自动地学到了语言中的深层结构。它学到了语法性别的概念（国王-男人+女人=皇后）。它学到了地理关系（俄罗斯+河流=伏尔加河）。它甚至学到了拼写和词形的规律。

这是一个深刻的发现。它表明，语言不是任意的符号的集合。在某种深层的意义上，语言具有规律的结构。这种结构不需要被显式地教授——机器学习模型可以从数据中自动发现它。

### 历史背景：分布式表示的确认

需要指出的是，Mikolov并不是第一个观察到词向量中存在结构的人。早在1989年，就有研究者用自组织的神经网络（自组织映射，Self-Organizing Map）来学习词的二维表示，并观察到这些表示具有语义结构。

但有一个关键的区别：在1989年的方法中，词的表示不是端到端学习的。它们是预先计算好的，然后输入到自组织映射中。而Mikolov的方法不同。词向量是直接从语言建模任务中学到的。更重要的是，这些词向量具有明确的线性结构。你可以用向量加法和减法来进行推理。这不仅仅是存在结构，而是**可计算的结构**。

这一点很重要，因为它确认了Bengio在2003年提出的一个关键假设：**相似的词应该有相似的向量表示，因此，在向量空间中的小变化应该导致语言性质的小变化。** Mikolov的工作用令人信服的例子证明了这一点。

突然之间，那个看起来很简单的想法——用神经网络学习分布式表示——被证明不仅在理论上有效，而且在实践中会产生非常有趣的涌现性性质。这些性质甚至没有被显式编程，而是从数据中自动学到的。

### 从词到上下文

Word2Vec的一个明显的局限是，它给每个词一个单一的、固定的向量。但在现实中，词的含义往往依赖于上下文。英文中的"bank"既可以指一家银行，也可以指河岸。哪个意思取决于上下文。一个固定的词向量必然会在这两种含义之间做某种平均，因此在两种情况下都不是最优的。

到2015年左右，研究者们开始探索**上下文相关的词表示**。一种方法是使用双向LSTM（长短期记忆网络的双向版本）来为每个词生成上下文相关的表示。LSTM是一种循环神经网络，它能很好地处理长序列，而双向的意思是它可以同时考虑一个词的左上下文和右上下文。

最著名的这种方法之一是2018年的ELMo模型（深度上下文化词表示）。在ELMo中，研究者们用双向LSTM从大规模文本中学习表示，然后将这些表示用作下游任务（比如命名实体识别或情感分析）的特征。更创新的是，ELMo结合了预训练和任务特定的微调。模型首先在大规模无标签文本上进行预训练（学习语言的通用性质），然后在特定任务上进行微调（适应特定的应用）。

这个想法——预训练然后微调——会变得非常重要。但在2018年，这仍然是一个新颖的想法。

### 向量空间中的一场文艺复兴

在Word2Vec之后的几年里，词向量领域迎来了真正的文艺复兴。研究者们提出了许多不同的方法来学习词表示，每种方法都有不同的权衡和优势。

2014年，Pennington等人发表了GloVe（全局向量词表示）。GloVe提供了一个统一的视角，将两类常见的词向量学习方法——全局矩阵分解和局部上下文窗口方法——联系起来。2015年有Skip-thought向量，2015年有Eigenwords（特征词）。几乎每个月都有新的论文提出学习词表示的新方法。

这可能听起来像是学术界的一种过度炫耀，但实际上它反映了一个更深层的事实：一旦人们意识到词向量有多么有用，大家都想在这个方向上改进。而且确实，这个时期的不同方法各有优缺点。有些更快，有些学到更好的语义关系，有些对稀有词表现更好。

但有一点是清晰的：从2013年左右开始，词向量成为了NLP的基础。任何一个尊重自己的NLP系统都会先用一个预训练的词向量模型（比如Word2Vec或GloVe），而不是随机初始化词表示。这些向量捕捉的语言知识使得下游模型能更快地学习，学得更好。

### 一个模式的显现

现在让我们退一步，看看正在浮现的更大的模式。

2003年，Bengio提出了一个想法：用神经网络学习词的分布式表示。但在当时，这太贵了，所以几乎没有人采用。

2012年，AlexNet和GPU的出现改变了计算的可行性。

2013年，Mikolov等人通过简化模型（移除隐藏层）和使用巧妙的技巧（分层softmax和负采样）来加速计算，使得Word2Vec成为了实际可行的。

2013年，同样是Mikolov，他们发现这些简化的模型学到的词向量具有令人惊讶的语言规律。

从这个序列中，一个重要的原则开始浮现：**也许简单的方法，当应用于足够大的数据和有足够的计算力时，会超越复杂的方法。** 这个原则最终会被称为"苦涩而又温暖的教训"（The Bitter Lesson），在今天被广泛讨论。但在2013年，这还只是一个模式，还没有被明确表述。

然而，它的力量已经很明显了。Bengio的复杂模型被Mikolov的简单模型打败了。不是在理论上，而是在实践上。而且简单模型学到的东西和复杂模型一样深刻。

这一课会在接下来的故事中一次次地重复。每次，答案都是：走向更大的规模，使用更简单的方法，相信数据和计算会处理复杂性。

这正是通向大语言模型的路。