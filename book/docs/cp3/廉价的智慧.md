---
sidebar_position: 3
---

### 简化的力量

AlexNet的胜利激励了NLP研究者们，但它也留下了一个尴尬的问题：深度学习在图像识别上表现如此出色，为什么应用到语言处理上还这么困难？

答案就隐藏在计算的复杂性中。

让我们回到Bengio的模型。虽然这个模型在概念上优雅，但在工程上却是一场噩梦。模型需要处理一个巨大的矩阵乘法：将隐藏层的输出（维度为H，通常在2000左右）与整个词汇表大小（V，百万级别）相乘，以计算每个单词的概率。这个V×H的计算主导了整个复杂度分析。更糟糕的是，计算完这些原始概率后，你还必须进行softmax归一化——对所有百万个单词计算指数和。这等于要计算V次指数运算。

用数字说话：如果你的隐藏层大小H是500，词汇表大小V是100万，那么每处理一个单词就需要进行5亿次浮点运算，再乘以你要处理的总单词数和训练轮数。即使用最快的CPU，这也意味着训练一个中等规模的模型需要数周甚至数月。这不仅是缓慢的问题，更是根本上不可行的问题。

2012年，AlexNet的成功吸引了人们对深度学习的关注，但当NLP研究者们试图重现这个成功时，他们很快发现：我们没有时间等待这么慢的训练。

正在这个时候，一个叫Tomas Mikolov的研究者有了一个激进的想法：**为什么我们需要一个完整的神经网络语言模型呢？** 为什么不能只是学习好的词向量？

2013年，Mikolov团队发表了两篇非常相关的论文，题目分别是《向量空间中词表征的有效估计》和《词与短语的分布式表征及其组合性》。这两篇论文一起引入了现在最著名的词向量工具——**Word2Vec**。

Mikolov做了什么？他采取了一系列看似激进的简化，最终创造了一个既简单又高效的模型。

首先，他抛弃了Bengio模型中昂贵的非线性隐藏层。记得那个用tanh激活函数的隐藏层吗？消除它。只保留线性运算。结果是一个所谓的"对数线性模型"（Log-linear Model）——输出概率的对数与输入是简单的线性关系。

其次，他采用了两种技巧来解决softmax的计算瓶颈。一种是**分层softmax**（Hierarchical Softmax），这是前人开发的技巧，它用一棵二叉树来组织词汇表，使得计算概率的复杂度从O(V)降低到O(log₂V)。当V是100万时，log₂V只有大约20。这是一个从数百万到二十的跳跃。第二种是**负采样**（Negative Sampling），这是一个更巧妙的技巧：与其计算所有单词的概率，不如只抽取几个"负样本"（不应该出现的单词）和一个正样本（应该出现的单词），训练模型去区分它们。理论证明这种做法虽然只使用K个样本（K通常是2到20之间），但仍然能收敛到正确的概率分布。

这两个简化一起，使得计算复杂度从数百万级别降低到了可以在几个小时内训练完成的程度。

### 两个孪生模型

Mikolov提出的模型有两种变体。第一种叫**连续词袋模型**（CBOW，Continuous Bag-of-Words），它用周围的词来预测中心词。第二种叫**Skip-gram模型**，反过来用中心词预测周围的词。

让我们聚焦于Skip-gram，因为这个模型在后来的应用中更为广泛。Skip-gram的想法非常简洁：给定一个词，预测它周围的词。

形式上，假设我们有一个句子w₁:T，我们要最大化对数似然：

$$\frac{1}{T}\sum_{t=1}^{T} \sum_{-C \le j \le C, j \ne 0} \log p(w_{t+j} | w_t)$$

这里C定义了"周围"的范围。如果C=2，那么我们在预测一个词时，只考虑它左边和右边各两个词。

关键的创新在于如何计算条件概率p(w_{t+j}|w_t)。Mikolov用一个简单的点积：

$$p(w_{t+j} | w_t) = \frac{\exp(\langle u_{w_{t+j}}, c_{w_t} \rangle)}{\sum_{i \in V} \exp(\langle u_i, c_{w_t} \rangle)}$$

这里c和u分别是"输入词向量"和"输出词向量"。这些向量就是模型要学习的参数。注意这里没有任何非线性变换，没有隐藏层，只是词向量之间的点积。

这听起来太简单了。但正是这种简单性，使得模型可以在巨大的数据集上快速训练。而且，令人惊讶的是，这个简单的模型学到的词向量具有非常有趣的性质。

### 向量中的奥秘

当Mikolov的团队用Skip-gram模型在数十亿单词的语料上训练后，发生了一件奇妙的事：词向量中出现了**语言规律**。

这听起来可能有点抽象，但具体例子会让它变得非常清晰。比如，模型学到的"king"（国王）、"queen"（皇后）、"man"（男人）、"woman"（女人）的词向量之间有一个令人惊讶的关系：

$$\vec{\text{king}} - \vec{\text{man}} + \vec{\text{woman}} \approx \vec{\text{queen}}$$

换句话说，如果你取"国王"的向量，减去"男人"的向量，再加上"女人"的向量，你会得到一个与"皇后"向量非常接近的新向量。

或者另一个例子：

$$\vec{\text{Russia}} + \vec{\text{river}} \approx \vec{\text{Volga River}}$$

"俄罗斯"加上"河流"大约等于"伏尔加河"。

让我停下来强调一下，这有多么不同寻常。Mikolov的模型是在一个简单的、无关的任务上训练的——只是预测相邻的词。模型没有被明确地告知"king"和"queen"之间的语义关系，也没有被教导什么是"首都"或"河流"。它纯粹通过在大量文本上进行统计学习，自动发现了这些关系。

从数学的角度，这意味着词向量空间的结构捕捉了自然语言中的某些基本规律。相似的词在这个空间中聚集在一起，而语义关系对应于向量的偏移。如果"国王"和"皇后"之间的关系是某个方向和距离，那么"男人"和"女人"之间的关系大约是同一个方向和距离。

这个发现激发了人们的想象。如果一个为了预测相邻词而训练的简单线性模型能学到这样的结构，那么更复杂的模型会学到什么？如果我们在更大的数据集上训练，会发生什么？如果我们用这些预训练的词向量作为其他NLP任务的初始化，会发生什么？

### 为什么这有效？

但这里有一个哲学问题：为什么一个简单的线性模型，通过在"预测下一个词"这个看似简单的任务上进行训练，能学到如此丰富的语义结构？

答案涉及一个深刻的观察，我们在Bengio的论文中已经看到过。如果词向量被很好地组织，使得语义上相似的词拥有相似的向量，那么预测任务会变得容易。为什么？因为如果"cat"（猫）和"dog"（狗）在向量空间中靠近，那么句子"The cat is sleeping"和"The dog is sleeping"中的"cat"和"dog"位置互换时，模型应该给出相似的概率。

但这还不足以解释语义关系的线性结构。更深层的原因与**组合性**（Compositionality）有关——自然语言中的复杂含义是通过简单含义的组合而成的。"King"这个词的含义可以分解为某些基本属性的组合，比如"权力"、"男性"等等。如果模型通过学习预测邻近词来隐含地建模这些属性，那么它自然会在向量空间中形成一个能够表达这些属性的结构。

在Mikolov的第三篇相关论文《连续空间中的语言规律》中，作者们直接研究了这个现象，发现许多语义和句法关系都对应于向量空间中的近似恒定偏移。这不是一个理论预测，而是一个实证发现——他们在大规模词向量上系统地验证了这一点。

### 一场静悄悄的革命

但这里有个容易被忽视的细节：Word2Vec的模型本身**不是完整的语言模型**。回忆我们之前的定义，语言模型应该能够计算任意单词序列的概率p(w₁:T)。但Word2Vec做的是邻近词预测，这完全不同。

这是有意为之的。Mikolov在论文中明确指出，他们的目标不是构建完整的语言模型，而是**学习好的词向量作为特征**，这些特征可以被用于其他任务。他们承认Bengio在2003年就有了学习词向量的想法，但Bengio的方法非常昂贵，因为他试图同时学习完整的语言模型。Mikolov的洞察是：也许我们可以分离这两个目标。先用一个廉价的、简单的方法学习词向量，然后用这些向量作为其他模型（比如完整的语言模型或分类模型）的输入。

这个想法改变了一切。突然间，学习有用的词向量变成了一个可以在几小时而不是几周内完成的任务。而且这些词向量可以被免费提供给整个社区使用。研究者们可以下载预训练的Word2Vec词向量，用于他们自己的NLP系统，获得显著的性能提升。

从这一刻起，词向量成为了NLP的基础设施。这不仅仅是因为Word2Vec本身的聪明设计，也是因为它在时间和空间上的可访问性。在AlexNet的鼓舞下，随着GPU计算的普及和深度学习工具的改进，NLP研究者们现在可以用廉价而高效的方式获得高质量的词表征。

当然，Word2Vec不是唯一的方法。2014年，Pennington等人发表了GloVe（Global Vectors for Word Representation），提供了一个统一的视角来理解全局矩阵分解方法和局部上下文窗口方法。之后还有Skip-thought vectors、Eigenwords等许多其他方法。但Word2Vec因其简洁、高效和有效性，成为了这个时代最标志性的成果。

### 上下文的觉醒

但是，Word2Vec有一个根本的局限性：它为每个词学习了一个固定的向量表示，而不考虑该词出现的上下文。这是一个问题，因为词的含义常常依赖于上下文——这个现象叫做**多义性**（Polysemy）。比如"bank"这个词，可以指金融机构也可以指河岸。一个固定的词向量必须在这两个含义之间进行某种妥协。

2016年前后，研究者们开始探索**上下文相关的词向量**。一种方法是使用双向LSTM（长短期记忆网络）的隐藏状态作为上下文相关的词表示。比如Context2vec或者后来的McCann等人的方法，都采用了这个思路。

但最著名的是2018年的ELMo（Embeddings from Language Models），由Peters等人提出。ELMo做了两件重要的事。第一，它使用双向LSTM来计算上下文相关的词表示。第二，它将这些表示与下游任务的微调相结合——先在大规模无标注数据上预训练，然后在具体的任务数据上进行有监督的微调。

这个想法虽然看似简单，但它预示了未来的发展方向。它表明，我们可以用无监督的预训练学习一般性的语言表示，然后通过有监督的微调适应特定任务。这个范式在几年后会被广泛采用，成为现代NLP的标准做法。

但在2018年，这还只是一个有前景的开始。真正的下一步——从词向量到整句甚至整个段落的更强大表示——需要一个不同的架构。这个架构已经在计算机视觉中被证明了，现在轮到NLP领域了。

它的名字叫Transformer。

但在我们到达那里之前，我们需要先理解循环神经网络为什么陷入了困境，以及为什么Transformer能够突破这个困境。