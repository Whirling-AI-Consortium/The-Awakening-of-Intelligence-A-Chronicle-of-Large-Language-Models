---
sidebar_position: 3
---

## AlexNet之后的困境：为什么深度学习在NLP中举步维艰

### 计算复杂性的魔咒

2012年ImageNet竞赛中AlexNet的胜利不仅改变了计算机视觉领域，更激励了自然语言处理领域的研究者们尝试将深度学习的成功复制到语言处理任务中。这个转移看似合理——毕竟，如果深度学习在图像识别上表现如此出色，为什么不能在语言处理上取得类似的成功？

然而，当NLP研究者们试图将深度学习方法应用于语言问题时，他们很快遭遇了一个尴尬但现实的难题。深度学习方法的计算要求与NLP问题的特殊结构之间存在根本的矛盾。这个矛盾隐藏在看似无关的计算复杂性细节中。

回到Yoshua Bengio在2003年提出的神经网络语言模型。虽然这个模型在概念上优雅而富有启发性，但在工程实现上却是一场计算噩梦。核心的瓶颈在于模型需要处理一个巨大的矩阵乘法运算。模型的隐藏层输出通常维度为H，规模在2000左右。但这个隐藏层输出需要与整个词汇表大小V相乘，以计算词汇表中每个单词出现的概率。词汇表大小V通常是百万级别的——在现实的、覆盖广泛语言现象的NLP系统中，通常包含50万到500万的词汇。这个V×H的矩阵乘法运算成为了整个模型复杂度分析的主导项。

更加棘手的是，计算完这些原始的、未归一化的概率之后，模型还必须进行softmax归一化操作。Softmax要求计算V个指数函数值，然后将它们求和——即使用最优化的计算方法，这也意味着需要执行V次指数运算。当V是一百万时，这个计算成本是不可忽视的。

用具体的数字来说明这个问题的严峻性：假设隐藏层大小H为500，词汇表大小V为一百万。那么处理一个单词就需要进行大约5亿次浮点运算（multiply-accumulate operations）。一个实际的训练过程需要处理数十亿到数百亿个单词，并进行多轮训练迭代。即使利用当时最快的CPU硬件，这也意味着训练一个中等规模的模型需要数周甚至数月的时间。这不仅仅是一个速度问题——它在基本上变成了一个可行性问题。对于大多数研究机构来说，投入数月的计算资源来训练单个模型是不现实的。

2012年，AlexNet在ImageNet上的压倒性胜利吸引了全世界对深度学习的关注。深度学习的浪潮开始兴起。但当NLP研究者们满怀期待地尝试用深度学习方法时，他们遭遇了一个冷酷的现实：计算时间的限制使得他们根本没有机会等待训练完成。AlexNet可以在几周内完成训练；但NLP的深度模型可能需要数月。在这样的约束下，重复实验、进行超参数调优、测试新想法变成了几乎不可能的任务。

## 简化的艺术：Word2Vec与词向量的革命

### Mikolov的激进简化

正在整个NLP领域陷入计算复杂性困境的时候，一位名叫Tomas Mikolov的研究者提出了一个看似激进但深刻的问题：**我们真的需要一个完整的神经网络语言模型吗？**

Mikolov的关键洞察是：也许我们应该分离两个问题。传统的方法试图同时解决两个问题：（1）学习好的词向量表示；（2）用这些向量构建一个完整的语言模型，能够计算任意词序列的概率。Mikolov的想法是，也许可以先用一个简单、廉价的方法专注于解决第一个问题——学习好的词向量。这些词向量本身就有价值，可以被用于下游的各种NLP任务，而无需完整的语言模型。

2013年，Mikolov与他的Google团队发表了两篇紧密相关的论文：《向量空间中词表征的有效估计》（Efficient Estimation of Word Representations in Vector Space）和《词与短语的分布式表征及其组合性》（Distributed Representations of Words and Phrases and their Compositionality）。这两篇论文一起引入了现在最著名的词向量工具——**Word2Vec**。

为了实现这个激进的简化，Mikolov采取了一系列看似大胆的设计决策。首先，他完全舍弃了Bengio模型中那个昂贵的非线性隐藏层。记得Bengio模型中用tanh激活函数的隐藏层吗？Mikolov的Word2Vec直接消除了它。模型中只保留线性运算。这种结构被称为"对数线性模型"（Log-linear Model），因为输出概率的对数与输入词向量之间是线性关系。这个简化乍看之下似乎会严重限制模型的表达能力，但实际上它带来了巨大的计算效益。

其次，Mikolov采用了两种巧妙的技巧来解决softmax计算的瓶颈——这个一直困扰NLP深度学习的主要计算障碍。

**第一种技巧是分层softmax**（Hierarchical Softmax）。这个想法来自之前的研究，但Mikolov将其应用于Word2Vec。基本思想是不直接计算V个单词的概率，而是用一棵二叉树来组织词汇表。计算任何单词的概率只需要沿着这棵树从根部走到叶子节点，大约需要log₂V步。当V是一百万时，log₂V只有约20。这是一个从百万量级到二十的戏剧性跳跃——计算复杂度下降了五个量级。

**第二种技巧是负采样**（Negative Sampling）。这是Mikolov提出的一个更加巧妙的方法。与其计算所有V个单词的概率分布，不如只从负样本（不应该在该上下文中出现的单词）中随机采样几个，再加上一个正样本（应该出现的单词），然后训练模型去正确区分它们。这似乎是对问题的粗暴简化，但Mikolov基于噪声对比估计（Noise Contrastive Estimation）的理论论证了这种做法的有效性。理论表明，即使只使用K个采样的样本（K通常在2到20之间），这个方法在数学上仍然能够收敛到正确的概率分布。而在实践中，K取值为5到20时通常能获得最佳性能。

这两个计算简化相结合，将原来需要数百万次操作的softmax计算降低到可以在几小时内训练完成的水平。实际上，使用这些技巧的Word2Vec模型可以在标准的计算机硬件上训练，而无需专门的GPU。这是一个根本性的转变——从"需要数周/数月的训练"变成了"需要数小时的训练"。

### Skip-gram模型与CBOW模型

Mikolov提出的框架包含两种主要的模型变体，分别解决了词向量学习的不同方面。第一种叫**连续词袋模型**（CBOW，Continuous Bag-of-Words），这种模型的思想是：给定某个词周围的上下文词，预测这个中心词。CBOW的优势是它在数据量较小的情况下能表现得相对较好。

第二种叫**Skip-gram模型**，它采用相反的方向：给定中心词，预测它周围的上下文词。虽然CBOW在某些任务上更高效，但Skip-gram在后续的应用中证明了其优越性，特别是当处理大规模语料库时。Skip-gram能够更好地捕捉稀有词汇的表示，因为每个出现都被当作一个潜在的预测任务。

Skip-gram的核心思想非常简洁而优雅。给定一个词序列w₁:T，模型要最大化的目标函数为：

$$\frac{1}{T}\sum_{t=1}^{T} \sum_{-C \le j \le C, j \ne 0} \log p(w_{t+j} | w_t)$$

这里C参数定义了"周围"或"上下文窗口"的范围。如果C=2，模型在预测一个词时考虑其左边两个词和右边两个词，总共四个上下文词。C的选择是一个重要的超参数——C越大，模型考虑的上下文越广，计算成本也就越高。在实践中，C通常在1到10之间，根据具体应用而定。

Skip-gram模型的关键创新在于如何高效地计算条件概率p(w_{t+j}|w_t)。Mikolov采用了一个简单而强大的方法——使用两个词向量之间的点积：

$$p(w_{t+j} | w_t) = \frac{\exp(\langle u_{w_{t+j}}, c_{w_t} \rangle)}{\sum_{i \in V} \exp(\langle u_i, c_{w_t} \rangle)}$$

这里c_w表示"输入词向量"或"中心词向量"，u_w表示"输出词向量"或"上下文词向量"。这些向量就是模型要学习的参数。值得注意的是，这个公式中完全没有非线性激活函数，没有隐藏层，只有词向量之间的点积。分母中的求和便是计算成本的主要来源，这正是分层softmax和负采样所解决的问题。

这听起来可能过于简单，甚至有些令人怀疑——这么简单的模型真的能学到有意义的表示吗？这正是Word2Vec的天才之处：正是这种简洁性，使得模型能够在巨大的数据集上进行高效训练。而令人惊讶的是，这个简单的模型实际上学到了非常丰富和结构化的词表示。

## 向量空间中的语言规律：从统计到语义

### 语言规律的涌现

当Mikolov的团队用Skip-gram模型在数十亿单词的大规模语料库上进行训练时，发生了一件科学上令人惊讶而深刻的事情：词向量中自发地涌现了**高阶的语言规律和语义关系**。

这个发现乍看之下有些抽象，但具体的例子会立即揭示其非凡之处。考虑模型学到的"king"（国王）、"queen"（皇后）、"man"（男人）、"woman"（女人）这四个词的向量表示。研究者们发现这些向量之间存在一个令人瞩目的几何关系：

$$\vec{\text{king}} - \vec{\text{man}} + \vec{\text{woman}} \approx \vec{\text{queen}}$$

用语言表述，如果取"国王"的向量，减去"男人"的向量，再加上"女人"的向量，结果会得到一个与"皇后"的向量在向量空间中非常接近的新向量。这不是近似的或模糊的相似——在高维向量空间中，这个关系可以达到相当高的精确度。

类似的例子还有很多。比如：

$$\vec{\text{Russia}} + \vec{\text{river}} \approx \vec{\text{Volga}}$$

"俄罗斯"的向量加上"河流"的向量近似于"伏尔加河"的向量。或者：

$$\vec{\text{Paris}} - \vec{\text{France}} + \vec{\text{Germany}} \approx \vec{\text{Berlin}}$$

"巴黎"减"法国"加"德国"近似于"柏林"。

停顿一下，完全理解这个发现的不同寻常之处是至关重要的。Mikolov的模型在一个非常简单、看似无关的任务上进行训练——仅仅是学会预测词序列中的相邻词对。模型从未被明确地告知"king"和"queen"之间存在某种特定的语义关系。模型没有被教导什么是"皇家身份"这个概念，没有被教导男性和女性之间的语义对立，也没有被给予任何关于"首都"、"河流"或地理关系的明确指导。

尽管缺乏这所有的明确指导，模型仅仅通过在大量自然语言文本上进行统计学习，自动发现并编码了这些深层的语义和概念关系。这表明了某种深刻的东西：词之间的数学关系在某种意义上反映了自然语言中真实的语义结构。

### 向量空间的结构与组合性

从数学和语言学的角度分析这个现象，可以得出几个重要的见解。首先，词向量空间的结构捕捉了自然语言中的某些基本规律和规则。相似的词在这个空间中聚集在一起。比如，所有关于国家的词（"France"、"Germany"、"Japan"）会聚集在某个区域，所有表示城市的词会聚集在另一个区域。同时，不同的语义关系对应于向量空间中的不同方向和偏移。

更具体地说，如果"国王"和"皇后"之间的关系可以用向量空间中的某个特定偏移来表示，那么"王子"和"公主"之间应该对应类似的偏移。这反映了自然语言中的一个深层特性：**语言的组合性**（Compositionality）。

语言的组合性原则指出，复杂的语言意义是从简单意义的组合而成的。"King"这个词的含义可以分解为若干基本属性的组合——比如"权力"、"男性"、"统治者"等。如果词向量空间能够隐含地建模这些基本属性维度，那么不同词通过在这些维度上的不同坐标值来表示，相关语义关系就会自然地对应于向量空间中的可预测的偏移。

在Mikolov的第三篇相关论文《连续空间中的语言规律》（Linguistic Regularities in Continuous Space of Words）中，作者们直接研究了这个现象的广泛性和系统性。他们系统地验证了许多语义关系（如国家-首都、性别对偶、形容词-副词转换等）和句法关系（如动词时态变化等）都对应于向量空间中近似恒定的偏移。这不仅仅是理论预测或孤立的例子——通过大规模的实证验证，研究者证明了这是Word2Vec学到的词向量中广泛存在的系统性规律。

## 一场静悄悄的革命：预训练词向量的时代到来

### 一个微妙但根本的转变

需要特别强调的是，Word2Vec所做的工作与传统意义上的"语言建模"有着根本的区别。回顾早期的定义，语言模型应该能够计算任意词序列的概率p(w₁:T)。这是一个完整的、无所不能的目标——如果拥有一个完美的语言模型，理论上可以用它来完成任何NLP任务。

但Word2Vec做的事情完全不同。它执行的是**邻近词预测**任务——给定一个词，预测它附近可能出现的词。这个任务与完整的语言建模问题看起来相差很远。实际上，Word2Vec根本无法计算任意词序列的概率——这甚至不是它的目标。

这个转变是有意为之的，也是根本性的。Mikolov在论文中明确指出，他们的主要目标不是构建完整的语言模型，而是**学习高质量的词向量作为特征**，这些特征可以被应用于其他下游NLP任务。他们坦诚地承认，Yoshua Bengio早在2003年就有了学习词向量的想法，但Bengio的方法极其昂贵，因为它试图同时实现两个目标。Mikolov的关键洞察和创新是：也许可以将这两个目标分离。先用一个计算上廉价而高效的方法专注于学习好的词向量，然后将这些向量作为其他模型（比如完整的语言模型、分类模型、序列标注模型等）的初始化或输入特征。

这个想法的影响深远。它改变了整个领域的研究范式。突然间，学习高质量的词向量表示从一个极其昂贵、需要数周或数月计算的任务，变成了可以在几小时内完成的任务。而且这些词向量可以被免费提供给整个研究社区使用。任何研究机构都可以下载预训练的Word2Vec词向量，用于他们自己的NLP系统，获得显著的性能改进。

这不是一个孤立的技术进步。它代表了一个范式转变：从"从零开始学习表示"转变为"使用预训练的表示"，从"每个任务都需要大规模计算"转变为"只需要下载预训练模型然后进行轻量级微调"。

### Word2Vec之后的生态发展

从Word2Vec的成功开始，词向量成为了自然语言处理的基础设施。这不仅因为Word2Vec本身的聪明设计和理论优雅，更因为它在实践中的可获得性和效率。在AlexNet的鼓舞下，随着GPU计算的逐步普及和深度学习工具和框架的改进，NLP研究者现在拥有了以廉价而高效的方式获得高质量词表示的手段。

当然，Word2Vec不是唯一的解决方案。学术界随后提出了许多其他方法来学习词向量。2014年，Pennington、Socher和Manning发表了**GloVe**（Global Vectors for Word Representation），这个方法提供了一个统一的视角来理解全局矩阵分解方法和局部上下文窗口方法。GloVe既结合了全局矩阵分解（如隐语义分析）的优点，也借鉴了Word2Vec等局部方法的高效性。之后还有其他方法如Skip-thought vectors、Eigenwords等相继出现。但在简洁性、计算效率和实际有效性的综合考量上，Word2Vec因其精妙的设计而成为了这个时代最标志性、最广泛使用的成果。

### 静态向量的局限与上下文意识的觉醒

然而，Word2Vec方法有一个根本性的局限，这个局限最终推动了领域向着更加复杂和强大的方向发展。Word2Vec为每个词学习并固定了一个唯一的向量表示，这个表示对所有出现上下文都是相同的。这忽视了一个自然语言的基本特征：**词义的多样性和上下文依赖性**，学术上称为**多义性**（Polysemy）或**同形异义**（Homonymy）。

考虑一个简单的例子："bank"这个词在英文中至少有两个常见的含义。它可以指金融机构（"I went to the bank to withdraw money"），也可以指河岸（"I walked along the river bank"）。在Word2Vec的框架中，模型需要学习一个单一的向量表示来代表"bank"这个词，这个向量必然是两个含义的某种折衷或混合。但在实际的语言理解中，这两个含义在语义上相差很远，它们的特征和属性大不相同。一个理想的表示方法应该能够根据上下文来为"bank"分配不同的向量。

这个观察在2016年前后激发了研究社区对**上下文相关的词向量**（Contextualized Word Representations）的兴趣。研究者开始探索使用循环神经网络（RNN）或其他架构的隐藏状态作为上下文相关的词表示。比如Context2vec和McCann等人的工作都采用了这个思路。

最著名和最有影响力的工作是2018年的**ELMo**（Embeddings from Language Models），由Matthew Peters等人发表。ELMo做了两件重要且相辅相成的事。第一，它使用双向LSTM（Bidirectional LSTM）来计算上下文相关的词表示。每个词的表示不是一个固定的向量，而是由前向LSTM和后向LSTM的隐藏状态组合而成，因此随着上下文的不同而变化。第二，它将这种上下文相关表示与下游任务的监督微调相结合——在大规模的无标注语料库上进行无监督的预训练，学习一个通用的语言表示模型，然后将这个模型应用到具体的任务数据上进行有监督的微调。

虽然ELMo的这个想法看似直接和简明，但它实际上预示了未来发展的方向。它表明一个关键的范式：可以用无监督的预训练学习通用的、任务无关的语言表示，然后通过有监督的微调使这些表示适应特定的下游任务。这个两阶段的范式（预训练+微调）在几年后会被广泛采用，成为现代自然语言处理的标准工作流程。

## 走向更强大的表示：从ELMo到Transformer

然而，即使ELMo取得了显著的成功，它仍然不是故事的终点，甚至不是故事的中点。在2018年，ELMo代表了词向量和上下文表示研究的最前沿，但局限性仍然存在。使用双向LSTM来计算上下文表示虽然比固定的词向量要好，但仍然受限于RNN架构本身的特性——特别是其顺序处理的特点和计算并行性的限制。

真正的下一步——从单个词的上下文向量到整个句子、段落甚至长文本的更强大的统一表示——需要一个根本不同的、更强大的架构。这个架构需要能够更有效地建模词之间的长距离依赖，需要能够充分利用现代GPU硬件的并行计算能力，需要能够扩展到更大的模型和更大的数据集。

这个架构已经在计算机视觉社区中被证明了其效用和强大能力，现在轮到自然语言处理领域来采纳和改进它了。它的名字叫**Transformer**。

但在深入Transformer的细节之前，需要理解为什么循环神经网络（RNN）在处理长序列时面临的困难，以及为什么Transformer的设计能够突破这些困难。这是理解大语言模型崛起的关键一步。