---
sidebar_position: 2
---

### 从理论到实践的关键问题

Bengio论文的出现确实开启了一扇新的大门，但它只是一个开始。任何优秀的理论都会面临一个残酷的问题：如何实践？

具体来说，有三个关键的工程问题需要回答。第一个是：我们用什么损失函数来训练这个模型？第二个是：训练过程中到底发生了什么？第三个是：一旦模型训练好了，我们如何用它来生成新的文本？这些问题看似技术性很强，但实际上它们触及了大语言模型的灵魂。因为当你仔细观察后会发现，从Bengio的2003年模型到今天最先进的GPT系列模型，在概念层面上，它们的训练方法几乎没有本质区别。理解这一点，就是理解所有现代大模型的关键。

让我们用一个具体的例子来说明。假设我们有一个来自弗吉尼亚·伍尔夫著作《一间自己的房间》的句子：

"Intellectual freedom depends upon material things."

现在，假设我们的模型的上下文窗口大小为2（N=2），这意味着模型只能看两个单词来预测第三个。我们从第一个单词"Intellectual"开始。模型的第一个输入就是这个单词的向量表示。

输入到模型里的是一对向量，第一个是全零向量（填充用），第二个是"intellectual"的词向量。模型的输出是一个向量，它代表所有可能单词的概率分布——更准确地说，是p(w₂|w₁)的概率分布。这个输出向量有100万维，每一维对应一个单词。在我们的例子中，其中"freedom"这一维应该有最高的概率（或接近最高）。

然后我们计算交叉熵损失。这是一个标准的信息论概念：我们用真实的分布（一个one-hot向量，在"freedom"这一维是1，其他地方都是0）和模型预测的分布来计算距离。这个距离就是损失。我们想要让这个损失尽可能小。

接着，我们对第二个单词重复同样的过程。模型的输入现在是"intellectual"和"freedom"的向量对，输出是p(w₃|w₁:₂)的概率分布。我们期望"depends"这一维有最高的概率。我们再次计算交叉熵损失。

这个过程一直进行到句子的结尾。关键的观察是，当我们处理第四个单词时，由于上下文窗口只有大小2，模型只能看"freedom"和"depends"，而"intellectual"已经从窗口中滑落了。模型完全忘记了这个词的存在。

从数学的角度，我们想要最大化的目标是**对数似然**（Log Likelihood）：

$$\Theta^* = \arg\max_{\Theta} \sum_{t=1}^{T} \log g_{\Omega}(c_{I(w_{t-N})}, ..., c_{I(w_{t-1})})$$

这个公式说的是：我们想要找到一组参数Θ*，使得模型对训练数据中所有单词序列的预测概率尽可能高。换成反面的表述，就是最小化**负对数似然**（Negative Log Likelihood），这正好就是我们用梯度下降和反向传播要优化的东西。

这就是全部了。在概念层面上，这个框架和今天最先进的大语言模型的训练方式完全相同。当然，现代模型中有大量额外的工程技巧——更复杂的神经网络架构、注意力机制、归一化层、优化算法的改进等等——但核心的目标函数并没有改变。我们仍然在做同一件事：最大化对数似然。

### 生成的魔法

现在假设我们已经训练好了模型，找到了一个不错的参数集合Θ*。我们如何用这个模型来生成新的文本？

方法是这样的：首先，我们随机抽取一个起始单词。假设我们抽到了"The"。然后，我们用这个单词来计算下一个单词的条件概率分布p(w₂|w₁="The")。根据这个分布，我们随机抽取第二个单词。假设抽到了"cat"。然后，我们用前两个单词"The"和"cat"来计算p(w₃|w₁:₂)，根据这个分布抽取第三个单词。以此类推。

这个过程能一直进行到我们决定停止，或者模型预测出了特殊的"句子结束"标记。

这听起来很简单，但它的含义却是深刻的。这意味着，一个用来预测下一个单词的模型，同时也是一个生成模型。它不仅能理解语言，还能创造语言。这正是所有大语言模型的核心能力。

我们把这种训练方式称为**自回归**（Autoregressive）。这个术语来自统计学，指的是一个变量用自己的历史值来预测。就像一个经典的AR(1)模型那样——明天的温度由今天的温度预测得出，后天的温度由明天的温度预测得出。在我们的情况中，下一个单词是由所有前面单词预测得出的。

但这里有个重要的限制：我们只考虑前面N个单词，而不是所有的历史。这个N被称为"上下文窗口"。在Bengio的论文中，N通常很小，比如2或3。这个限制之所以存在，不是出于理论上的原因，而是出于实践上的原因：计算消耗。更大的上下文窗口意味着更多的输入维度，意味着更大的神经网络，意味着更长的训练时间。

### 十年的沉寂

Bengio 2003年的论文确实很有远见，但它的影响力被延迟了大约十年。为什么？因为在当时，训练神经网络非常困难。

如果你今天去读那篇论文，你会被它的"原始性"震撼。他们在CPU上训练模型，没有自动微分库，没有现代的深度学习框架。一个相对较小的模型可能需要花费数天甚至数周才能训练完成。相比之下，这根本无法与当时已经成熟的N-gram方法竞争。N-gram方法很简单，训练快速，而且在实践中已经经过了几十年的验证。为什么要冒险用一个缓慢、不稳定的神经网络模型呢？

在那之后的十年里，虽然有人继续改进Bengio的方法，但进度缓慢。2008年，Collobert和Weston展示了Bengio的神经语言模型可以用在各种NLP任务上。2010年，Turian等人展示了词向量可以改进现有的NLP系统。但这些都不是令人信服的突破性进展。

现实是，到了2010年左右，在实际应用中，**N-gram模型仍然是最先进的**。这令人惊讶，但这是历史的事实。在2010年发表的一篇关于循环神经网络语言模型的论文中，作者们写道：

> "质疑是否在简单的N-gram模型上取得了任何显著的进展……实际上，大多数提议的先进语言建模技术只能在简单基准上提供微不足道的改进，在实践中很少被使用。"

两年后，2012年，另一篇论文的作者写了类似的话：

> "尽管神经概率语言模型性能更好，但由于训练时间长得可怕（即使在中等规模数据集上也要花费数周），它们仍然远不如N-gram模型应用广泛。"

这是一个令人沮丧的局面。100年过去了，从Markov的开创性工作以来，研究者们仍然在为将人类语言表示为可计算的形式而苦恼，而N-gram仍然被认为是一个合理的选择。

### AlexNet时刻：硬件改变一切

然后，2012年，一切改变了。

那一年，一篇论文发表在深度学习的顶级会议NeurIPS上，标题是《ImageNet分类的深度卷积神经网络》，作者包括Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton。这篇论文描述的一个叫AlexNet的深度卷积神经网络，在当时最大的图像分类竞赛ImageNet上取得了压倒性的胜利。

这个胜利有多压倒性呢？用数字说话：AlexNet的错误率是15.3%，而第二名的错误率是26.2%。这不仅仅是稍好一点，这是一个相对错误率降低了40%左右。为了对比，2012年男子马拉松世界纪录是2小时3分23秒。如果突然有人把记录改到1小时33分，这将彻底改写体育史。AlexNet对计算机视觉做的正是这样的事情。

更令人惊讶的是，这个突破是在GPU上进行的。Krizhevsky等人利用当时新兴的GPU计算能力，训练了一个有约6千万个参数的卷积神经网络，在ImageNet的125万张图像上进行了几周的训练。在那之前，这被认为是不可能的——不是技术上不可能，而是实践上不可行。

计算机视觉领域在当时仍然被手工设计的特征管道主导。这些所谓的"特征工程"方法——比如从图像中提取特定的模式和纹理——在过去十年里一直是最先进的。就在AlexNet论文被发表的同一年前，有人在arXiv上发表了一篇关于"词袋"方法在图像分类中成功的论文，高度赞扬了这种手工特征方法的优越性。结果不到一年，深度学习就彻底摧毁了这一切。

我想强调的是，这个时刻有多么令人惊讶。当然，有一些研究者一直相信深度学习会成功。Geoffrey Hinton从1970年代就开始信奉这个理念。但这绝不是当时的共识。从Hinton的贡献来看，他是这个领域真正的先驱和持续的倡导者，但即使是对他来说，看到他多年的坚持在2012年突然被证明是正确的，也一定是令人欣慰的。

AlexNet的出现具有深刻的象征意义。它不仅展示了深度神经网络在实际任务上的超越能力，更重要的是，它改变了研究者的心态。它证明了，当你给神经网络足够的数据、足够的计算力和合理的架构时，它们可以超越几十年来精心打磨的手工方法。这个消息很快在学术界传播。如果深度学习在计算机视觉上这么有效，为什么不试试自然语言处理呢？

### 从视觉到语言

有趣的是，AlexNet使用的是卷积神经网络（CNN）。但NLP研究者们意识到，CNN在处理可变长度的序列（比如句子）时不是理想的。真正适合序列数据的是另一种架构：循环神经网络（RNN）。

2012年恰好也是RNN重获关注的转折点。Tomas Mikolov等人开始用RNN来构建语言模型，并发现它们的性能确实超过了N-gram。Ilya Sutskever（AlexNet的作者之一）等人也开始用RNN进行更复杂的NLP任务。虽然RNN本身不是新的概念——事实上，RNN的基本理念可以追溯到1980年代——但2012年前后的GPU技术使得训练更深、更大的RNN成为可能。

更重要的是，2012年也是Yoshua Bengio 9年前打下的种子终于开花结果的时刻。人们意识到，词向量（word embeddings）——即Bengio论文中提出的单词分布式表示——可以大大改进各种NLP系统。几个月后，Tomas Mikolov等人会发表Word2Vec论文，将词向量的计算从Bengio的昂贵的神经网络方法简化为可以在几小时内训练的快速算法。但那是另一个故事了。

重点是这样的：2003年，Bengio在概念层面证明了分布式表示和神经网络语言模型可以工作。2012年，硬件技术进步和AlexNet的成功证明了在实际规模上，这些方法可以工作得极其出色。这两个时刻缺一不可。一个是理论的胜利，一个是工程的胜利。

当这两个胜利汇合时，大语言模型时代的真正到来就已成为不可逆转的历史必然。