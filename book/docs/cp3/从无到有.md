---
sidebar_position: 6
---

## 能力与价值观的裂隙：对齐问题的出现

### 统计学习与道德价值的困境

Transformer架构的出现和规模化虽然解决了计算效率的问题，使得更大的模型训练成为可能，但这仅仅是大语言模型发展的一个方面。随着模型规模的扩大，一个更深层、更困难的问题随之凸显：**仅仅让模型变大、训练更长时间，并不能保证模型会按照人类的意图行动**。

考虑一个常见的聊天机器人场景。从统计学的角度看，互联网文本数据包含了极为丰富且相互矛盾的内容——既有诚实的回应，也有精心构造的谎言；既有尊重他人的讨论，也有充满仇恨的言论；既有有益的建议，也有会造成伤害的建议。一个基于下一个词预测目标进行训练的模型，在数学上没有任何理由偏向于诚实而非谎言，偏向于尊重而非仇恨，偏向于有益而非有害。模型所做的一切，不过是学习互联网文本的统计分布——它是一个无差别的学习器，对数据中的所有模式一视同仁。

这就是所谓的**对齐问题**（Alignment Problem）。模型的能力和人类的价值观、人类的意图之间存在一条深刻的裂隙。一个规模足够大、能力足够强大的系统，如果没有被正确地对齐到人类的具体意图和价值观上，可能会导致严重的后果。这个问题从理论上讲很简单，但从实践上讲却极其复杂。

早期的解决方案相对直接，主要利用有标注的数据集进行监督微调。研究者们会使用诸如ETHICS数据集或RealToxicityPrompts数据集这样的资源，其中包含了被明确标记为"好"和"坏"的模型回应示例。模型可以在这些数据上进行标准的有监督学习，从而学会避免有害的行为、倾向于更有益的输出。这个方法在早期取得了一些成果，展示了通过额外的有标注数据可以改进模型的行为。

然而，这个方法的局限性很快变得明显。首先，这样的标注数据集的规模必然有限，无法穷尽模型可能遇到的所有情况和场景。其次，对于许多核心的人类价值观，很难用清晰的、可量化的标注来定义。什么叫"有趣"或"乏味"？什么叫"深刻"或"肤浅"？什么叫"有帮助"或"具有误导性"？这些都是高度模糊、因人而异的概念，很难被精确地量化和统一标注。更进一步，不同文化、不同背景的人们对于"正确"行为的理解存在本质上的差异。一个全球化的AI系统如何能够同时满足所有这些不同的、有时相互矛盾的价值观？

### 人类反馈强化学习的诞生

2017年，与Transformer论文同年发表的关键工作，是Christiano及其同事的《深度强化学习来自人类偏好》（Deep Reinforcement Learning from Human Preferences）。这篇论文提出了一个优雅而富有启发性的解决方案：**强化学习与人类反馈**（Reinforcement Learning from Human Feedback，简称RLHF）。

RLHF的核心思想在于，与其试图用算法来定义什么是"好"或"坏"的行为，不如利用人类的偏好来直接定义奖励函数。这样，人类的价值判断被编码进了优化过程本身。具体的实现过程分为三个清晰的步骤，每个步骤都有其独特的目的。

**第一步：收集人类反馈**。给定一个具体的提示（Prompt），语言模型被要求生成多个候选回应。通常，模型会在相同的提示下独立生成4到8个不同的回应。然后，人类标注员评估这些回应的质量，通常不是通过绝对评分，而是通过对回应进行相对排序。例如，对于一个关于"如何撰写高质量学术论文"的问题，模型可能生成5个不同的回应，标注员根据实用性、准确性、清晰性等多维度因素对它们进行排序，从最优到最差。

**第二步：训练奖励模型**。现在，系统拥有了一个数据集，其中包含了模型生成的回应对和人类的偏好判断——哪些回应被认为比另一些更好。基于这个数据集，可以训练一个"奖励模型"（Reward Model）。这个奖励模型本身通常是一个神经网络，其目标是学习预测：给定一个特定的回应，人类评价者会认为它有多好。在数学上，奖励模型学习模拟人类的偏好函数——这是一个从文本空间到实数的映射，其中较大的值对应人类偏好的回应。

**第三步：强化学习微调**。现在系统已经拥有了一个可用的奖励函数。可以使用标准的强化学习算法（最常用的是近端策略优化，即PPO）来微调原始的语言模型。与原始的监督学习不同，目标不再是最小化下一个词预测的交叉熵损失，而是最大化奖励模型给出的分数。换句话说，模型被激励去生成能够获得高奖励分数的回应。通过这个过程，原始的预训练语言模型被逐步调整，以产生人类偏好的回应。

这个方法相对于直接的有监督学习具有一个至关重要的优势：**奖励模型本身可以在持续的人类反馈基础上进行迭代和更新，而不需要预先定义所有的规则或编写复杂的启发式方法**。当新的、意外的情况出现时，可以收集新的人类反馈，基于这个反馈重新训练奖励模型，然后再次进行强化学习微调。这创造了一个可扩展的、具有适应性的系统，理论上可以处理不断变化的、复杂的场景。

## RLHF的实践应用与对齐研究的扩展

### 工业应用中的RLHF标准化

在理论被提出后的几年内，RLHF迅速演变成了训练现代大语言模型的标准实践。几乎每一个主要的商业语言模型都采用了这种方法。OpenAI在2019年对GPT-2进行了RLHF微调。在2022年发表的论文《用人类反馈训练语言模型遵循指令》（Training Language Models to Follow Instructions with Human Feedback）中，研究人员详细描述了如何对GPT-3进行RLHF处理，创造了InstructGPT模型——这个模型虽然参数较少，但由于经过了适当的对齐，在用户研究中被认为比更大的原始GPT-3模型更有用。OpenAI还发表了《用人类反馈学习摘要》（Learning to Summarize from Human Feedback），展示了RLHF在具体文本摘要任务上的有效性。

甚至在OpenAI于2024年发表的GPT-4技术报告中，作者们也明确提到了RLHF的使用及其对模型性能和行为的影响。Google、Meta、Anthropic及其他从事大语言模型研发的主要机构，都已将RLHF集成到它们的模型开发流程中。RLHF已经从一个有趣的研究想法演变成了AI开发的工业标准。

然而，GPT-4的技术报告中有一个特别有趣且富有启发意义的细节。作者们明确写道：

> 该模型在考试中的能力似乎主要来自预训练过程，并不被RLHF显著影响。

这个简洁的表述深刻地揭示了一个关键事实：**大部分的核心能力来自于预训练阶段，而RLHF的主要作用是改进模型的行为、使其更好地与人类意图对齐，而非添加新的基础能力**。换个角度理解，RLHF是关于让模型更听话、更符合人类意愿，而不是让它更聪明、能力更强。这区分了两个完全不同的维度：*能力*与*对齐*。

### 对齐研究的多维探索

在RLHF之外，对于对齐问题的理解和解决方案在不断深化。Anthropic的研究人员在《通用语言助手作为对齐研究的实验室》（A General Language Assistant as a Laboratory for Alignment Research）中，系统地探讨了如何将"有益、诚实和无害"这三个核心原则有效地编码进语言模型中。在这项工作中，他们尝试了多种技术手段，包括模仿学习、二元判别、偏好排序、以及其他创新的微调方法。

研究结果表明，不同的技术在不同的场景下表现不同，没有单一的银弹解决方案。对齐是一个多维度、多层次的挑战，涉及技术、伦理、社会等多个方面的复杂权衡。最优的对齐方式仍然是学术界和工业界正在积极探索的开放问题。

## 四十年的积累：从神经网络复兴到规模的胜利

### 大语言模型发展的长期历史脉络

如果将视野扩大，从更宏观的历史维度看待当今大语言模型的出现，可以发现这是至少四十年科学研究积累的结果。这个故事的起源可以追溯到1980年代，当Geoffrey Hinton、David Rumelhart和Ronald Williams等先驱在分布式表示和反向传播算法上的理论工作。他们首次系统地论证了：复杂的认知现象可以通过分布式、分层的神经表示来实现。

故事的一个关键转折点出现在2003年。Yoshua Bengio及其同事发表了第一个使用神经网络的概率语言模型。这个工作在概念上看似简单，但却建立了神经网络与自然语言建模之间的联系。随后是一个漫长的等待期——大约十年间，这些基础想法都难以在任何显著的规模上得到实际应用。局限主要来自计算能力的不足和训练深层网络的技术困难。

2012年的ImageNet大赛成为了历史的分水岭。Alex Krizhevsky等人开发的AlexNet利用GPU进行深度学习训练，在图像分类任务上获得了压倒性的胜利，证明了深度学习在现代硬件上的可行性和威力。这个突破迅速扭转了整个领域的看法——深度学习从一个边缘的学术兴趣变成了主流方向。

三年后，即2015年，Dzmitry Bahdanau引入了注意力机制，解决了长序列建模的关键问题。又过了两年，2017年，Transformer架构问世。Transformer的核心创新是完全基于注意力的设计，消除了RNN的顺序处理瓶颈，使得模型可以充分利用现代GPU的并行计算能力。这个架构的简洁性和可扩展性，为后续的规模扩展奠定了基础。

从Transformer发布至今，主要的技术进展不再集中在架构创新上，而是在**规模、数据和训练方法**上。配合生成式预训练和RLHF这些训练技巧的完善，大语言模型以指数级的速度发展。整个演变轨迹可以用Feynman的一句话精准地总结："它不是复杂的。它只是很多。"

### 规模的指数级增长

当今，有多个主要组织在开发和部署大语言模型。OpenAI拥有GPT系列模型。Google开发了Gemini、PaLM和LaMDA等系列。Meta推出了LLaMA系列。Anthropic开发了Claude系列。此外，还有无数开源模型在社区中蓬勃发展，比如DeepSeek-R1在2024年底和2025年初创造了新闻，展现了开源模型的竞争力。

这些模型在具体细节上存在差异，采用了不同的训练策略、数据策略和对齐方法。但在本质上，它们都遵循相同的基本范式：一个大规模的Transformer架构，用下一个词预测目标进行大规模预训练，然后用RLHF进行微调以改进对齐。

真正的区别，如同许多AI研究者所观察到的，主要在于**规模**——参数数量、训练数据量、计算资源投入等关键指标上的差异。GPT-1（最初的生成式预训练Transformer）有1.17亿个参数。GPT-2增长到15亿个参数。GPT-3达到了1750亿个参数。有报告声称OpenAI还在研究规模更大的模型，参数可能达到数千亿级别。Google的Gopher在2021年拥有2800亿个参数。PaLM在2022年达到5400亿个参数。

而对于最新的模型如GPT-4、GPT-5或其他尚未公开发表的大型基础模型，具体的参数规模已经成为行业机密，主要出于商业竞争的考虑。但基于各种可获得的线索和行业分析，合理推测当代最先进的大型基础模型的参数规模已经达到了数万亿级别。

从GPT-1到GPT-3，参数数量增长了约1500倍。从GPT-3到推测中的下一代模型，参数增长幅度据信又是同等或更高的数量级。这种参数增长的速度和规模是史无前例的，体现了这个领域对于规模作为主要驱动力的坚定承诺。

## 规模律的奥秘与涌现能力的发现

### 规模律的实证研究

为什么规模如此重要和如此有效？这个问题的答案至今仍然包含着深刻的谜团。虽然实证证据清晰地表明规模确实有效，但对于*为什么*有效、规模的效用的基本原理是什么，理论理解仍然有限。

许多重要的研究工作都在试图理解和刻画"规模律"（Scaling Laws）这一现象。Jared Kaplan等人的《神经语言模型的规模律》（Scaling Laws for Neural Language Models）系统地研究了参数数量、数据集大小和训练计算量与模型性能之间的定量关系。Jack Rae等人的《语言模型的规模化方法、分析和Gopher训练的启示》（Scaling Language Models: Methods, Analysis and Insights from Training Gopher）进一步深化了这些分析。Jason Wei等人的《大型语言模型的涌现能力》（Emergent Abilities of Large Language Models）则提出了一个更深刻的观察。

这些论文反复验证了一个基本的观察规律：当模型的规模增大，用于训练的数据增加，计算资源增加时，模型的性能通常会以相对可预测的、幂律的方式改进。这本身并不特别令人惊讶——更大的模型有更多的参数来拟合更复杂的模式，这是符合直觉的。

但更令人惊异和深深困惑研究者的是，有时性能的改进并不是连续的、渐进的，而是**跳跃式的、阶段性的**。当模型规模跨过某个临界阈值时，模型突然学会了它在更小规模时完全无法做的事情。这个现象被称为**涌现**（Emergence）。能力似乎凭空出现，没有任何中间步骤。

一个具体而引人瞩目的例子是**链式思考推理**（Chain-of-Thought Prompting）。这是一种新颖的提示方法，通过强制模型逐步解释其推理过程中的每个步骤，而非直接给出最终答案，来显著改进模型的复杂推理能力。这个方法在2022年的研究中被详细研究，显示出了显著的效果。

但这里有一个关键的、高度反直觉的发现：这种提示方法在小模型上完全无效，甚至有害。对小模型应用链式思考提示会降低其性能。只有当模型达到大约100亿个参数时，链式思考提示才开始产生正面效果。而原始的Transformer论文发表时，模型的大小仅为1.3亿个参数。这意味着，需要一个大约**大50倍到100倍**的模型，才能从链式思考这个提示方法中受益。

这个现象深刻表明，模型能力的出现不是线性的。存在多个临界规模点，跨越这些点时，某些能力会突然激活。这暗示了某种深层的结构性变化正在发生，但当前的理论框架还不足以完全解释这一现象。

### Sutton的苦涩教训与其含义

这一切使人想起了Richard Sutton——强化学习的先驱之一——在2019年发表的一篇有影响力的博客文章《苦涩的教训》（The Bitter Lesson）。Sutton在这篇文章中论证了AI历史中反复出现的一个令人沮丧但深刻的"教训"：**通用的、计算高效的、可扩展的学习方法最终会战胜具有人类知识、领域特定的洞察和精心设计的启发式方法**。

这个教训之所以被称为"苦涩的"，是因为它具有深深的含义：专家知识、基于深厚领域理解的理论、手工设计的特征工程、优雅的数学理论、巧妙的算法——这些学术界和工业界长期以来引以为傲的成就，最终都会被相对蛮力的搜索、大规模并行计算和学习表示所超越。

这个模式在计算机科学的多个领域中重复出现。考虑计算机视觉的历史。在深度学习时代之前，该领域的研究者付出了数十年的努力，精心设计了各种特征提取算法、边缘检测器、形状描述符等。这些都是领域专家心血结晶的成果。然后，2012年的AlexNet出现，用纯粹的、黑箱般的深度学习方法彻底摧毁了这个精心构建的生态。在短短几年内，所有手工设计的特征都被神经网络学到的特征所取代。

类似地，考虑数学问题解决这个代表性的领域。早期的大语言模型在数学推理上表现得非常糟糕，这是众所周知的。该领域的研究者尝试了各种方法来改进模型的数学能力。但今天，最先进的语言模型正在赢得国际数学奥林匹克大赛的金牌。数学家Terry Tao将OpenAI的推理模型o1比作"中等水平的研究生"。这个转变发生在短短的三到五年内。

如果一个人觉得用"预测互联网文本中的下一个词"这样的目标来最终解决精英数学竞赛问题有点荒谬，如果这让一个人感觉像是一只"随机鹦鹉"在用某种统计技巧欺骗我们，那么这个人正在直观地经历早期语言学家和符号主义AI研究者面对统计语言建模兴起时的不适。这是Sutton所说的苦涩教训的当代表现。我们的专业知识感觉变得多余，我们对于"理解"和"推理"的直观感受在原始计算能力和大规模的统计学习面前变得无关。

## 对话与前景：专业知识的未来地位

### 专业知识与机器学习的协同

然而，对于本章的最后部分，观点是——特别是考虑到对这个故事的理解已经深入——目前的证据表明，机器学习系统仍然是强大的工具，可以与真正的、深厚的专业知识相结合，产生超越单独运用任何一方的成就。

最具说服力的例子是Google DeepMind开发的AlphaFold，在论文《用AlphaFold进行高度准确的蛋白质结构预测》（Highly Accurate Protein Structure Prediction with AlphaFold）中发表。AlphaFold在蛋白质三维结构预测这个经典问题上达到了接近实验精度的水平，这个问题困扰了结构生物学家超过五十年。

一方面，AlphaFold的成功是用黑箱深度学习做到的——它不是从第一原理推导的，不是基于物理定律，而是一个庞大的深度神经网络。另一方面，这项工作严重依赖于生物学的先前知识和生物信息学的理解。它巧妙地使用了进化相关蛋白的序列数据和已知的同源蛋白结构的3D坐标作为模型的输入特征。它在绕过所谓的"Levinthal的组合爆炸"（指如果按照所有可能的构象来搜索，蛋白质需要极长时间才能找到折叠的最低能量构象）时展现了一种深刻的聪明，即使我们不能完全解释它是如何做到的。

所以，将来会发生什么？关于AI的未来轨迹，即使是世界顶级专家也存在深刻的分歧。但如果有人应该获得发表最后意见的权利，那就是Geoffrey Hinton。

### Hinton对AI未来的观点

Geoffrey Hinton自1980年代以来就是神经网络的坚定倡导者和深度信徒。他与Yoshua Bengio和Yann LeCun一起在2018年获得了图灵奖，这个奖项部分正是为了表彰他们在本章中多次涉及的开创性工作。

在2024年接受BBC采访时，Hinton表达了一个大胆的、具有挑战性的观点：**大语言模型确实理解自然语言，而且它们可能是我们当前关于大脑如何理解和处理语言的最好的科学理论**。在他的框架中，大语言模型最终超越人类智能在许多维度上仅仅是时间问题。实际上，按照某些特定的指标和某些具体的维度，它们已经做到了——在某些数学问题上，在某些科学问题上，甚至在某些创意任务上。

## 回到开始：从McCulloch-Pitts到现在

故事的开始可以追溯到1943年。Warren McCulloch和Walter Pitts发表了第一个人工神经元的数学模型。他们的论文标题——《神经活动中固有的想法的逻辑演算》（A Logical Calculus of Ideas Immanent in Nervous Activity）——反映了他们宏大的、甚至有些渺茫的野心：用数学符号和逻辑来理解和模拟思维本身。

从那时到现在，已经过去了超过八十年。神经网络这个领域经历了多个寒冬，在这些时期，资金枯竭、期望破灭、许多研究者被迫放弃这条路线，转向其他看起来更有前景的方向。但也有一些人——Geoffrey Hinton、Yann LeCun、Yoshua Bengio等少数坚定的信徒——坚持不懈地工作，相信最终，大脑的基本原理是可以被理解、被抽象、被模拟的。

现在，我们拥有了能够进行复杂数学推理、理解历史文献、创作诗歌、编写和调试计算机代码的系统。这些系统从未被人类明确教导过这些具体的技能。它们学会这些技能，仅仅通过在包含数十亿或数万亿个词的互联网文本数据集上进行下一个词预测这样一个简单的任务。

这是一个非凡的、令人惊叹的、值得深思的成就。这值得被充分地标记、被庆祝、被严肃地思考。