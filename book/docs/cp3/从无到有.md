---
sidebar_position: 6
---

### 能力与意图的裂隙

但我们还没有讲完整个故事。仅仅让模型变大、训练更长时间，并不能保证模型会按照人类的意图行动。

想象一个聊天机器人。从统计学的角度，互联网上既有诚实的回应，也有谎言。既有尊重他人的讲话，也有仇恨言论。既有有益的建议，也有有害的建议。一个纯粹基于下一个词预测训练的模型，没有理由偏向于诚实而非谎言，尊重而非仇恨。它只是在学习互联网文本的统计分布。

这就是所谓的**对齐问题**（Alignment Problem）。模型的能力和人类的价值观之间存在一条裂隙。一个非常强大的系统，如果没有被正确地对齐到人类的意图上，可能会造成伤害。

早期的解决方案是利用有标注的数据集进行微调。比如，研究者们会使用ETHICS数据集或RealToxicityPrompts数据集，其中包含了"好"和"坏"的回应。模型可以在这些数据上进行监督学习，学会避免有害的行为。

但这个方法有明显的局限。首先，这样的数据集规模有限，无法涵盖所有可能的情况。其次，对于许多人类价值观，很难用一个清晰的标注来定义。什么叫"有趣"？什么叫"深刻"？这些都是模糊的概念，很难被量化和标注。

### 人类反馈强化学习

2017年，与Transformer同一年，Christiano等人发表了《深度强化学习来自人类偏好》（Deep reinforcement learning from human preferences）。这篇论文提出了一个优雅的解决方案：**强化学习与人类反馈**（Reinforcement Learning from Human Feedback，RLHF）。

核心思想是利用人类的偏好来定义奖励函数，而不是试图用算法来定义。具体的过程分为三步：

**第一步：收集人类反馈**。给定一个提示（Prompt），模型生成多个候选回应。然后，人类标注员评分这些回应，或者对它们进行排序。比如，对于一个关于如何写论文的问题，模型可能生成5个不同的回应，标注员根据质量对它们进行排序。

**第二步：训练奖励模型**。现在，我们有了一个数据集，其中包含了模型生成的回应和人类的偏好判断。我们可以训练一个"奖励模型"，这个模型学习预测给定一个回应，人类会多喜欢它。这个奖励模型本身可能就是一个神经网络，它学会了如何给任意的文本打分。

**第三步：强化学习微调**。现在我们有了一个奖励函数。我们可以使用标准的强化学习算法（比如近端策略优化，PPO）来微调原始的语言模型。目标不再是最小化下一个词预测的损失，而是最大化奖励模型给出的分数。

这个方法相对于直接的有监督学习有一个巨大的优势：**奖励模型本身可以在人类反馈的基础上学习，而不需要预先定义所有的规则**。当新的情况出现时，我们可以收集新的人类反馈，重新训练奖励模型。这是一个可扩展的、适应性强的系统。

### RLHF的实践

RLHF现在已经成为了训练大语言模型的标准实践。GPT-2在2019年用这种方式进行了微调。GPT-3在2022年的《用人类反馈训练语言模型遵循指令》（Training language models to follow instructions with human feedback）中也使用了RLHF。OpenAI还发表了《用人类反馈学习摘要》（Learning to summarize with human feedback），展示了RLHF在文本摘要任务上的应用。

甚至GPT-4的技术报告也明确提到了RLHF的使用。

但这里有一个有趣的细节。在GPT-4的技术报告中，作者们写道：

> 该模型在考试中的能力似乎主要来自预训练过程，不被RLHF显著影响。

这暗示了一个深刻的事实：**大部分能力来自预训练，而RLHF主要用于改进行为和对齐，而不是添加新的能力**。换句话说，RLHF是关于让模型更听话，而不是让它更聪明。

Anthropic的研究人员探索了对齐的其他方面。在《通用语言助手作为对齐研究的实验室》（A general language assistant as a laboratory for alignment）中，他们探讨了如何将"有益、诚实和无害"这些原则编码到语言模型中。他们尝试了多种技术，从模仿学习到二元判别到偏好排序。但最佳的对齐方式仍然是一个开放的研究问题。

### 四十年的积累

现在，让我们退一步，从更高的视角看这整个故事。

大语言模型是至少四十年研究的结果。故事开始于1980年代Hinton、Rumelhart和其他人关于分布式表示的工作。经过了关键的2003年，Bengio等人发表了第一个使用神经网络的概率语言模型。然后是漫长的等待——十年间，这些想法都无法在规模上付诸实践。

然后，2012年的AlexNet改变了一切。突然间，深度学习变成了一个可行的途径。三年后，2015年，Bahdanau引入了注意力机制。又两年，2017年，Transformer出现了，彻底简化了序列模型，使其能够充分利用并行计算。

从那时起，主要的进展不是架构上的创新，而是**规模**。更多的参数，更多的数据，更长的训练时间。加上生成式预训练和RLHF这些训练技巧，我们得到了今天的大语言模型。

整个故事可以用Feynman的一句话来总结："它不是复杂的。它只是很多。"没有单一的想法超出了一个聪慧的中学生的理解范围。分布式表示？这是一个直观的想法。Transformer中的注意力？这是一个优雅但不过度复杂的数学运算。下一个词预测？这是一个基础的统计概念。

但奇妙的、令人惊讶的、非凡的事物是，通过这些简单规则的相互作用，一个复杂的系统涌现了。这个系统能够做许多我们认为需要"理解"才能完成的事情。

### 规模的胜利

今天，有多个组织在开发大语言模型。OpenAI有GPT系列。Google有Gemini、PaLM和LaMDA。Meta有LLaMA。Anthropic有Claude。还有许多开源模型，比如DeepSeek-R1在2025年初创造了新闻。

这些模型在细节上有所不同，但本质上都是相同的：一个大规模Transformer，用下一个词预测进行预训练，然后用RLHF进行微调。

真正的区别在于规模。GPT-1有1.17亿个参数。GPT-2有15亿个参数。GPT-3有1750亿个参数。（虽然有报告称OpenAI还训练过更大的6.7亿参数的模型）。Google的Gopher在2021年有2800亿个参数。PaLM在2022年有5400亿个参数。

而我们并不知道GPT-4或GPT-5的确切大小，因为这些已经成为行业机密。但安全地说，当代的大型基础模型可能已经达到了数万亿个参数。

这个数字的增长速度是令人难以置信的。从GPT-1到GPT-3，参数增长了1500倍。从GPT-3到推测中的GPT-4或更大的未发表模型，增长幅度可能又是数十倍。

### 规模律的奥秘

为什么规模如此重要？答案至今仍然是一个谜。

有许多工作研究了"规模律"（Scaling Laws）。比如Kaplan等人的《神经语言模型的规模律》（Scaling laws for neural language models）、Rae等人的《语言模型的规模化方法、分析和Gopher训练的启示》（Scaling language models: methods analysis, and insights from training Gopher），以及Wei等人的《大型语言模型的涌现能力》（Emergent abilities of large language models）。

这些论文反复证实了一个观察：当你让模型变大，用更多数据训练，性能通常会以可预测的方式改进。但更令人惊讶的是，有时性能的改进不是渐进的，而是跳跃式的。模型突然学会了它之前完全无法做的事情。

比如，链式思考推理（Chain-of-thought Prompting）。这是一种新的提示方法，通过强迫模型解释每个推理步骤来改进它的推理能力。但有一个关键的发现：这种方法在小模型上没有任何帮助。对小模型的性能有害。只有当模型达到大约100亿个参数时，链式思考才开始产生帮助。而Transformer最初的大小只有1.3亿个参数。所以你需要一个大约大50000倍的模型，才能从链式思考中受益。

这个现象被称为**涌现**（Emergence）。能力似乎从某个阈值开始凭空出现。在这个阈值之前，模型是无法使用的。在这个阈值之后，它突然变得有用。

### 苦涩的教训

这一切让我想起了Richard Sutton发表的一篇著名的博客文章《苦涩的教训》（The Bitter Lesson）。Sutton是强化学习的先驱之一。在这篇文章中，他论证了AI历史中的"苦涩的教训"是：**通用的、计算高效的、可扩展的方法最终会战胜人类知识和领域特定的洞察**。

这个教训之所以"苦涩"，是因为它意味着专家知识、聪慧的领域特定理论、手工设计的特征、优雅的数学和漂亮的算法——这些都会被蛮力搜索和学习表示所超越。

想象早期的计算机视觉。研究者们精心设计了各种特征提取器和分类器。这是领域专家的心血。然后AlexNet出现了，用纯粹的深度学习彻底摧毁了这一切。

类似地，考虑数学。早期的大语言模型在数学上表现很差。但今天，最先进的模型正在赢得国际数学奥林匹克金牌。著名数学家Terry Tao将o1（OpenAI的一个推理模型）比作"中等水平的研究生"。这个变化发生在短短几年内。

如果你觉得用下一个词预测这样的目标来解决精英数学问题有点荒谬，如果这让你觉得这是一只"随机鹦鹉"在欺骗你，那么你可能会感受到早期语言学家对统计语言建模的不适感。这是对苦涩教训的直观感受。我们的专业知识感觉是多余的，我们对"理解"的直觉在原始计算能力面前变得无关。

### 但专业知识仍然重要

不过，我的观点是——既然你已经读到这里了——目前为止，机器学习系统仍然是强大的工具，可以与真正的专业知识结合。

最好的例子是Google DeepMind的AlphaFold，发表在《用AlphaFold进行高度准确的蛋白质结构预测》（Highly accurate protein structure prediction with AlphaFold）。这个模型在蛋白质预测问题上达到了接近实验精度。

一方面，它是用黑箱深度学习做到的。另一方面，这项工作严重依赖于先前的生物学知识。比如，它使用了进化相关蛋白的序列和同源结构的3D坐标作为模型的输入。它巧妙地绕过了Levinthal的组合爆炸，即使我们不知道它如何做到的。

所以，将来会发生什么？即使是世界顶级专家也存在分歧。但我认为，如果有人应该发表最后意见，那就是Geoffrey Hinton。

### Hinton的预言

Hinton从1980年代以来就是神经网络的坚定信徒。他与Yoshua Bengio和Yann LeCun一起在2018年获得了图灵奖，部分原因正是他们在本文中提及的工作。

2024年，在BBC的一次采访中，Hinton表达了一个大胆的观点：**大语言模型确实理解自然语言，而且它们是我们目前关于大脑如何理解语言的最好理论**。在他的观点中，大语言模型最终超越人类智能只是时间问题。事实上，按照某些指标和某些维度，它们已经做到了。

### 回到开头

让我们回到这个故事的开头。1943年，McCulloch和Pitts发表了第一个人工神经元的数学模型。他们的论文标题是《神经活动中固有的想法的逻辑演算》。这个标题反映了他们深刻的野心：用数学来理解思维本身。

从那时到现在，已经过去了80年。神经网络经历了多个冬天，当资金枯竭、期望破灭时，许多研究者被迫放弃。但也有一些人——Hinton、LeCun、Bengio——坚持不懈地工作。他们相信，最终，大脑的原理可以被理解和模拟。

现在，我们有了能够进行数学推理、理解历史、写诗、编程的系统。这些系统从未明确被教导这些技能。它们学会这些，仅仅通过在海量的互联网文本上进行下一个词预测。

这是一个非凡的成就。这值得被标记和庆祝。

### 结语

这一章讲述的是大语言模型如何从最简单的想法中涌现而来的故事。从Bengio在2003年的分布式表示到今天的GPT-4，每一步都是逻辑上的、渐进的改进。没有任何一个步骤是不合理的。但当所有这些步骤累积在一起，当规模被推向极端时，某种新的东西出现了。

这就是人工智能的现状：不是因为任何单一的天才洞察，而是因为简单想法的长期、系统的应用。不是因为复杂的理论，而是因为坚持不懈的工程努力和充足的计算资源。

如果你感到困惑、惊讶或不安，这是可以理解的。因为许多领先的研究者也有同样的感受。我们正在目睹一个历史的转折点。我们正在见证一个不再完全由人类智能驱动的世界的开始。

这个世界会是什么样的？没有人真正知道。但我们可以确定的是，理解我们如何走到这一步是理解我们要去哪里的第一步。

这，正是这一章试图做的。