---
sidebar_position: 5
---

## 一个简洁而大胆的主张：注意力就是一切所需

### 历史时刻的宣言

2017年6月，来自Google Brain的一个研究团队发表了一篇论文。其标题简洁而大胆，仅仅四个单词：**《注意力就是一切所需》**（Attention Is All You Need）。

这个标题本身远不仅仅是论文的名称，它是一个宣言，一个对整个领域的直接挑战。标题直指问题的核心，以最少的词汇表达了最大的雄心。这篇论文的作者包括Ashish Vaswani、Sharan Katagiri、Lukasz Kaiser、Illia Polosukhin和其他研究者。他们提出的主张极其激进和具有革命性意义：

> 目前在序列转导任务上表现最优的模型基于复杂的循环神经网络或卷积神经网络，通常在编码器-解码器的配置中。性能最佳的模型也通过注意力机制来连接编码器和解码器。我们提议了一个新的、简洁的网络架构——Transformer，它完全基于注意力机制，完全摒弃了循环和卷积。

这不是一个温和的建议或谨慎的改进。这是对整个领域既定智慧的直接挑战。在2017年的背景下，RNN编码器-解码器架构已经成为了NLP领域的正统教义。无论有多少架构的改进和技巧的创新，整个研究社区都没有人质疑这样一个基本信念：**循环结构是处理序列数据的必要和基础的条件**。毕竟，从一个直观的角度，序列就是其名字所示——一个顺序的、时间相关的数据结构。而循环神经网络正是按照这个顺序逐步处理信息的架构。放弃循环结构似乎就是放弃了序列本身的基本特性。

但Vaswani和他的团队提出了一个非常不同的、根本性的问题：**我们真的需要循环吗？** 这个看似简单的问题背后隐含着对一个整个领域假设的质疑。

### 可扩展性的困境与并行化的力量

为什么这个根本性的问题恰好在2017年被提出，而不是更早或更晚？答案在于**可扩展性**问题变得越来越紧迫和不可忽视。

Vaswani团队在论文中明确指出了RNN所继承的根本限制：

> 循环模型通常沿着输入和输出序列的符号位置来分解计算。将位置对齐到计算时间的步骤，这些模型生成一系列隐藏状态$h_t$，其中每个$h_t$是前一个隐藏状态$h_{t-1}$和位置$t$处的输入的函数。这种内在的顺序特性排除了在训练示例内的并行化，这在较长序列长度的情况下变得至关重要，因为内存限制限制了示例之间的批处理。

这个技术描述的含义是深刻的。由于RNN固有的顺序处理特性，当处理一个有100个单词的句子时，模型必须进行恰好100个顺序的计算步骤。即使拥有最强大的GPU或最高端的硬件，这个根本的限制也无法绕过。在深度学习时代，当需要处理数十亿个训练样本时，这个顺序性变成了一个不可容忍的、严重的系统瓶颈。

与RNN形成鲜明对比的是，注意力机制具有完全不同的计算特性——它**本质上是高度并行化的**。给定所有的查询（Query）、键（Key）和值（Value）矩阵，可以在一个或少数几个矩阵乘法运算中同时计算所有的注意力权重。GPU正是为什么而设计的？就是为了进行大规模、并行的矩阵运算。这两者的配合是天然的和完美的。

这个观察与我们之前看到的另一个历史模式形成了有趣的呼应——Word2Vec之所以比Bengio的神经网络语言模型更成功，不仅是因为它的架构更简单，还因为它能够被扩展到数十亿单词的大规模数据集。现在，Transformer提出的是同样的策略但应用于整个序列模型：**简化架构并使其能够充分利用现代硬件的大规模并行计算能力**。

Vaswani团队提供的实验证据清楚地展示了这个优势：

> Transformer架构允许进行显著更多的并行化，在仅有8块NVIDIA P100 GPU上进行训练，仅需12小时就能在机器翻译质量上达到新的技术水平。

相比之下，许多基于RNN的竞争模型需要花费数周的训练时间。从数小时到12小时。从12小时到数周。这不仅仅是速度上的改进——这是一个数量级的差异，是一个基础性的改善。

## Transformer架构的设计与创新

### 架构的整体设计逻辑

从表面上看，Transformer的架构看起来相当复杂，充满了各种不同的组件和层次。但如果追踪其发展的逻辑和来龙去脉，其核心其实相当直接和优雅。核心思想可以用一句话总结：**保留Seq2Seq模型的编码器-解码器框架的整体逻辑，但用注意力机制替代掉所有其他的、复杂的部分——特别是循环结构**。

编码器部分由多个完全相同的堆叠层组成。每一层都让序列中的每个位置能够"看到"、能够关注序列中的所有其他位置，并学习为这些不同的位置赋予不同的注意力权重。这样，信息可以在序列内部任意距离的位置之间自由流动。

解码器部分也采用了类似的多层堆叠设计，包含自注意力机制，但包含了一个关键的、技术上重要的修改：**掩码**（Masking）机制。这个修改是必要的，因为解码器采用自回归的方式从左到右逐词生成输出。在任何给定的时间步，解码器不应该被允许"看到"或关注未来的、尚未生成的词。否则，模型就能"作弊"——通过直接观察它应该预测的内容来进行预测。为了防止这种泄露，Transformer使用掩码注意力，有系统地遮挡掉所有未来位置的信息。

编码器和解码器之间存在**跨注意力**（Cross-Attention）连接，这与Dzmitry Bahdanau在2015年首次提出的注意力机制在功能上完全相似。这个跨注意力允许解码器在生成每个输出词时关注编码器的输出，从而从编码器提供的表示中"对齐"和提取相关的源序列信息。

### 位置编码的必要性

Transformer架构中有一个在某些方面显得巧妙但同时也令人惊讶的组件，叫做**位置编码**（Positional Encoding）。这个组件存在的必要性源于一个微妙但重要的事实：纯粹的注意力机制本身没有固有的顺序性或位置意识。

为了理解这个问题的严重性，考虑这样一个例子：一个包含词序列[A, B, C]的输入和一个包含词序列[C, B, A]的输入。从纯粹的注意力计算的角度，如果它们使用完全相同的词向量，那么这两个输入在没有额外信息的情况下是完全相同的。注意力不知道哪个词是第一个、哪个词是最后一个。为了保留和传达序列中的相对位置信息，Transformer为输入序列中每个位置的词向量添加了位置相关的向量。

这些位置编码向量被巧妙地设计为使用正弦和余弦函数的组合，这样可以捕捉相对位置的信息。具体的公式是：

$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/D})$$
$$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/D})$$

其中pos是位置，i是维度，D是模型维度。这个设计的优雅之处在于，它能够让模型学到相对位置的概念。

### 多头注意力与扩展

Transformer的另一个关键创新是**多头注意力**（Multi-Head Attention）。与其使用单一的、单一的注意力函数，Transformer并行地运行多个独立的注意力函数（通常称为"头"，典型情况下有8个或16个头），每个头学习不同的表示方式和关注点。

这个想法在理论上并不是全新的——事实上，在Bahdanau原始的注意力论文中，作者们原则上可以这样做。但由于计算成本的限制，他们没有在实践中这样做。但在Transformer架构中，由于其计算的高效性和矩阵化的实现，多头注意力变成了标准做法，而且成为了成功的关键因素。不同的注意力头可以学会关注序列的不同方面——某个头可能学会关注句法关系，另一个头可能学会关注语义相似性，第三个头可能学会关注长距离的依赖。

Vaswani等人用一个优雅的数学公式定义了缩放点积注意力（Scaled Dot-Product Attention）：

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{D_k}}\right)V$$

这里Q是查询矩阵，K是键矩阵，V是值矩阵，D_k是键的维度。分母中的$\sqrt{D_k}$是一个缩放因子，其目的是防止点积值变得过大——大的点积值会导致softmax函数的梯度变得过小，这是一个数值稳定性的问题。整个计算可以在一个或少数几个高度优化的矩阵运算中完成，因此在现代硬件上极其高效。

## Transformer出现的历史脉络与必然性

### 思想的继承与积累

理解Transformer的真正关键在于将其正确地置于完整的历史背景中。Transformer绝不是一个凭空出现的、与前期工作无关的天才灵感的产物。恰恰相反，它是一系列想法、发现和创新在十多年中逐步积累的自然而必然的结晶。

追溯这些想法的谱系：首先，Yoshua Bengio在2003年提出的**分布式表示**的基本思想——通过连续向量来表示词的含义。其次，Tomas Mikolov在2013年提出的**简化和扩展**的思想——Word2Vec展示了如何通过简化复杂的架构，使得模型能够扩展到大规模数据。第三，Dzmitry Bahdanau在2014年提出的**注意力机制**——解决了长序列中的信息压缩瓶颈。第四，Minh-Thang Luong在2015年进行的**系统化研究**——对注意力的多个维度进行了全面分析。第五，Cheng等人在2016年提出的**自注意力**——展示了注意力可以应用于序列内部本身。

Transformer将所有这些想法综合在一起，去除了RNN这个最后的复杂性残留。从这个角度看，Transformer是一个**概念上的飞跃**吗？是的。但更准确地说，它是一个**合理的、可预期的、逻辑上必然的飞跃**。在这个时间点，在这个计算能力充足的时代，这样的架构的出现变成了历史的必然。

### 首个验证与实际影响

当这篇Transformer论文在2017年NeurIPS发表时，其反应并不是像AlexNet那样的震撼性胜利。Transformer在机器翻译基准测试上表现更好，但这种改进并不是压倒性的、一骑绝尘的。从数字上看，改进是显著但不是革命性的。

真正的胜利在于**计算效率**和**可扩展性**。论文对比了同时期的另一个摒弃循环、转而采用卷积来处理序列的模型ConvS2S。在某些翻译质量基准上，两个模型表现相当接近。但在计算效率上，Transformer取得了压倒性的胜利：所需的浮点运算数减少了**360倍**！

这正是大模型时代的关键洞察所在。有时候，进步不来自于争取每一个百分点的性能提升。有时候，最重要的改进来自于架构的简化和效率的提升，特别是当这种简化使得模型能够被扩展到前所未有的规模时。**在可扩展性的前沿，"足够好"加上"高效"就足以改变一切**。

## 生成式预训练与微调范式的出现

### 预训练加微调的范式转变

但仅有Transformer架构本身还不够，不足以创造出今天人们与之互动的、具有强大能力的大语言模型。还需要另一个同样重要、甚至更加根本的创新：**如何有效地训练这些模型**。

这个答案在2018年得到了明确的表述，恰好在Transformer论文发表的一年后。OpenAI发表了一篇论文，标题是《通过生成式预训练改进语言理解》（Improving Language Understanding by Generative Pre-Training）。这篇论文提出了一个看似简单但实际上深刻的、革命性的想法：

采用**两阶段的训练方法**。第一阶段，用尽可能多的、从互联网收集的无标注文本数据，对一个Transformer模型进行预训练。第二阶段，取这个预训练好的模型，然后用特定任务的、带有标注的有监督数据进行微调，使模型适应特定的下游任务。

第一阶段被称为**生成式预训练**（Generative Pre-training）。在这个阶段，目标仍然是我们从Bengio时代就已经知道的东西：最大化对数似然，也就是最小化下一个词预测的损失函数：

$$L_{GPT}(\Theta) = \sum_{t=1}^{T} \log p_\Theta(w_t | w_{t-N:t-1})$$

这个预训练过程是完全无监督的。互联网上拥有数十亿、甚至数万亿的文本，所有这些文本都可以被用来进行这个预训练任务。不需要人工标注者，不需要昂贵的数据标注过程，只有纯粹的、自回归的语言建模。

第二阶段被称为**判别式微调**（Discriminative Fine-tuning）。在预训练完成后，预训练的模型被应用到具体的、特定的下游任务上——比如问答、情感分析、文本摘要、命名实体识别等。在这个阶段，我们拥有真实的(输入，输出)标注对，可以用监督学习的标准方法来训练和调整模型，使其对特定任务进行优化。

为什么这两个步骤都是必要的？为什么不能只进行第一步？想象一个纯粹通过生成式预训练训练出来的模型。这个模型确实能够相当好地预测句子中的下一个词。但这未必意味着它能够做一些有实际用处的事情。例如，考虑一个关于精神健康建议的用户查询。模型可能会基于从互联网文本的统计规律生成一个冷漠的、甚至可能有害的回答——因为从互联网文本中，这样的回答在统计上是"可能的"。因此，第二个阶段的微调是必要的，目的是将模型的行为与人类的真实意图对齐。

### 预训练概念的历史溯源

但这里有一个有趣而值得思考的历史细节：生成式预训练的核心概念本身并不是全新的发明。这个想法实际上有着相当长的历史。

早在2006年，Yoshua Bengio等人在论文《深度网络的贪心逐层训练》（Greedy Layer-Wise Training of Deep Networks）中就已经系统地讨论了预训练的想法：

> 我们假设这个策略的三个方面特别重要：首先，以贪心的逐层方式进行预训练；其次，在每一层使用无监督学习来保留和学习输入中的重要信息；最后，根据最终的、具体的任务目标对整个网络进行微调。

这段引用清楚地表明，预训练加微调的基本范式在至少十年前就已经被学术界讨论过了。更早的工作中，研究者们就已经在使用预训练的词向量，然后在特定任务上进行微调。我们之前讨论过的ELMo（在2018年发表）正是这个范式的又一个实例。

那么，OpenAI在2018年做的究竟是什么新创新？严格来说，他们并没有发明一个本质上全新的概念。他们所做的是**在前所未有的、巨大的规模上应用和完善这个已经存在的概念**。他们用尽可能多的互联网文本数据，采用最先进的架构（Transformer），配合巨大的计算资源，来进行这个生成式预训练。这个**规模上的质变**导致了**能力上的质变**——前所未有的大规模预训练产生了前所未有的语言理解和生成能力。

## 从GPT系列到能力的涌现

### 参数规模与数据的指数级增长

在生成式预训练范式被确立之后，接下来的发展遵循了一个相当直接而清晰的轨迹：不断扩大模型的规模和数据量。

GPT-1（2018年）包含1.17亿个参数，在一个包含约800万个网页的数据集上进行训练。从历史的长期视角，这个规模可能看起来相对较小，但在2018年，这是完全前所未有的、开创性的规模。

一年后，2019年，OpenAI发表了GPT-2。论文的标题是《语言模型是无监督的多任务学习者》（Language Models Are Unsupervised Multitask Learners）。这篇论文中的关键发现改变了人们对语言模型能力的理解：一个用Transformer架构训练的、足够大的语言模型，在许多下游任务上都表现出色——**甚至完全不需要进行任务特定的微调**！这种能力被称为"零样本"学习：模型可以被直接应用到它从未见过、从未被明确训练过的任务上，仍然能够产生有意义的、有用的结果。

GPT-2有15亿个参数，在一个包含40亿个网页的更大数据集（WebText）上进行训练。相比GPT-1，这已经是参数数量增加了大约10倍，训练数据也相应地大幅增加。性能的改进是显著的，不仅是量的增加，更是质的提升。

然后在2020年，OpenAI发表了GPT-3的论文，标题是《语言模型是少样本学习者》（Language Models Are Few-Shot Learners）。GPT-3代表了这个时期大语言模型发展的顶峰。它包含1750亿个参数，这是一个接近两万亿的数字。训练数据包括Common Crawl、WebText2和其他多个来源，总计约45TB的文本数据。

论文中的一个关键声明是：

> 对于所有的任务，GPT-3都在没有任何梯度更新或微调的情况下进行了评估。

这个陈述对许多人、甚至对许多AI研究者来说都是震撼性的。一个单一的、经过预训练的、从未在特定任务上微调过的模型，不做任何内部参数的调整，就能在翻译、问答、数学推理、代码生成、摘要、创意写作等各种不同的任务上表现得相当不错，甚至在某些任务上表现出令人惊讶的能力。这种现象暗示了一个深刻的、令人困惑的真理：**在大型语言模型中，似乎存在着某种通用的、任务无关的"智能"或"理解能力"，这种能力可以被不同的提示方式激活，以完成许多不同种类的任务。**

从GPT-1的1.17亿参数到GPT-3的1750亿参数，增长了大约**1500倍**。这个参数数量上的爆炸性增长伴随着能力的质的飞跃。这不仅仅是一个量的增加——模型变大了，参数多了。更重要的是，某种新的、更高级的、之前不存在的能力似乎从这个规模的增长中涌现了出来。

### 缩放律与神秘的涌现现象

在GPT-3论文和随后许多关于大型语言模型的研究中，一个重要的科学规律变得越来越清晰和明确：**当神经网络变得更大、用更多数据训练时，其性能通常会以一种相对可预测的、遵循幂律的方式改进**。这个规律被称为"缩放律"（Scaling Laws）。

但更加令人惊讶和困惑的现象是，有时候这种性能改进不是平缓的、连续的。有时候，性能会在某个特定的规模阈值突然、剧烈地跳跃。模型突然学会了它在更小规模时完全无法做的事情。这个现象被称为**涌现**（Emergence）。

涌现是大型语言模型中最神秘、最令人困惑、最令人着迷的现象之一。具体的例子包括：当模型规模达到某个阈值时，模型突然学会了进行多位数的数学加法运算；当规模进一步增加时，模型突然学会了链式思考推理——通过逐步明确其推理过程来解决复杂问题；还有许多其他看起来出人意料的能力突然出现。

为什么会这样？为什么一个较小的模型完全无法进行多位数加法，而仅仅增加参数数量到某个特定阈值的稍大模型就能？如果我们增加的只是参数数量和训练数据量，没有改变基础的架构或目标函数，为什么会出现完全新的、之前不存在的能力？这仍然是一个开放的、深刻的科学问题，目前还没有被充分理解。

但无论其内在机制如何，涌现这个现象是不可否认的、反复被验证的。当模型规模达到某个临界阈值时，新的能力似乎就从中浮现出来。这个观察对AI安全、AI能力预测和AI治理都具有深刻的含义。

## 一个范式的转变与新时代的到来

### 从特殊设计到通用预训练

从更宏观的、范式层面回顾，Transformer的出现和生成式预训练的应用标志着深度学习在自然语言处理中的真正、完全的胜利，以及一个全新研究范式的开始。

在Transformer出现之前的NLP领域，主流的研究思想是：为不同的任务和问题设计特殊的、专门化的架构和目标函数。想要完成机器翻译任务吗？需要设计一个特定的编码器-解码器架构。想要进行文本分类任务吗？需要添加一个适当的分类头和损失函数。想要进行问答任务吗？又需要另一个特殊的、针对性的设计。这种方法要求研究者具有深刻的、对具体任务的理解，需要精心设计模型。

但Transformer和生成式预训练改变了这一切，根本性地改变了这一切。这个新范式的核心主张是：采用一个通用的、与任务无关的架构（Transformer），使用一个简单的、通用的目标函数（下一个词预测），在足够大的规模和足够多的数据上进行预训练。一旦完成了这个通用的预训练，同一个模型可以通过简单的适配或提示就被应用到许多不同的、多样化的下游任务上。这是一个从"许多特殊的、定制化的模型"向"一个通用的大模型加上轻量级适配"的转变。

这个转变的含义是深远的。它意味着从这一刻开始，NLP不再是关于巧妙地、创意性地设计特殊模型和特殊损失函数的领域。它根本性地演变成了一场关于**数据、计算能力和模型规模**的竞争。谁能获得最多、最高质量的数据？谁能获得最好的硬件进行最长时间、最密集的训练？谁能部署最多的计算资源来处理最大的模型？谁就能构建最强大、最有能力的模型。

这场竞争是激烈的、持久的、充满资源需求的。它直接推动了GPU市场的爆炸性增长。它改变了什么样的研究机构能够进行什么样的工作。它集中了AI能力和AI研究资源到拥有巨大资金和计算资源的少数大型公司。

但最重要的是，这个转变证实了一个古老的、一直存在于计算机科学中的直觉是正确的——当面对一个复杂的、多面的问题时，最好的、往往是唯一真正有效的解决方案不是更聪明的算法设计或更复杂的启发式方法，而是**更大的规模、更多的数据和更多的计算**。这个直觉从计算机科学诞生以来就一直存在，但它从来没有像在大语言模型这个领域中那样彻底、那样全面、那样无可争议地被验证和证实。

### 时代的标志与未来的预示

而这个根本性的转变，这个范式的重组，正是我们所在时代的最根本的特征。大语言模型的出现和快速发展，不仅改变了自然语言处理，更改变了整个人工智能领域，甚至开始改变人类社会本身。这是一个转折点，一个历史的分水岭。