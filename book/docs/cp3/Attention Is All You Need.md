---
sidebar_position: 5
---

### 一个简洁而大胆的主张

2017年6月，Google Brain的一个研究团队发表了一篇论文。标题简洁而大胆，就四个单词：《注意力就是一切所需》（Attention Is All You Need）。

这个标题本身就是一个宣言。它直指问题的核心，同时挑战了当时NLP领域的共识。这篇论文的作者包括Ashish Vaswani、Sharan Katagiri、Lukasz Kaiser、Illia Polosukhin和其他人。他们的主张非常激进：

> 目前主流的序列转导模型基于复杂的循环或卷积神经网络，包括编码器和解码器。性能最好的模型也通过注意力机制连接编码器和解码器。我们提出了一个新的简洁网络架构——Transformer，它完全基于注意力机制，完全摒弃了循环和卷积。

这不是一个温和的建议。这是一个对整个领域的直接挑战。在2017年，RNN编码器-解码器已经是正统教义。无论有多少改进，没有人怀疑循环结构是处理序列的必要条件。毕竟，序列就是——按顺序的。而循环神经网络就是按顺序处理信息的方式。

但Vaswani的团队问了一个根本性的问题：**我们真的需要循环吗？**

### 为什么是现在？

这个问题为什么在2017年被提出，而不是更早或更晚？

关键在于**可扩展性**。Vaswani团队指出了RNN的根本限制：

> 循环模型通常沿着输入和输出序列的符号位置分解计算。将位置对齐到计算时间中的步骤，它们生成一系列隐藏状态h_t，作为前一个隐藏状态h_{t-1}和位置t的输入的函数。这种内在的顺序性质排除了在训练示例内的并行化，这在较长序列长度时变得至关重要，因为内存限制限制了跨示例的批处理。

换句话说，RNN的顺序性是致命的瓶颈。当处理一个100个词的句子时，你必须进行100个顺序步骤。即使你有最强大的GPU，也无法绕过这个限制。而在大模型时代，处理数十亿个样本时，这个顺序性成为了不可容忍的瓶颈。

但注意力机制不同。注意力是高度并行化的。给定所有的查询、键和值，你可以在一个矩阵乘法中同时计算所有的注意力权重。GPU擅长做什么？就是进行大规模的矩阵运算。

这正是Word2Vec成功的另一个原因。回忆一下，Word2Vec之所以胜过Bengio的方法，不仅是因为它更简单，还因为它能扩展到大数据集。现在，Transformer提出的是同样的策略应用于序列模型：**简化架构，使其能够充分利用现代硬件的并行计算能力**。

Vaswani团队的结果震撼了整个领域：

> Transformer允许显著更多的并行化，在8个P100 GPU上仅训练12小时后就能达到机器翻译质量的新技术水平。

从数小时到12小时。相比之下，许多RNN模型需要几周才能训练。这不仅是速度上的改进，这是一个数量级的差异。

### 架构：简洁但强大

Transformer的架构看起来很复杂，但如果理解了来龙去脉，其实相当直接。核心思想是：**保留编码器-解码器框架，但用注意力替代所有其他部分**。

编码器由多层自注意力组成。每一层都让序列中的每个位置能够看到序列中的所有其他位置，并学习对不同位置赋予不同的注意力。解码器也有自注意力，但增加了一个关键的修改：**掩码**。因为解码器是从左到右逐词生成输出的（自回归框架），我们不能让某个位置"看到"未来的词。所以我们使用掩码注意力，遮挡掉所有未来的位置。

编码器和解码器之间有跨注意力，这完全类似于Bahdanau提出的注意力机制。解码器中的每个词都可以关注编码器的输出，从而"对齐"源序列中的相关部分。

还有一个关键的部分叫做**位置编码**（Positional Encoding）。这是必要的，因为纯粹的注意力机制没有固有的顺序性。一个包含词A、词B、词C的序列和包含词C、词B、词A的序列，在纯粹的注意力下是相同的。为了保留序列的顺序信息，Transformer为每个位置的输入添加了位置相关的向量。这些向量被设计为捕捉相对位置的信息。

多头注意力是另一个关键部分。与其使用单一的注意力函数，Transformer并行运行多个注意力函数（比如8个或16个"头"），每个头学习不同的表示方式。这让模型能够同时关注序列的不同方面。这个想法并不新颖——在Bahdanau的论文中，作者们实际上可以这样做，只是因为计算成本太高而没有这样做。但在矩阵化的Transformer中，这变成了标准做法。

Vaswani用一个优雅的公式定义了缩放点积注意力：

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{D_k}}\right)V$$

这里Q是查询矩阵，K是键矩阵，V是值矩阵。那个$\sqrt{D_k}$的缩放因子是为了防止点积变得过大（这会导致softmax的梯度过小）。整个计算可以在一个矩阵操作中完成，极其高效。

### 上下文的力量

理解Transformer的关键在于将其置于正确的历史背景中。Transformer不是凭空出现的天才灵感。它是一系列想法的自然结晶：

- Bengio关于分布式表示的想法（2003年）
- Word2Vec关于简化和扩展的想法（2013年）
- Bahdanau关于注意力机制的想法（2014年）
- Luong系统化注意力的想法（2015年）
- Cheng关于自注意力的想法（2016年）

Transformer将这些想法整合在一起，去除了RNN这个最后残留的复杂性。这是一个概念上的飞跃吗？是的。但它是一个**合理的、可预期的飞跃**。

当这篇论文被发表时，它并不是AlexNet那样震撼性的胜利。Transformer在机器翻译基准测试上表现更好，但并不是压倒性的改进。真正的优势是效率。论文对比了同时期的另一个摒弃循环、转而使用卷积的模型ConvS2S。在某些基准上，两者的翻译质量接近。但在计算效率上，Transformer赢得了压倒性的胜利：所需的浮点运算数减少了360倍！

这正是大模型时代的关键洞察：**在可扩展性的前沿，有时候"足够好"加上"高效"就足以改变一切**。

### 生成式预训练的时代

但是，仅有Transformer架构还不够。要创造今天人们与之互动的大语言模型，还需要另一个关键创新：**如何训练这些模型**。

2018年，一年之后，OpenAI发表了《通过生成式预训练改进语言理解》（Improving language understanding by generative pre-training）。这篇论文提出了一个看似简单但深刻的想法：用尽可能多的无标注数据预训练一个Transformer，然后用特定任务的有标注数据进行微调。

第一步叫做**生成式预训练**（Generative Pre-training）。目标仍然是我们从Bengio时代就知道的东西：最大化对数似然，也就是最小化下一个词预测的损失：

$$L_{GPT}(\Theta) = \sum_{t=1}^{T} \log p_\Theta(w_t | w_{t-N:t-1})$$

这完全是无监督的。互联网上有数十亿的文本，都可以被用来进行这个任务。没有标注者，没有昂贵的数据标注成本，只有纯粹的自回归语言建模。

但这不是全部。第二步叫做**判别式微调**（Discriminative Fine-tuning）。在预训练之后，模型被应用到具体的任务上，比如问答、情感分析、文本摘要等。在这一步，我们有真实的(输入, 输出)对，可以用监督学习来训练模型做特定的任务。

为什么这两步都需要？想象一个用生成式预训练训练出来的模型。它能预测句子中的下一个词。但这未必意味着它能做有用的事情。比如，一个关于心理健康建议的查询可能得到一个冷漠甚至有害的回答，因为从互联网文本的统计角度看，这样的回答是"可能的"。所以我们需要微调来将模型对齐到人类的意图。

### 预训练概念的历史

但这里有一个有趣的历史细节：生成式预训练的想法并不新。

早在2006年，Bengio等人在《深度网络的贪心逐层训练》（Greedy layer-wise training of deep networks）中就写道：

> 我们假设这个策略的三个方面特别重要：首先，以贪心的方式逐层预训练；其次，在每层使用无监督学习来保留输入信息；最后，根据最终目标准则微调整个网络。

甚至更早的工作中，研究者们就使用预训练的词向量，然后在特定任务上微调。还记得ELMo吗？那就是预训练然后微调。

所以OpenAI在2018年做的，严格来说不是发明了一个新概念。他们做的是**在前所未有的规模上应用这个概念**。用尽可能多的数据、最先进的架构（Transformer）和巨大的计算资源，来进行生成式预训练。这个规模的变化导致了质量的变化。

### 从GPT-1到GPT-3

GPT-1（2018年）有1.17亿个参数，在一个包含约800万个网页的数据集上训练。这个规模在今天看来可能很小，但在2018年是前所未有的。

一年后，2019年，OpenAI发表了GPT-2。标题是《语言模型是无监督的多任务学习者》（Language models are unsupervised multitask learners）。这篇论文的关键发现是，用GPT架构训练的大型语言模型，在许多下游任务上都表现出色，**甚至不需要任务特定的微调**！这叫做"零样本"学习——模型被直接应用到没见过的任务上。

GPT-2有15亿个参数，在一个包含40亿个网页的更大数据集上训练。这已经是GPT-1的大约10倍的参数和数据。

然后在2020年，OpenAI发表了GPT-3的论文《语言模型是少样本学习者》（Language models are few-shot learners）。GPT-3有1750亿个参数，训练数据包括Common Crawl、WebText2和其他来源，总共包含约45TB的文本。论文中的一个关键声明是：

> 对于所有任务，GPT-3在无任何梯度更新或微调的情况下应用。

这对许多人来说是震撼性的。一个单一的、经过预训练的模型，不经过任何任务特定的调整，就能在翻译、问答、摘要、推理等各种任务上表现得相当不错。这暗示了一个深刻的真理：**大型语言模型中存在着某种通用的"智能"，这种智能可以被激活来做许多不同的事情**。

从GPT-1的1.17亿参数到GPT-3的1750亿参数，增长了1500倍。这个规模的扩张伴随着能力的质的飞跃。这不仅仅是模型变大了，而是某种新的能力涌现了。

### 缩放律与涌现

在GPT-3论文和后来的许多研究中，一个模式变得清晰：**当你让神经网络变大、用更多数据训练时，性能通常会以可预测的方式改进**。这被称为"缩放律"（Scaling Laws）。

但更有趣的是，有时候改进不是平缓的。有时候，性能会突然跳跃。模型突然学会了它之前无法做的事情。这叫做**涌现**（Emergence）。这是大型语言模型中最神秘、最令人困惑的现象之一。

为什么会这样？如果一个较小的模型不能进行多位数加法，为什么一个稍大一点的模型就能？如果我们增加的只是参数数量和训练数据，为什么会出现完全新的能力？这仍然是一个开放的研究问题。

但无论其机制如何，涌现是不可否认的现象。当模型规模达到某个阈值时，新的能力似乎就从中浮现出来。这对AI安全和AI能力预测都有深刻的含义。

### 那一刻的意义

回过头来看，Transformer的出现和生成式预训练的应用，标志着深度学习在自然语言处理中的真正胜利。但更重要的是，它标志着一个新范式的开始。

在Transformer之前，NLP的主流思想是设计特殊的架构和目标函数来解决特定的问题。你想做机器翻译？设计一个编码器-解码器。你想做文本分类？设计一个适当的模型头。你想做问答？又是另一个特殊设计。

但Transformer改变了这一切。它说，一个通用的架构，用一个简单的目标函数（下一个词预测），在足够大的规模和足够多的数据上进行预训练，就能做许多不同的事情。这是一个范式转变。

从这一刻开始，NLP不再是关于巧妙地设计模型和任务的领域。它变成了一场关于**数据、计算和模型规模**的竞争。谁能获得最多的数据，谁能用最好的硬件进行最长的训练，谁就能构建最好的模型。

这场竞争激烈而持久。它驱动了GPU市场的增长。它改变了研究机构能够进行的工作类型。它集中了AI能力到拥有巨大资源的少数公司。

但最重要的是，它证明了一个古老的计算机科学直觉是正确的——当面对一个复杂问题时，有时候最好的解决方案不是更聪明的算法，而是**更大的规模、更多的数据和更多的计算**。这个直觉从来没有像在大语言模型中那样彻底地被验证过。

而这，正是我们所在时代的根本特征。