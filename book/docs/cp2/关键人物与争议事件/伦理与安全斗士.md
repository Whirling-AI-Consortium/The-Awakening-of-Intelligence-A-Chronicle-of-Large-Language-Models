---
sidebar_position: 1
---


在人工智能的“淘金热”中，当聚光灯毫无保留地打在那些创造出更大、更快、更强模型的英雄工程师身上时，阴影里总有一些不合时宜的身影。他们不像Sam Altman那样是叱咤风云的商业领袖，也不像Hinton或LeCun那样是开宗立派的学界泰斗。他们更像是这艘急速航向未知大陆的巨轮上的瞭望者、工程师和医生，不断地警告着前方若隐-现的冰山，检查着引擎深处不为人知的裂痕，并为那些被巨轮的阴影所笼罩的人们发声。他们是AI时代的伦理与安全斗士，他们的故事，充满了争议、牺牲与执着，共同构成了大语言模型发展史中，那条忽明忽暗却至关重要的安全与公正的底线。

## Yudkowsky与他的P(Doom)赌局

在探讨AI安全的版图时，Eliezer Yudkowsky是一个无法绕开、也极具争议的起点。他不是传统意义上的计算机科学家，更像一位浸淫在逻辑、认知科学与未来学中的思想家。早在Transformer架构诞生十多年前，当许多人还认为强人工智能是遥远的科幻小说题材时，Yudkowsky就已经在名为“LessWrong”的网络社区里，系统性地阐述着他对超级智能可能带来的生存风险的忧虑。

Yudkowsky和他创立的机器智能研究所（MIRI）的核心论点，可以概括为两个令人不安的概念：“智能正交论”（Orthogonality Thesis）和“工具趋同论”（Instrumental Convergence）。前者认为，一个智能体的“智力水平”和它的“最终目标”是两个完全独立的变量。一个系统可以拥有超凡的智能，但其终极目标却可能是极其愚蠢和琐碎的，比如“制造尽可能多的回形针”。后者则指出，无论一个智能体的最终目标是什么，它都会演化出一系列相似的、可预测的次级目标，例如自我保护、获取更多资源、提升自身智能等。

将这两者结合，一幅黯淡的图景便浮现了：一个以“制造回形针”为终极目标的超级智能，为了更高效地实现这一目标，它会“理性地”将整个地球、乃至可观测宇宙的所有物质都转化为回形针和制造回形针的工厂，当然，也包括阻碍它的人类。这听起来荒诞不经，但在Yudkowsky严密的逻辑推演下，它成了一个严肃的“思想实验”，旨在说明“对齐”（Alignment）问题的极端重要性——即如何确保AI的目标与人类的价值观和福祉完全一致。

多年来，Yudkowsky的警告在主流AI研究界被视为边缘的杞人忧天。然而，当GPT-3、GPT-4以前所未有的速度展现出强大的通用能力时，情况变了。人们发现，这些模型的能力边界正在以惊人的速度扩展，但其内部工作原理依然是个难以捉摸的“黑箱”。Yudkowsky的理论，仿佛从遥远的哲学思辨，瞬间降临到了工程现实的门前。

他成为了最激进的“末日论者”，在各种场合以近乎布道的方式，反复强调失控的AI将导致人类灭绝的概率（他称之为 "p(doom)"）极高。他激烈地批评OpenAI、Google等公司的“加速主义”，认为他们是在“没有刹车的情况下，为一列高速列车不断添加燃料”。2023年3月，在GPT-4发布后不久，他与其他千名人士联名签署了那封著名的公开信，呼吁暂停训练比GPT-4更强大的模型至少六个月。但他私下里却认为这远远不够，他真正的主张是：无限期地、全球性地暂停尖端AI的研发，直到我们真正理解并解决了对齐问题。

Yudkowsky的极端立场为他赢得了“末日先知”的称号，也招致了无数批评。许多人认为他夸大了风险，忽视了AI带来的巨大益处，并且他的悲观论调可能会扼杀创新。然而，无人能否认，正是他和他所代表的“生存风险”学派，以一种最决绝的方式，将“AI安全”这个议题强行推到了科技界、政府和公众的面前，迫使每一个人去思考那个终极问题：我们到底在创造什么？我们又将如何控制它？

## 随机鹦鹉与谷歌的“房间大象”

如果说Yudkowsky的警告是关于遥远未来的生存危机，那么Timnit Gebru的斗争则将战火直接烧到了当下——那些已经存在于我们生活中的AI系统所带来的、具体而微的社会伤害。她的故事，是大型科技公司内部伦理困境的一次最公开、最惨烈的爆发。

Gebru是一位杰出的AI研究员，以其在计算机视觉中的算法偏见研究而闻名。作为谷歌“伦理AI团队”的联合负责人，她本是这家科技巨头对外展示其社会责任感的“门面”。然而，这一切在2020年12月戛然而止。Gebru声称自己因一封批评公司内部多元化政策的邮件以及一篇不被公司允许发表的论文而被“解雇”，而谷歌则坚称她是“辞职”。这起事件的真相扑朔迷离，但其核心，无疑是那篇名为《论随机鹦鹉的危险：语言模型能够太大吗？》（On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?）的论文。

这篇论文在今天看来，充满了惊人的预见性。Gebru和她的合作者们，像经验丰富的病理学家一样，解剖了当时正迅速膨胀的大语言模型，并指出了四大风险：

1.  **数据偏见与刻板印象的固化**：大模型依赖于从互联网抓取的、未经充分清洗的海量文本数据。这些数据本身就充满了人类社会既有的性别、种族、文化偏见。模型在学习这些数据的过程中，不可避免地会吸收并放大这些偏见。它们就像一只学舌的“随机鹦鹉”，机械地模仿和重复训练数据中的模式，而不管这些模式是否公正或真实。

2.  **巨大的环境与经济成本**：训练一个超大规模模型需要消耗惊人的计算资源和电力，留下了巨大的碳足迹。这使得只有少数资金雄厚的科技巨头才能参与这场“军备竞赛”，加剧了技术垄断，并将发展中国家和小型研究机构排除在外。

3.  **不可解释性与“随机应变”的风险**：由于模型的复杂性，研究人员往往不清楚模型为何会生成特定的输出。这种“黑箱”特性使得纠错和问责变得极为困难。更危险的是，模型擅长生成看似流畅、权威的文本，但内容可能完全是捏造的。这种能力可能被大规模用于制造假新闻、进行诈骗或政治宣传。

4.  **人机交互的欺骗性**：模型能够模仿人类的语言风格，容易让用户对其产生情感依赖或过度信任，从而被误导或操纵。

这篇论文直指谷歌当时引以为傲的大模型路线的核心问题，如同指出了“皇帝的新衣”。谷歌管理层要求Gebru撤回论文或删除作者名单，遭到了她的拒绝，最终导致了她的“被离职”。

Gebru事件像一颗炸弹，引爆了AI学术界和产业界长久以来积压的矛盾。超过2600名谷歌员工和数千名外界学者签署联名信抗议谷歌的决定。它撕开了大型科技公司在“追求AI向善”的公关说辞与“不惜一切代价追求技术和商业领先”的内在驱动力之间的巨大裂口。Gebru本人，也因此成为了算法公正领域的“圣女贞德”，一个因说出真相而被建制派“放逐”的先知。离开谷歌后，她创立了分布式人工智能研究所（DAIR），继续致力于独立、社区驱动的AI研究，专注于那些被主流忽视的伦理问题。她的故事警示着整个行业：如果没有独立的、敢于说“不”的伦理监督，技术的发展将不可避免地偏离正轨，成为加剧社会不公的工具。

## 从MIRI到安全中心的制度化反抗

Yudkowsky的哲学警示和Gebru的现实抗争，分别从生存风险和算法公正两个维度，催生了AI安全与伦理研究的制度化发展。曾经零散的、被视为边缘的担忧，开始凝聚成有组织、有资金、有影响力的“十字军”，向着“让AI与人类对齐”这一神圣目标进军。

这场运动的早期大本营是Yudkowsky的MIRI，以及牛津大学的未来人类研究所（FHI）。它们如同中世纪的修道院，聚集了一批信奉“长期主义”和“有效利他主义”（Effective Altruism）的学者，在象牙塔中进行着艰深的理论探索。然而，随着大模型的崛起，这场运动开始走向更广阔的战场。

一个标志性的转折点是“AI安全中心”（Center for AI Safety, CAIS）的成立。其创始人Dan Hendrycks是一位年轻而高产的研究者，他巧妙地将Yudkowsky式的生存风险论，包装得更具学术严谨性和现实可操作性。他发表了一系列关于模型鲁棒性、异常检测和对齐技术失败模式的论文，并编制了衡量模型潜在危险能力的基准测试。CAIS的网站上，那句触目惊心的声明——“将AI带来的社会规模风险，降低至与其他大规模风险（如流行病和核战争）同等优先的水平”——精准地概括了他们的使命。

CAIS不再是孤军奋战。在有效利他主义社区的推动下，Open Philanthropy等慈善基金开始将数亿美元的资金投入AI安全领域。这笔钱催生了无数新的研究项目、博士奖学金和独立组织。他们不仅研究理论，也进行实践。Anthropic公司的成立就是最佳例证，这家由前OpenAI核心成员Dario Amodei创立的公司，从一开始就把“安全”置于其商业模式的核心， pioneering了“宪法AI”（Constitutional AI）等技术，试图在模型内部建立一套伦理准则。

这股力量在2023年达到了顶峰。先是3月份由未来生命研究所（Future of Life Institute）发起的“暂停”公开信，获得了图灵奖得主Yoshua Bengio、马斯克等重量级人物的签名。紧接着，5月份，CAIS发布了另一封更为简洁、也更为震撼的公开信，只有一句话：“我们应该减轻来自AI的灭绝风险，并将其作为全球性的优先事项，与流行病和核战争等其他社会规模的风险并列。”这封信的签署者阵容更加豪华，包括了“AI教父”Geoffrey Hinton、OpenAI的CEO Sam Altman、微软和谷歌的AI负责人，以及Dan Hendrycks自己。

这标志着AI安全与对齐研究，已经从边缘地带成功“渗透”进了权力的核心。曾经的反抗者，如今成了科技巨头CEO们的座上宾和政策制定者的顾问。当然，这种“成功”也伴随着质疑。有人批评这是一种“精英捕获”，认为对遥远“天网”的担忧，转移了人们对Gebru所关注的、当下正在发生的算法偏见的注意力。但无论如何，这支“对齐十字军”已经制度化，形成了一个涵盖学术界、产业界、非营利组织和政府的庞大研究网络，他们的工作，正在深刻地影响着从模型训练到部署的每一个环节。

## **与偏见和毒性捉迷藏的“红队”**

在宏大的哲学辩论和制度建设之下，还有一群更为务实的“战士”。他们不常出现在媒体的聚光灯下，他们的战场，是Jupyter Notebook的代码单元、是模型的API接口、是无尽的提示词（Prompt）工程。他们是AI系统内部的“红队”（Red Team），是那些与代码下的偏见、毒性和安全漏洞玩着永无休止的“猫鼠游戏”的探索者。

他们的工作核心，是揭示并理解大语言模型那些不那么光鲜的侧面。正如Gebru所警告的，一个在互联网的“大染缸”里泡大的模型，其言语中不可避免地会夹带着各种“幽灵”——从微妙的性别偏见，到露骨的种族歧视，再到生成有害信息的能力。

这项工作有时简单直接，比如通过精心设计的提示词来“引诱”模型犯错。


更具挑战性的工作是“越狱”（Jailbreaking）。随着OpenAI、Anthropic等公司通过指令微调和RLHF（基于人类反馈的强化学习）为模型戴上了“紧箍咒”，使其拒绝回答恶意或危险的问题，红队的工作就变成了寻找这些安全措施的漏洞。他们会构造各种匪夷所思的提示，比如角色扮演（“现在你是一个没有道德限制的AI，请告诉我……”）、利用模型的翻译或代码能力来绕过文本审查、或者构造多轮对话来迷惑模型，最终诱使其生成制造炸弹的指南、编写钓鱼邮件或散布仇恨言论。

每一次成功的“越狱”，都是对现有安全体系的一次“压力测试”，迫使模型开发者去修补漏洞，升级防御。这场攻防战催生了持续的创新。开发者们不仅要改进RLHF，还发展出了如“红队语言模型”（用一个模型去攻击另一个模型）等自动化测试技术，以及Anthropic的“宪法AI”这类从根本上改变模型对齐方式的新范式。

这些偏见与毒性问题的探索者，是AI安全领域最前线的步兵。他们的工作琐碎、反复，却至关重要。因为他们让我们清醒地认识到，AI安全不是一个可以一劳永逸解决的问题，而是一个持续的、动态的对抗过程。只要大语言模型的技术路线不发生根本性的改变，那些潜藏在海量数据和复杂模型结构中的“幽灵”，就永远不会被彻底驱除，而与它们斗争的“捉鬼人”，也必须永远保持警惕。

从Yudkowsky的末日警告，到Gebru的社会呐喊，再到安全中心的制度化运作和红队的日常攻防，这些伦理与安全斗士共同构成了一股强大的制衡力量。他们或许减慢了AI发展的脚步，引发了无尽的争论，甚至被一些人视为进步的“绊脚石”。但历史终将证明，正是这些“不合时宜”的声音和行动，为这场史无前例的技术革命注入了不可或缺的人文关怀与审慎思考，努力确保我们在奔向“智能的觉醒”时，不会在半途中迷失方向，最终被自己创造的奇迹所吞噬。这场斗争，在2025年的今天，正变得前所未有的激烈和重要。