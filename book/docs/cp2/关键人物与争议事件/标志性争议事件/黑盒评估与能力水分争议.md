---
sidebar_position: 2
---

当苹果公司在2025年初发布研究报告，宣称包括OpenAI的o3和DeepSeek的R1在内的顶级推理模型在面对复杂问题时会出现"完全准确性崩溃"时，更像是对整个AI评估体系的致命一击。这场争议的核心在于一个看似简单却极其尖锐的问题：当我们无法真正理解AI模型的内部运作机制时，我们如何才能相信它们所展现的惊人能力？

## 黑盒的双重含义

在技术层面上，黑盒指的是AI模型的不可解释性——即使是模型的创造者也无法完全理解其内部的决策过程。正如IBM在其技术报告中所描述的："这些先进的机器学习模型，包括像OpenAI的ChatGPT和Meta的Llama这样的大语言模型，都是黑盒AI。这些人工智能模型通过复杂的深度学习过程在海量数据集上进行训练，即使是它们自己的创造者也不完全理解它们是如何工作的。"

但在更深层的意义上，黑盒还指向了评估过程本身的不透明性。当AI公司发布新模型时，他们通常只展示结果——令人印象深刻的基准测试分数，却很少透露评估的具体过程、使用的确切数据集，或者模型在获得这些分数时的具体条件。这种双重黑盒创造了一个危险的信息不对称，让外界难以验证所宣称的能力是否真实。

2025年的一项跨学科研究彻底揭露了当前AI基准测试体系的系统性缺陷。这项研究发现，许多广泛使用的安全基准测试（包括ETHICS、TruthfulQA、GPQA、QuALITY、MT-Bench、LMSYS Chatbot ARENA等）与一般和上游模型能力高度相关，引发了对"安全清洗"的担忧——应用这些基准可能意味着"能力改进被误表为安全进步"。

更为严重的是，研究人员发现许多流行的AI测试要么没有衡量它们声称要衡量的内容，要么只涵盖有限的难度级别。例如，公务员考试基准测试本意是测试逻辑推理，但它也需要其他能力，如专业知识和元认知。这种构造效度问题意味着，即使模型在基准测试中表现出色，也可能并不具备我们期望的真实能力。

在基准测试的诸多问题中，数据污染可能是最具毁灭性的。这种现象发生在语言模型在训练过程中无意中接触到评估基准信息时，导致评估阶段出现不准确或不可靠的性能表现。根据一项全面的调查，数据污染可以分为四个层次：

语义层次的污染涉及基准测试的相同或衍生内容的暴露，通常内容涉及相同主题或来自与基准测试相同的来源。数据层次的污染指的是基准数据的暴露但不包括标签，如测试集的数据内容和没有相关标签的数据序列。信息层次的污染是指暴露于基准相关信息，导致模型在评估过程中出现倾向和偏见。最严重的标签层次污染则是基准数据的完全暴露，包括标签——当标签在训练期间可用时，模型可能直接记住它们，导致过拟合和泛化能力受损。

令人震惊的发现来自针对商业大语言模型的"测试集槽位猜测"（TS-Guessing）研究。研究人员发现，在MMLU基准测试中，ChatGPT和GPT-4在猜测基准测试数据中缺失选项的精确匹配率分别达到了52%和57%。这一发现强烈暗示这些模型在训练过程中已经见过测试数据，严重质疑了其评估结果的可信度。

## FrontierMath争议：学术诚信的试金石

2025年初爆发的FrontierMath争议为这场黑盒评估争论提供了一个具体而尖锐的案例。OpenAI因其参与由Epoch AI开发、但由OpenAI资助的FrontierMath基准测试而受到猛烈抨击。争议的核心在于OpenAI的特权访问权：尽管据称有口头协议限制数据使用，但OpenAI不仅资助了项目，还可以访问基准数据，并报告其O3模型达到了25%的成功率。

这种情况引发了"软作弊"的指控，因为只有口头协议限制OpenAI使用数据进行训练。社区推测，尽管口头协议应该防止OpenAI使用数据集进行训练，但对是否无意中发生数据污染存在相当大的怀疑。OpenAI可能策略性地筛选了训练数据、生成了反映基准问题类型的合成数据集，或使用测试数据来指导模型验证和选择过程。

这一争议突出了AI基准测试中透明度和完整性的根本问题。在没有书面协议或独立监督的情况下，利益相关者担心资金来源和访问特权可能导致潜在的利益冲突或评估实践中的偏见。

苹果公司的最新研究提供了对AI模型真实能力的清醒审视。研究人员让包括OpenAI的o1和o3模型、DeepSeek R1、Anthropic的Claude 3.7 Sonnet、Google的Gemini在内的通用和推理机器人解决四个经典谜题：过河、跳棋、堆块和汉诺塔。通过向这些谜题添加更多棋子，研究人员能够调整谜题的复杂性，从低到中到高。

结果令人震惊：当问题变得过于复杂时，这些被誉为最先进的推理模型经历了"完全准确性崩溃"。这一发现挑战了关于AI推理能力的基本假设，揭示了即使是最复杂的模型在面对真正挑战性任务时也存在根本性限制。

正如研究作者所指出的："我们认为缺乏系统分析调查这些问题是由于当前评估范式的局限性。现有评估主要关注既定的数学和编码基准，虽然有价值，但往往存在数据污染问题，并且不允许在不同设置和复杂性下进行受控实验条件。此外，这些评估没有提供对推理轨迹结构和质量的洞察。"

为了应对传统基准测试的问题，AI行业越来越依赖像Chatbot Arena这样的众包基准平台来探索其最新模型的优势和劣势。然而，专家们指出这种方法存在严重的伦理和学术问题。

华盛顿大学语言学教授Emily Bender指出："为了有效，基准需要衡量特定的东西，并且需要具有构造效度——即必须有证据表明构造是真实的。"Bender特别对Chatbot Arena提出了质疑，该平台任务志愿者提示两个匿名模型并选择他们喜欢的响应。

更令人担忧的是，众包评估容易受到操纵。研究显示，模型可以通过关注风格而非实质内容来"游戏化"系统。想象一下，面对一个困难的数学问题，给出两个答案：一个是混乱、详细的解释，另一个是简洁、正确的答案。大多数人会选择详细的那个，即使它是错误的，仅仅因为它感觉更完整。

最近发生的Meta Llama 4 Maverick模型争议完美说明了这一问题。Meta调优了Maverick的一个版本以在Chatbot Arena上表现良好，但却没有发布该模型，而是发布了一个表现较差的版本。这种行为引发了对基准测试被AI实验室"共同选择"以"促进夸大声明"的担忧。

面对黑盒问题，解释性AI（XAI）被提出作为解决方案。然而，最新研究揭示了这种方法的根本缺陷。研究人员发现，即使是被广泛使用的解释工具，如部分依赖图（Partial Dependence Plots），也可以被操纵以隐藏模型中的歧视性做法。

使用真实世界数据集，包括汽车保险索赔和刑事犯罪者数据，研究人员展示了他们的框架如何有效愚弄部分依赖图。这一发现对依赖AI解释进行决策或监管合规的组织具有重大意义。操纵解释工具的能力引发了关于它们在检测不公平或歧视性做法方面可靠性的质疑。

研究人员警告："我们建议不要使用部分依赖图作为验证敏感属性公平性或非歧视性的手段。这在对抗性场景中尤为重要，其中提供和利用解释方法的利益相关者具有相对的利益和激励。"

Tech Policy Press的研究员Eryk Salvaggio在其分析中指出了一个关键问题："那些在AI行业之外的人不知道他们遵循什么规则。"这种信息不对称不是偶然的，而是系统性的。AI公司通过控制评估过程、选择性发布结果，以及利用技术复杂性作为不透明的借口，维持着对叙事的控制。

正如Salvaggio所观察到的："AI作为一个行业，如果不能控制模型就无法增长，而如果不能控制数据就无法控制模型。"这种控制延伸到了评估领域，创造了一个自我强化的循环，其中那些最有能力评估AI系统的人——开发它们的公司——也是最有动机夸大其能力的人。

面对这些系统性问题，监管机构开始采取行动。美国版权局在2025年5月发布的报告中明确指出，当前的AI评估方法存在根本缺陷。报告强调，版权法的专有权利基于这样一个事实：人类对他们所经历的作品只保留"不完美的印象，通过他们自己独特的观点过滤"，而AI允许"创建完美的副本，并能够几乎瞬间分析作品"。

这一观察揭示了AI评估中的一个根本问题：传统的测试方法是为人类设计的，他们的记忆是不完美的，学习是渐进的。但AI系统具有完美的记忆和瞬时处理能力，这使得传统的评估范式变得不适用甚至误导。

## 新兴的解决方案与挑战

为了应对这些挑战，研究社区正在探索新的评估方法。微软研究院开发的ADeLe（标注需求水平）框架代表了一个有前途的方向。该框架通过评估任务需要的知识和认知能力类型，并将其与模型的能力进行对比，超越了测量整体准确性的做法。

ADeLe使用18种认知和基于知识的能力类型的测量量表来评估任务对AI模型的要求程度。这种难度评级基于详细的评价标准，最初为人类任务开发，并显示在AI模型应用时工作可靠。通过将任务需求与模型能力进行比较，ADeLe生成了一个能力配置文件，不仅预测性能，还解释为什么模型可能成功或失败。

然而，即使是这样的创新方法也面临挑战。如研究人员所承认的："虽然没有真正的一刀切方法来检测污染，但使用最长污染子串而不是所有匹配的联合在各个方面效果更好，充分检测了其他度量都没有做到的污染情况。"

AI评估可能需要根本性的重新思考。动态基准测试、实时评估和多模态评估正在成为新的趋势。但更重要的是，我们需要从关注单一指标转向全面评估模型在实际应用中的表现。正如一位研究人员所指出的："从业者应该避免过度使用黑盒模型，如果可解释模型能够达到相同的模型性能水平。"这一建议挑战了更复杂模型总是更好的常见假设，并鼓励企业批判性评估黑盒模型的好处是否真正超过了在可解释性和潜在公平性问题方面的成本。

真正的AI进步不应该通过操纵评估来证明，而应该通过在真实世界应用中的可靠性能来验证。只有当我们能够打开黑盒，让阳光照进AI评估的每一个角落时，我们才能建立起公众对这项变革性技术的真正信任。