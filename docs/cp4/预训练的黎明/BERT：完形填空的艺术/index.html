<!doctype html>
<html lang="zh-Hans" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-cp4/预训练的黎明/BERT：完形填空的艺术" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">BERT：完形填空的艺术 | 智能的觉醒</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/BERT：完形填空的艺术"><meta data-rh="true" property="og:locale" content="zh_Hans"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="BERT：完形填空的艺术 | 智能的觉醒"><meta data-rh="true" name="description" content="Google的王炸时刻"><meta data-rh="true" property="og:description" content="Google的王炸时刻"><link data-rh="true" rel="icon" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/BERT：完形填空的艺术"><link data-rh="true" rel="alternate" href="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/BERT：完形填空的艺术" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/BERT：完形填空的艺术" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/assets/css/styles.cd147f59.css">
<script src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/assets/js/runtime~main.23d89510.js" defer="defer"></script>
<script src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/assets/js/main.333741a6.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/logo.svg"><div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/"><div class="navbar__logo"><img src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/logo.svg" alt="My Book Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/logo.svg" alt="My Book Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">主页</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/本书导览">Read Book</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="文档侧边栏" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/本书导览">本书导览</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/intro">前言</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第一章智能的拂晓">第一章：智能的拂晓</a><button aria-label="展开侧边栏分类 &#x27;第一章：智能的拂晓&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第二章ai江湖-人物与格局">第二章：AI江湖-人物与格局</a><button aria-label="展开侧边栏分类 &#x27;第二章：AI江湖-人物与格局&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第四章从llm到agi">第四章：从llm到agi</a><button aria-label="折叠侧边栏分类 &#x27;第四章：从llm到agi&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/前言">从Transformer到LLM的技术图景</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/模板">模板</a><button aria-label="展开侧边栏分类 &#x27;模板&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/预训练的黎明">预训练的黎明</a><button aria-label="折叠侧边栏分类 &#x27;预训练的黎明&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/ELMo：上下文的双向觉醒">ELMo：上下文的双向觉醒</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/GPT：单向的野心">GPT：单向的野心</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/BERT：完形填空的艺术">BERT：完形填空的艺术</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/规模的魔法">规模的魔法</a><button aria-label="展开侧边栏分类 &#x27;规模的魔法&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/涌现的惊喜">涌现的惊喜</a><button aria-label="展开侧边栏分类 &#x27;涌现的惊喜&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/对齐的困境">对齐的困境</a><button aria-label="展开侧边栏分类 &#x27;对齐的困境&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/推理的觉醒">推理的觉醒</a><button aria-label="展开侧边栏分类 &#x27;推理的觉醒&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/效率的追求">效率的追求</a><button aria-label="展开侧边栏分类 &#x27;效率的追求&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/多模态的融合">多模态的融合</a><button aria-label="展开侧边栏分类 &#x27;多模态的融合&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/长文本的征途">长文本的征途</a><button aria-label="展开侧边栏分类 &#x27;长文本的征途&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第三章大语言模型技术发展历史">第三章：大语言模型技术发展历史</a><button aria-label="展开侧边栏分类 &#x27;第三章：大语言模型技术发展历史&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/结语：智能的新纪元">结语：智能的新纪元</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="页面路径"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主页面" class="breadcrumbs__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第四章从llm到agi"><span itemprop="name">第四章：从llm到agi</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/预训练的黎明"><span itemprop="name">预训练的黎明</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">BERT：完形填空的艺术</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本页总览</button></div><div class="theme-doc-markdown markdown"><header><h1>BERT：完形填空的艺术</h1></header><h2 class="anchor anchorWithStickyNavbar_LWe7" id="google的王炸时刻">Google的王炸时刻<a href="#google的王炸时刻" class="hash-link" aria-label="Google的王炸时刻的直接链接" title="Google的王炸时刻的直接链接">​</a></h2>
<p>2018年10月，当Jacob Devlin和他在Google Brain的团队在arxiv上发表《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》时，没有人意识到这会成为NLP历史上最具影响力的论文之一。不是因为技术有多新颖——事实上，BERT的每一个组件都已经存在。而是因为它找到了一个如此简单、如此有效、如此优雅的方式，来利用这些已有的技术。</p>
<p>这篇论文发表的时机非常微妙。ELMo已经证明了预训练的价值。GPT已经展示了Transformer可以用于语言建模。但业界对最优的预训练目标仍然没有共识。双向还是单向？完形填空还是自回归？在这个分水岭的时刻，Google交出了他们的答卷。</p>
<p>这个答卷的核心思想简单到几乎让人觉得有些荒谬：随机遮蔽15%的单词，然后让模型猜测它们是什么。就这样。没有复杂的目标设计，没有精妙的损失函数，只有一个小学生都能理解的游戏规则。</p>
<p>但是，有时候，最强大的想法正是最简单的那些。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="masked-language-model遮蔽的天才设计">Masked Language Model：遮蔽的天才设计<a href="#masked-language-model遮蔽的天才设计" class="hash-link" aria-label="Masked Language Model：遮蔽的天才设计的直接链接" title="Masked Language Model：遮蔽的天才设计的直接链接">​</a></h2>
<p>MLM（Masked Language Model）的核心思想源自一个古老的教学技巧。当你给一个学生一篇有部分词汇缺失的文章，让他根据上下文填空时，这不仅是一个语言理解的测试，更是一个极其高效的学习过程。学生必须理解周围词汇的语义，才能正确地填空。这种理解必须是双向的——前文和后文的信息都很关键。</p>
<p>BERT的设计者们意识到，这个古老的教学技巧可以完美地转化为一个深度学习的预训练目标。而且，由于BERT有意地遮蔽了部分输入，它就不会&quot;作弊&quot;地通过简单的顺序预测来完成任务。它必须真正理解语言。</p>
<p>但实现MLM并不是简单地把词删除。Devlin等人设计了一个更精妙的方案：</p>
<p><strong>三步遮蔽策略</strong></p>
<p>当选定要遮蔽的15%的词时，对于这些词：</p>
<ul>
<li>80%的时候，用特殊的[MASK]标记替换（例如&quot;The cat sat on the [MASK]&quot;）</li>
<li>10%的时候，用一个随机的词替换（例如&quot;The cat sat on the dog&quot;，虽然这在语义上毫无意义）</li>
<li>10%的时候，保持原词不变（例如&quot;The cat sat on the mat&quot;）</li>
</ul>
<p>这个看起来奇怪的设计有其深刻的理由。如果总是用[MASK]替换，模型在实际应用时会看到原始文本中没有[MASK]标记，可能导致分布转移。通过在20%的时候保持原词或随机替换，模型被迫学习在每个位置都预测原始词，而不是依赖[MASK]标记的出现来&quot;知道&quot;这里应该做预测。这强制了模型学习更深层的表示。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">masked_language_model_objective</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">tokens</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> vocab_size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> mask_prob</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.15</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    实现MLM的目标函数</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    tokens: 原始序列 [batch_size, seq_length]</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    batch_size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> seq_length </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> tokens</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">shape</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 决定哪些位置要进行MLM</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mlm_positions </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mlm_labels </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    modified_tokens </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> tokens</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">clone</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> i </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">batch_size</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> j </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">seq_length</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> tokens</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">i</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> j</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> PAD_ID</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># 跳过padding</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token keyword" style="color:#00009f">continue</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token comment" style="color:#999988;font-style:italic"># 以mask_prob的概率选择这个位置</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> random</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">random</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain"> mask_prob</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                mlm_positions</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">append</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">i</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> j</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                mlm_labels</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">append</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">tokens</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">i</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> j</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token comment" style="color:#999988;font-style:italic"># 三步策略</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                rand </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> random</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">random</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> rand </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.8</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    </span><span class="token comment" style="color:#999988;font-style:italic"># 80%: 用[MASK]替换</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    modified_tokens</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">i</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> j</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> MASK_ID</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token keyword" style="color:#00009f">elif</span><span class="token plain"> rand </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.9</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    </span><span class="token comment" style="color:#999988;font-style:italic"># 10%: 用随机词替换</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    modified_tokens</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">i</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> j</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> random</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">randint</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> vocab_size </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token comment" style="color:#999988;font-style:italic"># 10%: 保持原词不变</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Forward pass with modified tokens</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    logits </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">modified_tokens</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 计算损失：只在MLM位置计算</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mlm_loss </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">batch_idx</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> seq_idx</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> true_label </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">zip</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">mlm_positions</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> mlm_labels</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        pred_logits </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> logits</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">batch_idx</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> seq_idx</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">:</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        mlm_loss </span><span class="token operator" style="color:#393A34">+=</span><span class="token plain"> cross_entropy</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">pred_logits</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> true_label</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> mlm_loss </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> </span><span class="token builtin">len</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">mlm_positions</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>这个三步策略虽然复杂，但也正是为什么MLM如此有效。它防止了模型过度适应[MASK]标记这个虚拟的符号。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="next-sentence-prediction被遗忘的部分">Next Sentence Prediction：被遗忘的部分<a href="#next-sentence-prediction被遗忘的部分" class="hash-link" aria-label="Next Sentence Prediction：被遗忘的部分的直接链接" title="Next Sentence Prediction：被遗忘的部分的直接链接">​</a></h2>
<p>除了MLM，BERT的预训练还包括另一个目标：NSP（Next Sentence Prediction）。给定两个句子A和B，模型需要预测B是否是在文本中紧跟在A后面的真实句子，还是从语料库中随机选择的句子。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">next_sentence_prediction_objective</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">sentences_pair</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    NSP目标函数</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    sentences_pair: [(sent_A, sent_B, is_next), ...]</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    is_next: 布尔值，B是否真的跟在A后面</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    losses </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> sent_a</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> sent_b</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> is_next </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> sentences_pair</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 构造输入：[CLS] sent_a [SEP] sent_b [SEP]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        input_tokens </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">CLS</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> sent_a </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">SEP</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> sent_b </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">SEP</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Forward pass</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        cls_output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">input_tokens</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># 取[CLS]对应的输出</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># NSP分类器：二元分类</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        nsp_logits </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nsp_classifier</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">cls_output</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># 输出维度为2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 计算损失</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        nsp_loss </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> cross_entropy</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">nsp_logits</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token builtin">int</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">is_next</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        losses</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">append</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">nsp_loss</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> mean</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">losses</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>NSP的目标是为模型引入对&quot;句子关系&quot;的理解。这在一些任务中是有用的，比如句子配对任务。但后来的研究发现，NSP的贡献其实没有MLM那么大，甚至在某些情况下可能是多余的。后续的模型（如RoBERTa）会移除NSP，而只保留MLM。但在BERT的原始设计中，这两个目标是并行的。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="双向transformer-encoder的威力">双向Transformer Encoder的威力<a href="#双向transformer-encoder的威力" class="hash-link" aria-label="双向Transformer Encoder的威力的直接链接" title="双向Transformer Encoder的威力的直接链接">​</a></h2>
<p>BERT使用的模型架构是Transformer的Encoder部分，没有因果掩蔽。这意味着，在计算自注意力时，每个位置都能看到序列中的所有其他位置。</p>
<p>这与GPT的Decoder-only架构形成了鲜明对比。GPT因为要生成文本，所以需要因果掩蔽来防止模型看到未来的词。但BERT在预训练阶段不需要生成，只需要理解。所以它可以充分利用双向的信息流。</p>
<p>这个架构选择有两层意义。首先，从计算效率的角度，双向的信息流比单向更稠密。每个词都能与整个序列的所有词交互，而不仅仅是前面的词。这通常导致更丰富的表示学习。其次，从任务匹配的角度，大多数NLP任务（分类、配对、标注）都是&quot;理解&quot;型的，不需要生成。所以双向模型直接对应了这些任务的需求。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token class-name">BertEncoder</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Module</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> d_model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_layers</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_heads</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> d_ff</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token builtin">super</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">embedding </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Embedding</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">vocab_size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> d_model</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">pos_embedding </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> PositionalEncoding</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d_model</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">token_type_embedding </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Embedding</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> d_model</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># 句子A或B</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Encoder堆栈（无因果掩蔽）</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">encoder_layers </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ModuleList</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            TransformerEncoderLayer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d_model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_heads</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> d_ff</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> _ </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_layers</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">forward</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> input_ids</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> token_type_ids</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        input_ids: [batch_size, seq_length]</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        token_type_ids: [batch_size, seq_length] (0 for sent_A, 1 for sent_B)</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 嵌入层</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">embedding</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">input_ids</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">+=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">pos_embedding</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">input_ids</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">+=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">token_type_embedding</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">token_type_ids</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 创建attention mask（无因果约束）</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        attention_mask </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> create_attention_mask</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">input_ids</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># 仅mask padding</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 通过encoder层</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> encoder_layer </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">encoder_layers</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> encoder_layer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> attention_mask</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">attention_mask</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token comment" style="color:#999988;font-style:italic"># 注意：没有因果掩蔽，每个位置可以看到所有位置</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> x</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token class-name">TransformerEncoderLayer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Module</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> d_model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_heads</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> d_ff</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token builtin">super</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">self_attention </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> MultiHeadAttention</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d_model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_heads</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ffn </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> FeedForwardNetwork</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d_model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> d_ff</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">norm1 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">LayerNorm</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d_model</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">norm2 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">LayerNorm</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d_model</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">forward</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> attention_mask</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 自注意力（无因果掩蔽，完全双向）</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        attn_output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">self_attention</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> mask</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">attention_mask</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">norm1</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> attn_output</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 前馈网络</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ffn_output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ffn</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">norm2</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> ffn_output</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> x</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>值得注意的是，BERT还在嵌入中加入了token type embedding。这用来区分输入中的两个句子（如果有的话）。对于单句输入的任务，所有token的type_id都是0。对于句子对的任务，前半部分是0，后半部分是1。这个设计虽然简单，但在处理多句输入时非常有用。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="横扫11项任务的王者">横扫11项任务的王者<a href="#横扫11项任务的王者" class="hash-link" aria-label="横扫11项任务的王者的直接链接" title="横扫11项任务的王者的直接链接">​</a></h2>
<p>当BERT发表时，Google附加了一系列惊人的实验结果。在GLUE基准（General Language Understanding Evaluation）上，BERT在9个任务中都达到了SOTA。在SQuAD v1.1和v2.0问答基准上，BERT也超越了之前的最强结果。总计，论文声称在11项不同的NLP任务上刷新了记录。</p>
<p>这不仅仅是一个数字上的胜利。这代表了一个范式的转变。在BERT之前，NLP的做法通常是：对于每个特定任务，设计一个特定的架构，预训练一个特定的模型（如果需要的话），然后微调。结果是一个高度碎片化的生态——需要一个不同的系统来处理分类、一个不同的系统来处理序列标注、另一个不同的系统来处理问答。</p>
<p>BERT改变了这一切。一个统一的预训练模型，配上最小化的任务特定调整，就能在所有这些任务上取得最佳结果。这不仅简化了工程，也有深刻的科学含义：也许，存在一个通用的、与任务无关的&quot;语言理解&quot;能力，而BERT学到了它。</p>
<p><strong>BERT的微调策略</strong></p>
<p>对于不同的下游任务，BERT的微调方式是一致的：</p>
<ol>
<li>
<p><strong>分类任务</strong>（如情感分析）：取[CLS]标记（序列开头的特殊标记）对应的输出，通过一个线性分类器预测类别。</p>
</li>
<li>
<p><strong>标注任务</strong>（如NER）：取每个token对应的输出，通过一个线性层和softmax预测该token的标签。</p>
</li>
<li>
<p><strong>配对任务</strong>（如句子相似性）：输入格式为[CLS] sent_a [SEP] sent_b [SEP]，取[CLS]的输出进行二分类或回归。</p>
</li>
<li>
<p><strong>问答任务</strong>：输入格式为[CLS] question [SEP] passage [SEP]，预测在passage中的start和end位置。</p>
</li>
</ol>
<p>这种统一性是Transformer架构相对于之前RNN方法的一个主要优势。由于Transformer是完全非递归的，它可以灵活地处理各种长度和结构的输入。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="论文的科学洞察">论文的科学洞察<a href="#论文的科学洞察" class="hash-link" aria-label="论文的科学洞察的直接链接" title="论文的科学洞察的直接链接">​</a></h2>
<p>除了实验结果，BERT论文还进行了深入的分析实验。其中最有启发性的是对不同层的表示的分析。</p>
<p>论文的作者们进行了一个关键实验：他们取了BERT在不同层的隐状态，使用这些层的表示来解决具体的任务，而不使用最后一层。结果表明：</p>
<ul>
<li>低层（接近输入）的表示更多地捕捉浅层的句法信息。</li>
<li>中层的表示开始包含任务特定的信息。</li>
<li>高层（接近输出）的表示最有用，但不同任务的最优层数不同。</li>
</ul>
<p>这个发现意味着，BERT学到的是一个分层的表示——从具体的语法特征到抽象的语义特征，再到任务相关的信息。这与人类语言处理的多层次本质相吻合。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">analyze_layer_contributions</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> task_tokens</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> task_labels</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_layers</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    分析不同层对任务的贡献</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 获取所有层的输出</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    all_layer_outputs </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> layer_idx </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_layers</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        layer_output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_layer_output</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">task_tokens</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> layer_idx</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        all_layer_outputs</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">append</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">layer_output</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 对每一层单独进行微调和评估</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    layer_performance </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> layer_idx</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> layer_output </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">enumerate</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">all_layer_outputs</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 在该层之上加一个简单分类器</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        classifier </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> SimpleLinearClassifier</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">layer_output</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">shape</span><span class="token punctuation" style="color:#393A34">[</span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 微调并评估</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        acc </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> train_and_eval</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">classifier</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> layer_output</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> task_labels</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        layer_performance</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">append</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">acc</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> layer_performance</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 结果通常显示：</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># - 第0层（输入）：较低的性能</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># - 中间层：逐步提升</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># - 最后一层：最高性能</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># - 特殊情况：某些语法任务在中层表现更好</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="完形填空的哲学">完形填空的哲学<a href="#完形填空的哲学" class="hash-link" aria-label="完形填空的哲学的直接链接" title="完形填空的哲学的直接链接">​</a></h2>
<p>BERT的核心设计选择——完形填空——背后有一个深刻的哲学。这个设计反映了对&quot;理解&quot;的一种特定的认识。</p>
<p>完形填空测试的是什么？它测试的是你能否根据上下文推断一个缺失的单词。这需要：</p>
<ol>
<li><strong>语法理解</strong>：你需要知道这个位置应该是什么词性、什么时态。</li>
<li><strong>语义理解</strong>：你需要理解句子的含义，推断出逻辑一致的词。</li>
<li><strong>常识推理</strong>：在某些情况下，正确的填空需要对世界的理解。</li>
</ol>
<p>而且，完形填空是对称的、双向的。无论你看哪个方向的上下文，你都在做同样的推断任务。这与一个&quot;真正的理解&quot;应该具有的性质相符——理解不应该因为信息来自前还是后而改变。</p>
<p>相比之下，GPT的自回归语言建模只要求单向的推断——从前文推断下一个词。这对于生成任务是最优的，但对于&quot;理解&quot;可能不够全面。</p>
<p>BERT的设计者们似乎在说：如果你想要一个真正&quot;理解&quot;语言的模型，就让它做一个理解型的任务——完形填空。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="对预训练范式的深远影响">对预训练范式的深远影响<a href="#对预训练范式的深远影响" class="hash-link" aria-label="对预训练范式的深远影响的直接链接" title="对预训练范式的深远影响的直接链接">​</a></h2>
<p>BERT的发表是2018年NLP的一个分水岭。它不仅仅是一个更好的模型或一套新的技术。它定义了一个新的范式：预训练-微调。</p>
<p>在BERT之前，虽然ELMo已经展示了预训练的价值，但预训练仍然被看作是可选的、是&quot;调优&quot;的一部分。大多数研究者仍然习惯于从头开始训练任务特定的模型。</p>
<p>BERT改变了这一切。BERT表明，预训练不仅有用，而且是必须的。如果你不使用预训练模型，你的结果会明显更差。从BERT发表之后，预训练-微调范式成为了NLP的标准。没有预训练，你在2018年之后就被看作是在做&quot;旧式的&quot;NLP。</p>
<p>这个范式转变的影响是深远的。它改变了整个行业的工作流程。不再需要为每个任务单独训练模型。不再需要对每个任务做细致的架构设计。只需要：下载一个预训练模型，在你的数据上微调，完成。</p>
<p>这个转变也加速了&quot;大模型&quot;时代的到来。既然预训练是关键，那么在预训练上投入更多资源就变得有理。这直接推动了模型规模的增长——从BERT的3.4B参数（base版本）到后续模型的更大规模。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="开放的问题与批评">开放的问题与批评<a href="#开放的问题与批评" class="hash-link" aria-label="开放的问题与批评的直接链接" title="开放的问题与批评的直接链接">​</a></h2>
<p>尽管BERT取得了巨大的成功，但它也留下了一些问题。首先，MLM和NSP这两个目标中，哪一个更重要？后续研究（如RoBERTa）表明，NSP可能是不必要的，甚至是有害的。这提出了一个问题：最优的预训练目标应该包含什么？</p>
<p>其次，15%的遮蔽比例是最优的吗？为什么不是20%或10%？BERT论文中虽然进行了一些消融实验，但对这个超参数的选择的理论理由并不充分。</p>
<p>第三，完形填空真的是理解的最佳模型吗？一些批评者指出，完形填空与下游任务（如分类）之间仍然有显著的分布差异。一个在完形填空上表现完美的模型，可能在分类上的性能依然有限。</p>
<p>最后，也许最深刻的问题是：BERT学到的&quot;理解&quot;的极限是什么？当模型扩展到更大规模时，这种理解会如何发展？答案会在后续的GPT-3、DALL-E等大规模模型上逐渐揭露。</p>
<p>但在2018年的那个时刻，当《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》这篇论文发表时，它定义了一个新的纪元。完形填空这个古老的教学技巧，通过深度学习的方式，成为了理解语言最有效的方法之一。这就是BERT的艺术。</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/edit/main/docs/cp4/预训练的黎明/BERT：完形填空的艺术.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件选项卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/GPT：单向的野心"><div class="pagination-nav__sublabel">上一页</div><div class="pagination-nav__label">GPT：单向的野心</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/规模的魔法"><div class="pagination-nav__sublabel">下一页</div><div class="pagination-nav__label">规模的魔法</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#google的王炸时刻" class="table-of-contents__link toc-highlight">Google的王炸时刻</a></li><li><a href="#masked-language-model遮蔽的天才设计" class="table-of-contents__link toc-highlight">Masked Language Model：遮蔽的天才设计</a></li><li><a href="#next-sentence-prediction被遗忘的部分" class="table-of-contents__link toc-highlight">Next Sentence Prediction：被遗忘的部分</a></li><li><a href="#双向transformer-encoder的威力" class="table-of-contents__link toc-highlight">双向Transformer Encoder的威力</a></li><li><a href="#横扫11项任务的王者" class="table-of-contents__link toc-highlight">横扫11项任务的王者</a></li><li><a href="#论文的科学洞察" class="table-of-contents__link toc-highlight">论文的科学洞察</a></li><li><a href="#完形填空的哲学" class="table-of-contents__link toc-highlight">完形填空的哲学</a></li><li><a href="#对预训练范式的深远影响" class="table-of-contents__link toc-highlight">对预训练范式的深远影响</a></li><li><a href="#开放的问题与批评" class="table-of-contents__link toc-highlight">开放的问题与批评</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Read</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/intro">Introduction</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub Discussions<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models" target="_blank" rel="noopener noreferrer" class="footer__link-item" remarkplugins="" rehypeplugins="">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Whirling AI Consortium. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>