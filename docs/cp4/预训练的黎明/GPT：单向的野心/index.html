<!doctype html>
<html lang="zh-Hans" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-cp4/预训练的黎明/GPT：单向的野心" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">GPT：单向的野心 | 智能的觉醒</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/GPT：单向的野心"><meta data-rh="true" property="og:locale" content="zh_Hans"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="GPT：单向的野心 | 智能的觉醒"><meta data-rh="true" name="description" content="相反的方向"><meta data-rh="true" property="og:description" content="相反的方向"><link data-rh="true" rel="icon" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/GPT：单向的野心"><link data-rh="true" rel="alternate" href="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/GPT：单向的野心" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/GPT：单向的野心" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/assets/css/styles.cd147f59.css">
<script src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/assets/js/runtime~main.23d89510.js" defer="defer"></script>
<script src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/assets/js/main.333741a6.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/logo.svg"><div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/"><div class="navbar__logo"><img src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/logo.svg" alt="My Book Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/logo.svg" alt="My Book Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">主页</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/本书导览">Read Book</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="文档侧边栏" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/本书导览">本书导览</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/intro">前言</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第一章智能的拂晓">第一章：智能的拂晓</a><button aria-label="展开侧边栏分类 &#x27;第一章：智能的拂晓&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第二章ai江湖-人物与格局">第二章：AI江湖-人物与格局</a><button aria-label="展开侧边栏分类 &#x27;第二章：AI江湖-人物与格局&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第四章从llm到agi">第四章：从llm到agi</a><button aria-label="折叠侧边栏分类 &#x27;第四章：从llm到agi&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/前言">从Transformer到LLM的技术图景</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/模板">模板</a><button aria-label="展开侧边栏分类 &#x27;模板&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/预训练的黎明">预训练的黎明</a><button aria-label="折叠侧边栏分类 &#x27;预训练的黎明&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/ELMo：上下文的双向觉醒">ELMo：上下文的双向觉醒</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/GPT：单向的野心">GPT：单向的野心</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/BERT：完形填空的艺术">BERT：完形填空的艺术</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/规模的魔法">规模的魔法</a><button aria-label="展开侧边栏分类 &#x27;规模的魔法&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/涌现的惊喜">涌现的惊喜</a><button aria-label="展开侧边栏分类 &#x27;涌现的惊喜&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/对齐的困境">对齐的困境</a><button aria-label="展开侧边栏分类 &#x27;对齐的困境&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/推理的觉醒">推理的觉醒</a><button aria-label="展开侧边栏分类 &#x27;推理的觉醒&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/效率的追求">效率的追求</a><button aria-label="展开侧边栏分类 &#x27;效率的追求&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/多模态的融合">多模态的融合</a><button aria-label="展开侧边栏分类 &#x27;多模态的融合&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/长文本的征途">长文本的征途</a><button aria-label="展开侧边栏分类 &#x27;长文本的征途&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第三章大语言模型技术发展历史">第三章：大语言模型技术发展历史</a><button aria-label="展开侧边栏分类 &#x27;第三章：大语言模型技术发展历史&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/结语：智能的新纪元">结语：智能的新纪元</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="页面路径"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主页面" class="breadcrumbs__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第四章从llm到agi"><span itemprop="name">第四章：从llm到agi</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/预训练的黎明"><span itemprop="name">预训练的黎明</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">GPT：单向的野心</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本页总览</button></div><div class="theme-doc-markdown markdown"><header><h1>GPT：单向的野心</h1></header><h2 class="anchor anchorWithStickyNavbar_LWe7" id="相反的方向">相反的方向<a href="#相反的方向" class="hash-link" aria-label="相反的方向的直接链接" title="相反的方向的直接链接">​</a></h2>
<p>在2018年的上半年，如果你在一个NLP学术会议上进行一次小范围的调查，问研究者们一个问题：&quot;预训练一个双向模型和预训练一个单向模型，哪个更有前景？&quot;，绝大多数人的答案会毫不犹豫：双向。这个直觉是合理的。双向模型能看到完整的上下文，获得更多的信息，那为什么不用呢？</p>
<p>但OpenAI的一个年轻研究员Alec Radford，却在思考一个看似疯狂的反向问题：为什么一定要做双向的呢？</p>
<p>这个想法不是凭空产生的。Radford和他的团队在阅读了大量关于语言建模的文献后，意识到了一个被许多人忽视的事实：生成任务本质上是单向的。当你写一段文字时，你一次只能写一个词。前面的词已经决定了，你无法改变它们。你只能基于已经写下的内容，决定下一个词应该是什么。这不仅是计算上的现实，更是语言生成的本质。</p>
<p>而自回归建模（autoregressive modeling）——逐个生成下一个词的方式——自然适应了这种单向的因果结构。如果你用自回归模型来生成文本，给定前面所有的词，模型学会了预测下一个词。这个过程不需要&quot;展望未来&quot;。事实上，让模型能够看到它还没有生成的未来词，从因果关系上讲是不对的。</p>
<p>这就是GPT选择单向因果建模的核心逻辑。而这个选择，尽管与当时学术界的主流直觉相悖，却会在接下来的几年里证明自己是深谋远虑的。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="transformer-decoder-only的激进选择">Transformer Decoder-Only的激进选择<a href="#transformer-decoder-only的激进选择" class="hash-link" aria-label="Transformer Decoder-Only的激进选择的直接链接" title="Transformer Decoder-Only的激进选择的直接链接">​</a></h2>
<p>Radford团队在设计GPT时面临的第一个架构决策是：用Transformer的哪一部分？</p>
<p>Transformer论文发表于2017年，它由两个对称的部分组成：编码器（Encoder）和解码器（Decoder）。编码器处理输入序列，通过自注意力生成隐表示。解码器处理输出序列，既能看到前面生成的输出（通过因果掩蔽），也能通过交叉注意力看到编码器的输出。这个结构对于机器翻译这样的序列到序列的任务非常合适——你需要理解源语言（编码器），然后生成目标语言（解码器）。</p>
<p>但对于语言建模，情况不同。语言建模本质上是一个单一序列的任务：给定前面的词，预测下一个词。你不需要编码和解码这样的二元结构。你只需要一个能够进行自回归生成的模块。</p>
<p>Radford的团队做出了一个激进的简化：他们丢弃了Transformer的编码器部分，只保留了解码器。更准确地说，他们保留了解码器的自注意力机制，但移除了编码器-解码器的交叉注意力。结果是一个纯粹的、单向的、自回归的架构。这个架构后来被称为Transformer Decoder-Only或Transformer-LM（Language Model）。</p>
<p>这个决定看似大胆，实际上却是极其聪慧的。一方面，它化繁为简，减少了架构的复杂性。另一方面，它直接针对语言建模这个任务进行了优化。不需要的交叉注意力被移除了，模型的所有注意力都用于捕捉同一序列内的依赖关系。</p>
<p><strong>关键的因果掩蔽机制</strong></p>
<p>Decoder-only架构的核心是因果掩蔽（causal masking）。在标准的自注意力计算中，每一个位置都能看到序列中所有其他位置。但在语言建模中，这是不允许的。一个词不应该能看到在它之后的词，因为那些词在生成时还没有产生。</p>
<p>因果掩蔽通过一个简单的技巧来实现：在计算注意力权重时，对于位置i，我们将所有j &gt; i的注意力权重设置为-∞（或一个很大的负数），这样在softmax之后这些权重就会变为0。结果是，位置i只能注意到位置0到i的内容。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 因果掩蔽的实现原理</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">causal_attention</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">query</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> key</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> value</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> seq_length</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    query, key, value: [batch_size, seq_length, d_model]</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    返回应用了因果掩蔽的注意力输出</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 计算注意力分数</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    scores </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> matmul</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">query</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> transpose</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">key</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> sqrt</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d_model</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># scores shape: [batch_size, seq_length, seq_length]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 创建因果掩蔽矩阵</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 这是一个下三角矩阵，只有i &gt;= j的位置是1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    causal_mask </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> create_lower_triangular_mask</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">seq_length</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># causal_mask shape: [seq_length, seq_length]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 应用掩蔽：将不应该看到的位置设为很小的值</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    scores </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> where</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">causal_mask</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> scores</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">1e9</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 计算注意力权重</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    attention_weights </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> softmax</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">scores</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dim</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 此时，对于位置i，attention_weights[i, j&gt;i] ≈ 0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 应用权重到value</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> matmul</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">attention_weights</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> value</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> output</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>这个看似简单的掩蔽机制，却有深远的意义。它在数学上强制了模型的因果结构，确保了自回归生成的可行性。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="从117m到预训练的实验">从117M到预训练的实验<a href="#从117m到预训练的实验" class="hash-link" aria-label="从117M到预训练的实验的直接链接" title="从117M到预训练的实验的直接链接">​</a></h2>
<p>2018年的OpenAI GPT（后来被称为GPT-1）使用了一个相对较小的模型：117M参数。在今天看来，这个规模已经不值一提，但在当时，这已经是一个可观的模型。</p>
<p>这个模型的预训练使用了一个包括约800万篇文章的数据集，称为BookCorpus。选择图书作为预训练语料是一个重要的设计决策。与互联网爬虫数据相比，图书往往质量更高、句子结构更复杂、语言更规范。这个选择影响了模型学到的语言特征。</p>
<p>预训练的目标非常直接：最大化语言建模的目标函数。给定一个句子中的前n-1个词，模型学习预测第n个词。这个过程非常古老——早可以追溯到计算机科学的早期。但在Transformer+大规模预训练的背景下，这个古老的想法突然展现出了新的力量。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># GPT预训练的目标函数</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">gpt_language_model_loss</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> batch_tokens</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> seq_length</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    model: GPT模型</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    batch_tokens: [batch_size, seq_length]</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    seq_length: 最大序列长度（如512）</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 前向传播</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    logits </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">batch_tokens</span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">:</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">:</span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># 输入所有词除了最后一个</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># logits shape: [batch_size, seq_length-1, vocab_size]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 目标是预测下一个词</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    targets </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> batch_tokens</span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">:</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">:</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># 所有词除了第一个</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># targets shape: [batch_size, seq_length-1]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 计算交叉熵损失</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    loss </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> cross_entropy_loss</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">logits</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> targets</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> loss</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>在这种简单而纯粹的预训练下，GPT学到了什么？论文的实验给出了惊人的答案。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="微调和多任务能力">微调和多任务能力<a href="#微调和多任务能力" class="hash-link" aria-label="微调和多任务能力的直接链接" title="微调和多任务能力的直接链接">​</a></h2>
<p>真正令人瞩目的地方在于微调阶段。Radford等人在预训练后，将GPT应用于多个下游NLP任务，并通过最少量的任务特定修改来进行微调。他们使用了一个几乎通用的微调框架，只在任务的输入/输出格式上做了必要的调整。</p>
<p>这是对当时NLP实践的一次颠覆。在2018年，标准的做法是为每个任务设计特定的架构。一个用于文本分类的模型和一个用于句子相似性任务的模型，会有明显不同的架构设计。但GPT团队展示的是，一个统一的预训练模型，配合最小化的任务适配层，就能在多个任务上取得竞争力的结果。</p>
<p>他们测试的9个任务包括：文本蕴含识别（RTE）、相似性检测（MRPC）、语义相似性（STS-B）、问答（QNLI）、问题对相似性（QQP）、视觉常识推理（VCR）、情感分析（SST）、语言可接受性（CoLA）和词汇相似性（MRPC等）。在这些任务中，GPT取得了接近或优于之前的SOTA（State-of-the-Art）结果。</p>
<p>更令人印象深刻的是，这些改进是通过一个极为简洁的微调策略取得的。论文的作者们甚至可以说，他们的方法在某种意义上是&quot;无参数高效&quot;的——大部分参数来自预训练，微调时只需要添加一个小的任务特定的线性层。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="与bert的对比生成vs理解的分歧">与BERT的对比：生成vs理解的分歧<a href="#与bert的对比生成vs理解的分歧" class="hash-link" aria-label="与BERT的对比：生成vs理解的分歧的直接链接" title="与BERT的对比：生成vs理解的分歧的直接链接">​</a></h2>
<p>当GPT论文发表几个月后，Google的BERT论文闪亮登场时，学术界突然意识到自己站在了一个十字路口。两个方向，都有雄厚的理论基础和实验支持。</p>
<p>BERT采用了ELMo所启发的双向预训练方向，但用Transformer Encoder代替了LSTM。它的核心思想是：通过遮蔽一些词，让模型学习从上下文中恢复这些词。这是一个完形填空任务，本质上是一个理解任务。BERT的模型架构是Transformer Encoder-Only——它没有因果结构，完全可以双向地看。</p>
<p>GPT则坚持了生成方向。它的预训练目标是标准的自回归语言建模，模型通过因果掩蔽只能单向地看。在架构上，GPT使用的是Transformer Decoder（带因果掩蔽）。</p>
<p>这个差异看似技术性的，实际上代表了两种不同的哲学：</p>
<p><strong>BERT的哲学</strong>：语言理解是首要的。如果你能够真正理解一个句子（通过双向上下文），那么你可以处理任何依赖于理解的任务。生成可以看作理解的一个应用。因此，BERT将预训练的资源集中在理解任务上。</p>
<p><strong>GPT的哲学</strong>：生成是最基本的。语言建模——预测下一个词——是语言学习的最纯粹的形式。它不需要显式的任务定义，可以在任何原始文本上进行。而且，如果你学会了生成，你实际上也学会了理解（因为生成过程隐含了对语言结构和语义的理解）。</p>
<p>实际上，从模型能力的角度看，两个哲学都有其合理性。BERT在纯理解任务（如文本分类、句子配对）上表现优异，因为它的双向结构直接对应这些任务的需求。而GPT在生成和开放式任务上有天然的优势。</p>
<p>但在2018年底到2019年初的时间点上，业界普遍认为BERT会是赢家。理由很充分：BERT在主流的NLP基准上的表现更好，而生成任务相对来说是一个小众的应用。几乎没人预测到，五年后，生成任务会成为AI界最热门的应用。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="alec-radford的远见与openai的策略">Alec Radford的远见与OpenAI的策略<a href="#alec-radford的远见与openai的策略" class="hash-link" aria-label="Alec Radford的远见与OpenAI的策略的直接链接" title="Alec Radford的远见与OpenAI的策略的直接链接">​</a></h2>
<p>但OpenAI似乎坚持了一种长期的视角。Alec Radford和他的团队在2018年末透露，他们已经在训练一个更大的GPT模型——参数量达到1.5B。这个模型会在一个更大的数据集（WebText）上训练，后来会以GPT-2的名义发布。</p>
<p>在GPT-1的论文中，Radford等人虽然看到了他们模型的多任务能力，但似乎还没有完全意识到单向因果建模会有多大的潜力。那个时刻，他们的话语中充满了谨慎和好奇，而不是确定和宣言。</p>
<p>但有一个细节值得注意。在论文中，作者们提到，他们观察到，随着预训练数据的增加，模型在下游任务上的性能也在改善。这个观察，在当时相对来说不是新闻，但它为后来的Scaling Laws埋下了种子。</p>
<p>而且，Radford的选择有一个深层的原因。由于GPT采用了单向因果建模，它天然适应于文本生成。这意味着，一旦模型足够大和足够好，它就能直接被用于生成任务，而不需要复杂的微调或任务特定的修改。这是一个强大的特性，即使在当时还没有被充分利用。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="代码实现与细节">代码实现与细节<a href="#代码实现与细节" class="hash-link" aria-label="代码实现与细节的直接链接" title="代码实现与细节的直接链接">​</a></h2>
<p>要理解GPT为什么有效，最好的方法是看一个简化的实现：</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token class-name">GPTModel</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Module</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> vocab_size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> d_model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_layers</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_heads</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> d_ff</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token builtin">super</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">embedding </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Embedding</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">vocab_size</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> d_model</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">pos_embedding </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> PositionalEncoding</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d_model</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Decoder-only堆栈</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">decoder_layers </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ModuleList</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            TransformerDecoderLayer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d_model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_heads</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> d_ff</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> _ </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_layers</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">lm_head </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Linear</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d_model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> vocab_size</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">forward</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> input_ids</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> attention_mask</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        input_ids: [batch_size, seq_length]</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        返回logits: [batch_size, seq_length, vocab_size]</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 嵌入和位置编码</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">embedding</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">input_ids</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># [batch_size, seq_length, d_model]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> x </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">pos_embedding</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">input_ids</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 创建因果掩蔽</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        causal_mask </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> create_causal_mask</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">input_ids</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">shape</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 通过decoder层</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> decoder_layer </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">decoder_layers</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> decoder_layer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> causal_mask</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">causal_mask</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 输出层到logits</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        logits </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">lm_head</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># [batch_size, seq_length, vocab_size]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> logits</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token class-name">TransformerDecoderLayer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Module</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> d_model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_heads</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> d_ff</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token builtin">super</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">self_attention </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> MultiHeadAttention</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d_model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_heads</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ffn </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> FeedForwardNetwork</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d_model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> d_ff</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">norm1 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">LayerNorm</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d_model</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">norm2 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">LayerNorm</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d_model</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">forward</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> causal_mask</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 自注意力（带因果掩蔽）</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        attn_output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">self_attention</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> mask</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">causal_mask</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">norm1</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> attn_output</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 前馈网络</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ffn_output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ffn</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        x </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">norm2</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> ffn_output</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> x</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 训练循环</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">train_step</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">model</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> batch_tokens</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> optimizer</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Forward pass</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    logits </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">batch_tokens</span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">:</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">:</span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 计算损失（预测下一个词）</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    loss </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> nn</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">functional</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cross_entropy</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        logits</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">view</span><span class="token punctuation" style="color:#393A34">(</span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> vocab_size</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        batch_tokens</span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">:</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">:</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">contiguous</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">view</span><span class="token punctuation" style="color:#393A34">(</span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Backward pass</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    optimizer</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zero_grad</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    loss</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">backward</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    optimizer</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">step</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> loss</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">item</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>这个实现的简洁性本身就说明了Decoder-only架构的优雅。没有编码器-解码器的复杂性，没有交叉注意力的额外复杂度。只有单纯的、因果掩蔽的自注意力，和一个标准的前馈网络。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="对后续发展的影响">对后续发展的影响<a href="#对后续发展的影响" class="hash-link" aria-label="对后续发展的影响的直接链接" title="对后续发展的影响的直接链接">​</a></h2>
<p>虽然在2018-2019年，学术界和业界似乎更看好BERT这个方向，但GPT其实已经为未来的发展打下了伏笔。</p>
<p>首先，Decoder-only的简洁性意味着它可以更容易地扩展到更大的规模。没有编码器-解码器的不对称性，没有需要对齐的两个部分。只要堆积更多的层和参数，就能得到一个更大的模型。</p>
<p>其次，因果建模和自回归生成的直接对应关系，意味着GPT天然适应于在无标注数据上进行大规模预训练。你不需要任何标注，只需要原始文本。而BERT虽然也不需要标注，但它的双向结构和完形填空目标，在大规模生成任务中就变成了一个限制因素。</p>
<p>第三，也是最重要的一点，GPT选择的这个方向——纯粹的、无条件的、自回归的语言建模——它的上限是什么？没人知道。这种未知性，对于研究者来说是吸引人的。也许，这就是真正通向通用人工智能的道路。</p>
<p>当后来的GPT-2（2019）、GPT-3（2020）相继发布，并展现出越来越惊人的能力时，人们才开始意识到，Alec Radford在2018年的选择，也许不是一个激进的赌博，而是一种深邃的先见。单向的野心，也许正是指向多能力未来的灯塔。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="开放的问题">开放的问题<a href="#开放的问题" class="hash-link" aria-label="开放的问题的直接链接" title="开放的问题的直接链接">​</a></h2>
<p>GPT-1时代遗留给我们的问题，比BERT时代的问题更具有开放性。首先，纯粹的自回归建模是否真的足够？它能否通过纯粹的参数增加来完成越来越复杂的任务？其次，Decoder-only架构的极限在哪里？是否存在某个模型大小或数据量，之后性能开始下降？第三，也许最有趣的：一个在自回归语言建模上预训练的模型，真的能够学到足够的&quot;理解&quot;来处理理解型任务吗？</p>
<p>这些问题，到GPT-2的时代仍然没有明确的答案。但那个时代，答案开始变得有趣起来。</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/edit/main/docs/cp4/预训练的黎明/GPT：单向的野心.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件选项卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/ELMo：上下文的双向觉醒"><div class="pagination-nav__sublabel">上一页</div><div class="pagination-nav__label">ELMo：上下文的双向觉醒</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/预训练的黎明/BERT：完形填空的艺术"><div class="pagination-nav__sublabel">下一页</div><div class="pagination-nav__label">BERT：完形填空的艺术</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#相反的方向" class="table-of-contents__link toc-highlight">相反的方向</a></li><li><a href="#transformer-decoder-only的激进选择" class="table-of-contents__link toc-highlight">Transformer Decoder-Only的激进选择</a></li><li><a href="#从117m到预训练的实验" class="table-of-contents__link toc-highlight">从117M到预训练的实验</a></li><li><a href="#微调和多任务能力" class="table-of-contents__link toc-highlight">微调和多任务能力</a></li><li><a href="#与bert的对比生成vs理解的分歧" class="table-of-contents__link toc-highlight">与BERT的对比：生成vs理解的分歧</a></li><li><a href="#alec-radford的远见与openai的策略" class="table-of-contents__link toc-highlight">Alec Radford的远见与OpenAI的策略</a></li><li><a href="#代码实现与细节" class="table-of-contents__link toc-highlight">代码实现与细节</a></li><li><a href="#对后续发展的影响" class="table-of-contents__link toc-highlight">对后续发展的影响</a></li><li><a href="#开放的问题" class="table-of-contents__link toc-highlight">开放的问题</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Read</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/intro">Introduction</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub Discussions<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models" target="_blank" rel="noopener noreferrer" class="footer__link-item" remarkplugins="" rehypeplugins="">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Whirling AI Consortium. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>