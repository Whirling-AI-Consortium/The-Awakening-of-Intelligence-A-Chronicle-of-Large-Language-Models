<!doctype html>
<html lang="zh-Hans" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-cp4/长文本的征途/从4K到百万Token的突破" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">从4K到百万Token的突破 | 智能的觉醒</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/长文本的征途/从4K到百万Token的突破"><meta data-rh="true" property="og:locale" content="zh_Hans"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="从4K到百万Token的突破 | 智能的觉醒"><meta data-rh="true" name="description" content="一场无声的军备竞赛"><meta data-rh="true" property="og:description" content="一场无声的军备竞赛"><link data-rh="true" rel="icon" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/长文本的征途/从4K到百万Token的突破"><link data-rh="true" rel="alternate" href="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/长文本的征途/从4K到百万Token的突破" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/长文本的征途/从4K到百万Token的突破" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/assets/css/styles.cd147f59.css">
<script src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/assets/js/runtime~main.23d89510.js" defer="defer"></script>
<script src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/assets/js/main.333741a6.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/logo.svg"><div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/"><div class="navbar__logo"><img src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/logo.svg" alt="My Book Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/logo.svg" alt="My Book Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">主页</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/本书导览">Read Book</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="文档侧边栏" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/本书导览">本书导览</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/intro">前言</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第一章智能的拂晓">第一章：智能的拂晓</a><button aria-label="展开侧边栏分类 &#x27;第一章：智能的拂晓&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第二章ai江湖-人物与格局">第二章：AI江湖-人物与格局</a><button aria-label="展开侧边栏分类 &#x27;第二章：AI江湖-人物与格局&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第四章从llm到agi">第四章：从llm到agi</a><button aria-label="折叠侧边栏分类 &#x27;第四章：从llm到agi&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/前言">从Transformer到LLM的技术图景</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/模板">模板</a><button aria-label="展开侧边栏分类 &#x27;模板&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/预训练的黎明">预训练的黎明</a><button aria-label="展开侧边栏分类 &#x27;预训练的黎明&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/规模的魔法">规模的魔法</a><button aria-label="展开侧边栏分类 &#x27;规模的魔法&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/涌现的惊喜">涌现的惊喜</a><button aria-label="展开侧边栏分类 &#x27;涌现的惊喜&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/对齐的困境">对齐的困境</a><button aria-label="展开侧边栏分类 &#x27;对齐的困境&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/推理的觉醒">推理的觉醒</a><button aria-label="展开侧边栏分类 &#x27;推理的觉醒&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/效率的追求">效率的追求</a><button aria-label="展开侧边栏分类 &#x27;效率的追求&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/多模态的融合">多模态的融合</a><button aria-label="展开侧边栏分类 &#x27;多模态的融合&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/长文本的征途">长文本的征途</a><button aria-label="折叠侧边栏分类 &#x27;长文本的征途&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/长文本的征途/RoPE与ALiBi：位置编码的进化">RoPE与ALiBi：位置编码的进化</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/长文本的征途/Sliding Window与Sparse Attention">Sliding Window与Sparse Attention</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/长文本的征途/从4K到百万Token的突破">从4K到百万Token的突破</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第三章大语言模型技术发展历史">第三章：大语言模型技术发展历史</a><button aria-label="展开侧边栏分类 &#x27;第三章：大语言模型技术发展历史&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/结语：智能的新纪元">结语：智能的新纪元</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="页面路径"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主页面" class="breadcrumbs__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第四章从llm到agi"><span itemprop="name">第四章：从llm到agi</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/长文本的征途"><span itemprop="name">长文本的征途</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">从4K到百万Token的突破</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本页总览</button></div><div class="theme-doc-markdown markdown"><header><h1>从4K到百万Token的突破</h1></header><h2 class="anchor anchorWithStickyNavbar_LWe7" id="一场无声的军备竞赛">一场无声的军备竞赛<a href="#一场无声的军备竞赛" class="hash-link" aria-label="一场无声的军备竞赛的直接链接" title="一场无声的军备竞赛的直接链接">​</a></h2>
<p>2022年初，大语言模型的上下文长度还停留在一个令人尴尬的水平。OpenAI的GPT-3支持2048个tokens，看起来勉强能处理一篇短新闻，但面对任何稍微长一些的文档就捉襟见肘。当时一个常见的使用方式是将长文本分割成多个片段，分别喂给模型，然后试图整合结果。这不仅低效，更切割了文本的连贯性。</p>
<p>但到了2023年，整个局面发生了戏剧性的转变。这一年成为了上下文长度的&quot;奇点&quot;：Claude从原始的9K扩展到100K tokens，GPT-4 Turbo支持128K tokens，而Gemini Pro在功能演进中也开始支持更长的上下文。到了2024年中期，Gemini 1.5 Pro宣布了一个让人瞠目结舌的数字——百万tokens。从2048到1000000，短短两年间上升了500倍。</p>
<p>这场竞赛的背后不仅仅是企业之间的品牌较量，更代表了一次深刻的技术突破。长上下文能力直接决定了大模型能做什么和不能做什么。一个支持4K tokens的模型只能处理约15页的文档；一个支持100K的模型可以一次处理整部小说；而一个百万token的模型可以同时容纳数百本书籍的信息。这不是简单的量变，而是质变——它重新定义了大模型应用的边界。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="从瓶颈到突破技术的五重门">从瓶颈到突破：技术的五重门<a href="#从瓶颈到突破技术的五重门" class="hash-link" aria-label="从瓶颈到突破：技术的五重门的直接链接" title="从瓶颈到突破：技术的五重门的直接链接">​</a></h2>
<p>要理解为什么从2K跨越到百万token如此困难，必须理解长上下文面临的五个核心瓶颈。</p>
<p><strong>第一道门：计算复杂度。</strong> 如前所述，标准的Attention机制是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。我们已经通过Sliding Window和稀疏Attention部分解决了这个问题，但代价是舍弃了某些全局信息。支持百万token意味着即使采用稀疏化，仍需处理数百亿的计算操作。</p>
<p><strong>第二道门：内存压力。</strong> 一个百万token的序列，即使只以INT8精度存储激活值，也需要超过1GB的显存仅用于KV缓存。在生成阶段，KV缓存成为了最严重的瓶颈。当批处理多个请求时，内存需求会成倍增长。</p>
<p><strong>第三道门：位置编码的外推。</strong> 我们在前一章讨论过，大多数预训练使用的位置编码（包括RoPE）都是在有限长度下训练的。当推理长度超过训练长度时，位置编码会出现失效。虽然内插方法（如NTK-Aware缩放）能缓解这个问题，但仍会导致性能下降。</p>
<p><strong>第四道门：&quot;Lost in the Middle&quot;问题。</strong> 这个看似反直觉但真实存在的现象表明，当上下文足够长时，模型对中间的信息会产生关注不足。在被要求从一个很长的文本中提取特定信息时，模型往往能记住开始和结尾的部分，但对中间部分的检索能力会下降。这不是因为模型不够聪明，而是因为Attention的学习动态——在长上下文下，注意力权重的分布会产生某种偏差。</p>
<p><strong>第五道门：训练的不稳定性。</strong> 在极长序列上进行预训练涉及到极其复杂的数值稳定性问题。梯度可能会出现爆炸或消失，学习率的设置变得极其敏感。此外，长序列训练对硬件同步和通信的要求也指数级增长。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="分布式的突破ring-attention与超越">分布式的突破：Ring Attention与超越<a href="#分布式的突破ring-attention与超越" class="hash-link" aria-label="分布式的突破：Ring Attention与超越的直接链接" title="分布式的突破：Ring Attention与超越的直接链接">​</a></h2>
<p>Anthropic在其100K token Claude模型中采取的方案之一是Ring Attention，这是一个在2023年由领先的研究团队提出的分布式注意力机制。Ring Attention的核心洞察是：与其在单个GPU上存储完整的KV缓存，不如在多个GPU间分布式地管理这些缓存。</p>
<p>Ring Attention的工作原理是这样的：假设我们有8个GPU，序列被分成8个段，每个GPU负责一个段。在计算Attention时，GPU 0上的查询需要与所有的Key和Value交互。Ring Attention采用了一个&quot;流水线&quot;的思路：GPU 0先与自己的KV计算，然后接收GPU 1的KV，计算完后将自己的KV传递给GPU 2，同时接收GPU 3的KV。这样形成了一个&quot;环&quot;——数据在GPU间流动，而不是集中存储。</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">ring_attention</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">Q</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> K</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> V</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_gpus</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    Ring Attention的简化伪代码</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    Q, K, V: (seq_len//num_gpus, dim)，每个GPU上的本地段</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">    &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    seq_len_local </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> Q</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">shape</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    output </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zeros_like</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">Q</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># 初始化：本地KV</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    KV_remote </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">K</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> V</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> step </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">num_gpus</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 1. 使用当前KV计算局部Attention</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        scores </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">matmul</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">Q</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> KV_remote</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">transpose</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        scores </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> scores </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> math</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">sqrt</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">Q</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">shape</span><span class="token punctuation" style="color:#393A34">[</span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        attn </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">softmax</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">scores</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dim</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">-</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        output </span><span class="token operator" style="color:#393A34">+=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">matmul</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">attn</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> KV_remote</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># 2. 循环传递KV到下一个GPU（环形拓扑）</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> step </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain"> num_gpus </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            KV_remote </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> rotate_to_next_gpu</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">KV_remote</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> output</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>这个设计的妙处在于它将分布式计算与Attention的全局依赖性巧妙地协调起来。通过8个GPU的Ring Attention，可以支持单GPU的8倍序列长度，同时计算时间几乎没有增加。这个技术成为了支持超长序列的关键基础设施。</p>
<p>但Ring Attention本身还不足以跨越到百万token。Google在Gemini 1.5的技术报告中暗示，他们采用了多层级的策略。对于最长的层，可能使用了更激进的稀疏化方案；对于中间层，使用Ring Attention；对于浅层，甚至可能使用某种动态的上下文管理——模型在解码过程中动态决定应该关注序列的哪些部分。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="长上下文检索从被动到主动">长上下文检索：从被动到主动<a href="#长上下文检索从被动到主动" class="hash-link" aria-label="长上下文检索：从被动到主动的直接链接" title="长上下文检索：从被动到主动的直接链接">​</a></h2>
<p>支持长上下文的另一个关键突破来自于检索增强（Retrieval Augmentation）的演进。最初的想法很简单：不要试图让模型一次性处理所有信息，而是在需要时动态检索相关部分。但这引发了一个新的问题：模型如何知道应该检索什么？</p>
<p>2023年的一个重要发现是，大模型本身可以学会&quot;元认知&quot;——即对自己知识的认知。论文《Self-RAG: Learning to Retrieve, Generate, and Reflect Critically》展示了一个有趣的想法：让模型在生成过程中，根据需要插入检索步骤。模型会学到在什么时候需要外部信息，然后显式地调用检索工具，获取相关内容后继续生成。</p>
<p>这个想法的优雅之处在于它不需要复杂的路由算法。模型通过训练自然地学会了何时需要检索。在某些任务上，Self-RAG的性能甚至超过了直接微调的模型，因为模型能够更精准地控制信息流——关键信息被及时检索，噪声被最小化。</p>
<p>结合这个思想，Claude和其他模型采取了一个混合策略：短期上下文（比如过去100个tokens）完全存储在KV缓存中以确保实时性，中期上下文通过压缩或摘要的形式保存，远期上下文则通过检索动态获取。这样可以在支持长序列和保持计算效率之间达到平衡。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="lost-in-the-middle的诅咒与救赎">&quot;Lost in the Middle&quot;的诅咒与救赎<a href="#lost-in-the-middle的诅咒与救赎" class="hash-link" aria-label="&quot;Lost in the Middle&quot;的诅咒与救赎的直接链接" title="&quot;Lost in the Middle&quot;的诅咒与救赎的直接链接">​</a></h2>
<p>当研究者们在2023年仔细分析长上下文模型的行为时，发现了一个令人困扰的规律：给定一个100K token的上下文和一个检索任务，模型对出现在开头或结尾的信息检索效果很好，但对中间部分的检索能力显著下降——即使这些信息在上下文中完全清晰可见。</p>
<p>这个现象的根源是Attention的梯度流动。在长序列上，Attention权重的学习动态导致模型偏向于&quot;关注&quot;序列的边界。论文《Lost in the Middle: How Language Models Use Long Contexts》的作者们测试了多个模型，包括GPT-3.5、Text-davinci-003等，都展现出了这种偏差。有趣的是，较小的模型（70B以下）的问题更严重，而更大的模型虽然有所缓解但仍存在。</p>
<p>解决这个问题的方法包括：第一，在训练阶段显式地使用随机长度的上下文，让模型学会在任意位置寻找信息。第二，在Attention设计中加入位置相关的权重调整，确保模型不会自动忽视中间部分。Anthropic在Claude中采取的策略可能包括两者的结合，加上某种显式的信息标记机制——让重要信息通过特殊的token被标记，从而吸引模型的注意力。</p>
<p>有趣的是，最新的研究表明，这个问题可能会在模型规模进一步扩大时自动缓解。OpenAI的研究暗示，当模型参数量达到某个阈值时，模型的全局定位能力会出现涌现——它能够更好地处理长上下文中的任意位置检索。这再次印证了我们在《规模的魔法》章节中讨论的缩放定律的威力。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="百万token的现实与愿景">百万token的现实与愿景<a href="#百万token的现实与愿景" class="hash-link" aria-label="百万token的现实与愿景的直接链接" title="百万token的现实与愿景的直接链接">​</a></h2>
<p>当Gemini 1.5在2024年中期宣布百万token支持时，技术社区为之震动。但这个成就到底意味着什么？它是如何实现的？</p>
<p>从Google发布的有限信息来看，百万token的实现可能涉及以下几个层面的创新：首先，该模型采用了某种多层级的表示——长上下文被压缩或层级化表示，不是所有token都以相同精度存储。其次，Attention可能被进一步稀疏化，或者采用了某种条件计算的方案——模型能够动态决定哪些部分需要精细的注意力计算。再次，KV缓存的管理被彻底重新设计，很可能采用了某种激进的量化或者分页式的内存管理。最后，位置编码被显著改进，可能不再是简单的RoPE或ALiBi，而是某种自适应的位置表示。</p>
<p>但即使有这些技术突破，百万token也面临一个根本的限制：实际的有用性。一个百万token的上下文相当于大约300本200页的书籍，但人工智能是否真的能够在这么庞大的信息中进行有意义的推理？这不仅涉及技术问题，更涉及认知科学的基本问题。</p>
<p>初步的实验结果令人惊讶。用百万token上下文的Gemini 1.5在某些信息检索任务上表现可靠，甚至在处理超长视频转录（百万token相当于约100小时的转录文本）时也表现出色。但在需要复杂推理的任务上，性能提升的幅度相对有限。这启发了一个思考：超长上下文的真正价值可能不在于复杂推理，而在于信息容纳和表面层面的检索——这本身就足以开启许多新的应用。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="未来无限长度的可能性">未来：无限长度的可能性<a href="#未来无限长度的可能性" class="hash-link" aria-label="未来：无限长度的可能性的直接链接" title="未来：无限长度的可能性的直接链接">​</a></h2>
<p>现在的问题是：我们是否能够实现真正的&quot;无限&quot;上下文？</p>
<p>从技术上看，瓶颈逐渐转向了硬件而非算法。如果我们有无限的GPU资源，Ring Attention之类的技术理论上可以支持任意长度的序列。但在实际中，通信开销会成为新的瓶颈——在成百上千个GPU间协调长序列的计算变成了一个配置和网络工程的问题，而不仅仅是算法问题。</p>
<p>另一个方向是彻底放弃&quot;处理完整上下文&quot;的想法，转向某种动态的、流式的上下文管理。想象一个模型能够以流式方式接收输入，动态地更新其内部状态，而不需要存储完整的历史。这种想法与某些人脑的工作方式更接近——我们不是同时处理所有记忆，而是动态地激活相关的信息。</p>
<p>论文《StreamingLLM: Efficient Streaming Language Models with Attention Sinks》探索了这个方向。关键的洞察是：在长推理过程中，模型的Attention权重会高度集中在最近的少数几个token（称为&quot;Attention Sinks&quot;）。StreamingLLM提议只保留这些关键token，加上最新的token，其他旧token则被逐渐遗忘。这样可以用常数内存支持无限长序列的流式处理。</p>
<p>还有一个根本性的问题需要思考：在百万token或更长的上下文中，模型是否真的在&quot;理解&quot;，还是在进行某种复杂的表面层面匹配？当Claude声称能够处理整部小说并回答深层问题时，它是真的在&quot;阅读&quot;小说，还是在执行某种高级的模式识别？这是一个哲学问题，但也是技术问题——因为答案决定了长上下文技术的真正价值上限。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="收官">收官<a href="#收官" class="hash-link" aria-label="收官的直接链接" title="收官的直接链接">​</a></h2>
<p>从2022年的2048 tokens到2024年的百万 tokens，我们见证了一次技术的飞跃。这个飞跃不是单一技术的胜利，而是多项创新的叠加：位置编码的进化（RoPE）、稀疏Attention的成熟（Sliding Window）、分布式算法的突破（Ring Attention）、模型架构的优化，以及对长上下文问题本质的更深理解。</p>
<p>每一步的背后都是研究者对问题的持续追问：为什么全局Attention是必要的？为什么位置信息会失效？为什么模型会忽视中间的信息？通过不断地问和答，我们逐渐从&quot;让模型处理更长序列&quot;这个工程问题，演进到&quot;模型如何理解长文本&quot;这个认知问题。</p>
<p>长上下文的军备竞赛还远未结束。即使实现了百万token，仍有更多的问题等待解答。但可以确定的是，长上下文能力已经成为了大模型的一项基本要求。它不再是一个边缘的能力指标，而是与模型的智能程度一样重要的维度。当下一代模型诞生时，我们可能会期待它不仅更聪慧，更重要的是能够记住更多。</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/edit/main/docs/cp4/长文本的征途/从4K到百万Token的突破.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件选项卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp4/长文本的征途/Sliding Window与Sparse Attention"><div class="pagination-nav__sublabel">上一页</div><div class="pagination-nav__label">Sliding Window与Sparse Attention</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第三章大语言模型技术发展历史"><div class="pagination-nav__sublabel">下一页</div><div class="pagination-nav__label">第三章：大语言模型技术发展历史</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#一场无声的军备竞赛" class="table-of-contents__link toc-highlight">一场无声的军备竞赛</a></li><li><a href="#从瓶颈到突破技术的五重门" class="table-of-contents__link toc-highlight">从瓶颈到突破：技术的五重门</a></li><li><a href="#分布式的突破ring-attention与超越" class="table-of-contents__link toc-highlight">分布式的突破：Ring Attention与超越</a></li><li><a href="#长上下文检索从被动到主动" class="table-of-contents__link toc-highlight">长上下文检索：从被动到主动</a></li><li><a href="#lost-in-the-middle的诅咒与救赎" class="table-of-contents__link toc-highlight">&quot;Lost in the Middle&quot;的诅咒与救赎</a></li><li><a href="#百万token的现实与愿景" class="table-of-contents__link toc-highlight">百万token的现实与愿景</a></li><li><a href="#未来无限长度的可能性" class="table-of-contents__link toc-highlight">未来：无限长度的可能性</a></li><li><a href="#收官" class="table-of-contents__link toc-highlight">收官</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Read</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/intro">Introduction</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub Discussions<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models" target="_blank" rel="noopener noreferrer" class="footer__link-item" remarkplugins="" rehypeplugins="">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Whirling AI Consortium. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>