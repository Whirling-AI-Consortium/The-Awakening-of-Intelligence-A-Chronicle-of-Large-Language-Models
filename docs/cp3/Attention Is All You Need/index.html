<!doctype html>
<html lang="zh-Hans" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-cp3/Attention Is All You Need" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Attention Is All You Need | 智能的觉醒</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/Attention Is All You Need"><meta data-rh="true" property="og:locale" content="zh_Hans"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Attention Is All You Need | 智能的觉醒"><meta data-rh="true" name="description" content="一个简洁而大胆的主张"><meta data-rh="true" property="og:description" content="一个简洁而大胆的主张"><link data-rh="true" rel="icon" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/Attention Is All You Need"><link data-rh="true" rel="alternate" href="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/Attention Is All You Need" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/Attention Is All You Need" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/assets/css/styles.cd147f59.css">
<script src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/assets/js/runtime~main.23d89510.js" defer="defer"></script>
<script src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/assets/js/main.333741a6.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/logo.svg"><div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/"><div class="navbar__logo"><img src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/logo.svg" alt="My Book Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/logo.svg" alt="My Book Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">主页</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/本书导览">Read Book</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="文档侧边栏" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/本书导览">本书导览</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/intro">前言</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第一章智能的拂晓">第一章：智能的拂晓</a><button aria-label="展开侧边栏分类 &#x27;第一章：智能的拂晓&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第二章ai江湖-人物与格局">第二章：AI江湖-人物与格局</a><button aria-label="展开侧边栏分类 &#x27;第二章：AI江湖-人物与格局&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第四章从llm到agi">第四章：从llm到agi</a><button aria-label="展开侧边栏分类 &#x27;第四章：从llm到agi&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第三章大语言模型技术发展历史">第三章：大语言模型技术发展历史</a><button aria-label="折叠侧边栏分类 &#x27;第三章：大语言模型技术发展历史&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/前言">前言</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/从词语开始">从词语开始</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/预测的艺术">预测的艺术</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/廉价的智慧">廉价的智慧</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/序列的记忆">序列的记忆</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/Attention Is All You Need">Attention Is All You Need</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/聚焦的艺术">聚焦的艺术</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/从无到有">从无到有</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/结语：智能的新纪元">结语：智能的新纪元</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="页面路径"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主页面" class="breadcrumbs__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第三章大语言模型技术发展历史"><span itemprop="name">第三章：大语言模型技术发展历史</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Attention Is All You Need</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本页总览</button></div><div class="theme-doc-markdown markdown"><header><h1>Attention Is All You Need</h1></header><h3 class="anchor anchorWithStickyNavbar_LWe7" id="一个简洁而大胆的主张">一个简洁而大胆的主张<a href="#一个简洁而大胆的主张" class="hash-link" aria-label="一个简洁而大胆的主张的直接链接" title="一个简洁而大胆的主张的直接链接">​</a></h3>
<p>2017年6月，Google Brain的一个研究团队发表了一篇论文。标题简洁而大胆，就四个单词：《注意力就是一切所需》（Attention Is All You Need）。</p>
<p>这个标题本身就是一个宣言。它直指问题的核心，同时挑战了当时NLP领域的共识。这篇论文的作者包括Ashish Vaswani、Sharan Katagiri、Lukasz Kaiser、Illia Polosukhin和其他人。他们的主张非常激进：</p>
<blockquote>
<p>目前主流的序列转导模型基于复杂的循环或卷积神经网络，包括编码器和解码器。性能最好的模型也通过注意力机制连接编码器和解码器。我们提出了一个新的简洁网络架构——Transformer，它完全基于注意力机制，完全摒弃了循环和卷积。</p>
</blockquote>
<p>这不是一个温和的建议。这是一个对整个领域的直接挑战。在2017年，RNN编码器-解码器已经是正统教义。无论有多少改进，没有人怀疑循环结构是处理序列的必要条件。毕竟，序列就是——按顺序的。而循环神经网络就是按顺序处理信息的方式。</p>
<p>但Vaswani的团队问了一个根本性的问题：<strong>我们真的需要循环吗？</strong></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="为什么是现在">为什么是现在？<a href="#为什么是现在" class="hash-link" aria-label="为什么是现在？的直接链接" title="为什么是现在？的直接链接">​</a></h3>
<p>这个问题为什么在2017年被提出，而不是更早或更晚？</p>
<p>关键在于<strong>可扩展性</strong>。Vaswani团队指出了RNN的根本限制：</p>
<blockquote>
<p>循环模型通常沿着输入和输出序列的符号位置分解计算。将位置对齐到计算时间中的步骤，它们生成一系列隐藏状态<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>，作为前一个隐藏状态<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9028em;vertical-align:-0.2083em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span></span></span></span>和位置<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span>的输入的函数。这种内在的顺序性质排除了在训练示例内的并行化，这在较长序列长度时变得至关重要，因为内存限制限制了跨示例的批处理。</p>
</blockquote>
<p>换句话说，RNN的顺序性是致命的瓶颈。当处理一个100个词的句子时，你必须进行100个顺序步骤。即使你有最强大的GPU，也无法绕过这个限制。而在大模型时代，处理数十亿个样本时，这个顺序性成为了不可容忍的瓶颈。</p>
<p>但注意力机制不同。注意力是高度并行化的。给定所有的查询、键和值，你可以在一个矩阵乘法中同时计算所有的注意力权重。GPU擅长做什么？就是进行大规模的矩阵运算。</p>
<p>这正是Word2Vec成功的另一个原因。回忆一下，Word2Vec之所以胜过Bengio的方法，不仅是因为它更简单，还因为它能扩展到大数据集。现在，Transformer提出的是同样的策略应用于序列模型：<strong>简化架构，使其能够充分利用现代硬件的并行计算能力</strong>。</p>
<p>Vaswani团队的结果震撼了整个领域：</p>
<blockquote>
<p>Transformer允许显著更多的并行化，在8个P100 GPU上仅训练12小时后就能达到机器翻译质量的新技术水平。</p>
</blockquote>
<p>从数小时到12小时。相比之下，许多RNN模型需要几周才能训练。这不仅是速度上的改进，这是一个数量级的差异。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="架构简洁但强大">架构：简洁但强大<a href="#架构简洁但强大" class="hash-link" aria-label="架构：简洁但强大的直接链接" title="架构：简洁但强大的直接链接">​</a></h3>
<p>Transformer的架构看起来很复杂，但如果理解了来龙去脉，其实相当直接。核心思想是：<strong>保留编码器-解码器框架，但用注意力替代所有其他部分</strong>。</p>
<p>编码器由多层自注意力组成。每一层都让序列中的每个位置能够看到序列中的所有其他位置，并学习对不同位置赋予不同的注意力。解码器也有自注意力，但增加了一个关键的修改：<strong>掩码</strong>。因为解码器是从左到右逐词生成输出的（自回归框架），我们不能让某个位置&quot;看到&quot;未来的词。所以我们使用掩码注意力，遮挡掉所有未来的位置。</p>
<p>编码器和解码器之间有跨注意力，这完全类似于Bahdanau提出的注意力机制。解码器中的每个词都可以关注编码器的输出，从而&quot;对齐&quot;源序列中的相关部分。</p>
<p>还有一个关键的部分叫做<strong>位置编码</strong>（Positional Encoding）。这是必要的，因为纯粹的注意力机制没有固有的顺序性。一个包含词A、词B、词C的序列和包含词C、词B、词A的序列，在纯粹的注意力下是相同的。为了保留序列的顺序信息，Transformer为每个位置的输入添加了位置相关的向量。这些向量被设计为捕捉相对位置的信息。</p>
<p>多头注意力是另一个关键部分。与其使用单一的注意力函数，Transformer并行运行多个注意力函数（比如8个或16个&quot;头&quot;），每个头学习不同的表示方式。这让模型能够同时关注序列的不同方面。这个想法并不新颖——在Bahdanau的论文中，作者们实际上可以这样做，只是因为计算成本太高而没有这样做。但在矩阵化的Transformer中，这变成了标准做法。</p>
<p>Vaswani用一个优雅的公式定义了缩放点积注意力：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi mathvariant="normal">⊤</mi></msup></mrow><msqrt><msub><mi>D</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{D_k}}\right)V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.8em;vertical-align:-0.65em"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.095em"><span style="top:-2.5903em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8567em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mtight" style="padding-left:0.833em"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3488em;margin-left:-0.0278em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em"><span></span></span></span></span></span></span></span></span><span style="top:-2.8167em"><span class="pstrut" style="height:3em"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1833em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.4461em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.927em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size2">)</span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span></span></span></span></p>
<p>这里Q是查询矩阵，K是键矩阵，V是值矩阵。那个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><msub><mi>D</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{D_k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1883em"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8517em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:0.833em"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.8117em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1883em"><span></span></span></span></span></span></span></span></span>的缩放因子是为了防止点积变得过大（这会导致softmax的梯度过小）。整个计算可以在一个矩阵操作中完成，极其高效。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="上下文的力量">上下文的力量<a href="#上下文的力量" class="hash-link" aria-label="上下文的力量的直接链接" title="上下文的力量的直接链接">​</a></h3>
<p>理解Transformer的关键在于将其置于正确的历史背景中。Transformer不是凭空出现的天才灵感。它是一系列想法的自然结晶：</p>
<ul>
<li>Bengio关于分布式表示的想法（2003年）</li>
<li>Word2Vec关于简化和扩展的想法（2013年）</li>
<li>Bahdanau关于注意力机制的想法（2014年）</li>
<li>Luong系统化注意力的想法（2015年）</li>
<li>Cheng关于自注意力的想法（2016年）</li>
</ul>
<p>Transformer将这些想法整合在一起，去除了RNN这个最后残留的复杂性。这是一个概念上的飞跃吗？是的。但它是一个<strong>合理的、可预期的飞跃</strong>。</p>
<p>当这篇论文被发表时，它并不是AlexNet那样震撼性的胜利。Transformer在机器翻译基准测试上表现更好，但并不是压倒性的改进。真正的优势是效率。论文对比了同时期的另一个摒弃循环、转而使用卷积的模型ConvS2S。在某些基准上，两者的翻译质量接近。但在计算效率上，Transformer赢得了压倒性的胜利：所需的浮点运算数减少了360倍！</p>
<p>这正是大模型时代的关键洞察：<strong>在可扩展性的前沿，有时候&quot;足够好&quot;加上&quot;高效&quot;就足以改变一切</strong>。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="生成式预训练的时代">生成式预训练的时代<a href="#生成式预训练的时代" class="hash-link" aria-label="生成式预训练的时代的直接链接" title="生成式预训练的时代的直接链接">​</a></h3>
<p>但是，仅有Transformer架构还不够。要创造今天人们与之互动的大语言模型，还需要另一个关键创新：<strong>如何训练这些模型</strong>。</p>
<p>2018年，一年之后，OpenAI发表了《通过生成式预训练改进语言理解》（Improving language understanding by generative pre-training）。这篇论文提出了一个看似简单但深刻的想法：用尽可能多的无标注数据预训练一个Transformer，然后用特定任务的有标注数据进行微调。</p>
<p>第一步叫做<strong>生成式预训练</strong>（Generative Pre-training）。目标仍然是我们从Bengio时代就知道的东西：最大化对数似然，也就是最小化下一个词预测的损失：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>G</mi><mi>P</mi><mi>T</mi></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="normal">Θ</mi><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi mathvariant="normal">Θ</mi></msub><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mi>N</mi><mo>:</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L_{GPT}(\Theta) = \sum_{t=1}^{T} \log p_\Theta(w_t | w_{t-N:t-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">GPT</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">Θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2809em;vertical-align:-0.2997em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">Θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<p>这完全是无监督的。互联网上有数十亿的文本，都可以被用来进行这个任务。没有标注者，没有昂贵的数据标注成本，只有纯粹的自回归语言建模。</p>
<p>但这不是全部。第二步叫做<strong>判别式微调</strong>（Discriminative Fine-tuning）。在预训练之后，模型被应用到具体的任务上，比如问答、情感分析、文本摘要等。在这一步，我们有真实的(输入, 输出)对，可以用监督学习来训练模型做特定的任务。</p>
<p>为什么这两步都需要？想象一个用生成式预训练训练出来的模型。它能预测句子中的下一个词。但这未必意味着它能做有用的事情。比如，一个关于心理健康建议的查询可能得到一个冷漠甚至有害的回答，因为从互联网文本的统计角度看，这样的回答是&quot;可能的&quot;。所以我们需要微调来将模型对齐到人类的意图。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="预训练概念的历史">预训练概念的历史<a href="#预训练概念的历史" class="hash-link" aria-label="预训练概念的历史的直接链接" title="预训练概念的历史的直接链接">​</a></h3>
<p>但这里有一个有趣的历史细节：生成式预训练的想法并不新。</p>
<p>早在2006年，Bengio等人在《深度网络的贪心逐层训练》（Greedy layer-wise training of deep networks）中就写道：</p>
<blockquote>
<p>我们假设这个策略的三个方面特别重要：首先，以贪心的方式逐层预训练；其次，在每层使用无监督学习来保留输入信息；最后，根据最终目标准则微调整个网络。</p>
</blockquote>
<p>甚至更早的工作中，研究者们就使用预训练的词向量，然后在特定任务上微调。还记得ELMo吗？那就是预训练然后微调。</p>
<p>所以OpenAI在2018年做的，严格来说不是发明了一个新概念。他们做的是<strong>在前所未有的规模上应用这个概念</strong>。用尽可能多的数据、最先进的架构（Transformer）和巨大的计算资源，来进行生成式预训练。这个规模的变化导致了质量的变化。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="从gpt-1到gpt-3">从GPT-1到GPT-3<a href="#从gpt-1到gpt-3" class="hash-link" aria-label="从GPT-1到GPT-3的直接链接" title="从GPT-1到GPT-3的直接链接">​</a></h3>
<p>GPT-1（2018年）有1.17亿个参数，在一个包含约800万个网页的数据集上训练。这个规模在今天看来可能很小，但在2018年是前所未有的。</p>
<p>一年后，2019年，OpenAI发表了GPT-2。标题是《语言模型是无监督的多任务学习者》（Language models are unsupervised multitask learners）。这篇论文的关键发现是，用GPT架构训练的大型语言模型，在许多下游任务上都表现出色，<strong>甚至不需要任务特定的微调</strong>！这叫做&quot;零样本&quot;学习——模型被直接应用到没见过的任务上。</p>
<p>GPT-2有15亿个参数，在一个包含40亿个网页的更大数据集上训练。这已经是GPT-1的大约10倍的参数和数据。</p>
<p>然后在2020年，OpenAI发表了GPT-3的论文《语言模型是少样本学习者》（Language models are few-shot learners）。GPT-3有1750亿个参数，训练数据包括Common Crawl、WebText2和其他来源，总共包含约45TB的文本。论文中的一个关键声明是：</p>
<blockquote>
<p>对于所有任务，GPT-3在无任何梯度更新或微调的情况下应用。</p>
</blockquote>
<p>这对许多人来说是震撼性的。一个单一的、经过预训练的模型，不经过任何任务特定的调整，就能在翻译、问答、摘要、推理等各种任务上表现得相当不错。这暗示了一个深刻的真理：<strong>大型语言模型中存在着某种通用的&quot;智能&quot;，这种智能可以被激活来做许多不同的事情</strong>。</p>
<p>从GPT-1的1.17亿参数到GPT-3的1750亿参数，增长了1500倍。这个规模的扩张伴随着能力的质的飞跃。这不仅仅是模型变大了，而是某种新的能力涌现了。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="缩放律与涌现">缩放律与涌现<a href="#缩放律与涌现" class="hash-link" aria-label="缩放律与涌现的直接链接" title="缩放律与涌现的直接链接">​</a></h3>
<p>在GPT-3论文和后来的许多研究中，一个模式变得清晰：<strong>当你让神经网络变大、用更多数据训练时，性能通常会以可预测的方式改进</strong>。这被称为&quot;缩放律&quot;（Scaling Laws）。</p>
<p>但更有趣的是，有时候改进不是平缓的。有时候，性能会突然跳跃。模型突然学会了它之前无法做的事情。这叫做<strong>涌现</strong>（Emergence）。这是大型语言模型中最神秘、最令人困惑的现象之一。</p>
<p>为什么会这样？如果一个较小的模型不能进行多位数加法，为什么一个稍大一点的模型就能？如果我们增加的只是参数数量和训练数据，为什么会出现完全新的能力？这仍然是一个开放的研究问题。</p>
<p>但无论其机制如何，涌现是不可否认的现象。当模型规模达到某个阈值时，新的能力似乎就从中浮现出来。这对AI安全和AI能力预测都有深刻的含义。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="那一刻的意义">那一刻的意义<a href="#那一刻的意义" class="hash-link" aria-label="那一刻的意义的直接链接" title="那一刻的意义的直接链接">​</a></h3>
<p>回过头来看，Transformer的出现和生成式预训练的应用，标志着深度学习在自然语言处理中的真正胜利。但更重要的是，它标志着一个新范式的开始。</p>
<p>在Transformer之前，NLP的主流思想是设计特殊的架构和目标函数来解决特定的问题。你想做机器翻译？设计一个编码器-解码器。你想做文本分类？设计一个适当的模型头。你想做问答？又是另一个特殊设计。</p>
<p>但Transformer改变了这一切。它说，一个通用的架构，用一个简单的目标函数（下一个词预测），在足够大的规模和足够多的数据上进行预训练，就能做许多不同的事情。这是一个范式转变。</p>
<p>从这一刻开始，NLP不再是关于巧妙地设计模型和任务的领域。它变成了一场关于<strong>数据、计算和模型规模</strong>的竞争。谁能获得最多的数据，谁能用最好的硬件进行最长的训练，谁就能构建最好的模型。</p>
<p>这场竞争激烈而持久。它驱动了GPU市场的增长。它改变了研究机构能够进行的工作类型。它集中了AI能力到拥有巨大资源的少数公司。</p>
<p>但最重要的是，它证明了一个古老的计算机科学直觉是正确的——当面对一个复杂问题时，有时候最好的解决方案不是更聪明的算法，而是<strong>更大的规模、更多的数据和更多的计算</strong>。这个直觉从来没有像在大语言模型中那样彻底地被验证过。</p>
<p>而这，正是我们所在时代的根本特征。</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/edit/main/docs/cp3/Attention Is All You Need.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件选项卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/序列的记忆"><div class="pagination-nav__sublabel">上一页</div><div class="pagination-nav__label">序列的记忆</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/聚焦的艺术"><div class="pagination-nav__sublabel">下一页</div><div class="pagination-nav__label">聚焦的艺术</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#一个简洁而大胆的主张" class="table-of-contents__link toc-highlight">一个简洁而大胆的主张</a></li><li><a href="#为什么是现在" class="table-of-contents__link toc-highlight">为什么是现在？</a></li><li><a href="#架构简洁但强大" class="table-of-contents__link toc-highlight">架构：简洁但强大</a></li><li><a href="#上下文的力量" class="table-of-contents__link toc-highlight">上下文的力量</a></li><li><a href="#生成式预训练的时代" class="table-of-contents__link toc-highlight">生成式预训练的时代</a></li><li><a href="#预训练概念的历史" class="table-of-contents__link toc-highlight">预训练概念的历史</a></li><li><a href="#从gpt-1到gpt-3" class="table-of-contents__link toc-highlight">从GPT-1到GPT-3</a></li><li><a href="#缩放律与涌现" class="table-of-contents__link toc-highlight">缩放律与涌现</a></li><li><a href="#那一刻的意义" class="table-of-contents__link toc-highlight">那一刻的意义</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Read</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/intro">Introduction</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub Discussions<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models" target="_blank" rel="noopener noreferrer" class="footer__link-item" remarkplugins="" rehypeplugins="">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Whirling AI Consortium. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>