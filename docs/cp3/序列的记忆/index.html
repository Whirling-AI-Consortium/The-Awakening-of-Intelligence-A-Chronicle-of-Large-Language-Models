<!doctype html>
<html lang="zh-Hans" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-cp3/序列的记忆" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">序列的记忆 | 智能的觉醒</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/序列的记忆"><meta data-rh="true" property="og:locale" content="zh_Hans"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="序列的记忆 | 智能的觉醒"><meta data-rh="true" name="description" content="固定窗口的局限"><meta data-rh="true" property="og:description" content="固定窗口的局限"><link data-rh="true" rel="icon" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/序列的记忆"><link data-rh="true" rel="alternate" href="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/序列的记忆" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://whirling-ai-consortium.github.io/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/序列的记忆" hreflang="x-default"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/assets/css/styles.cd147f59.css">
<script src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/assets/js/runtime~main.23d89510.js" defer="defer"></script>
<script src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/assets/js/main.333741a6.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/logo.svg"><div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/"><div class="navbar__logo"><img src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/logo.svg" alt="My Book Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/img/logo.svg" alt="My Book Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">主页</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/本书导览">Read Book</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="文档侧边栏" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/本书导览">本书导览</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/intro">前言</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第一章智能的拂晓">第一章：智能的拂晓</a><button aria-label="展开侧边栏分类 &#x27;第一章：智能的拂晓&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第二章ai江湖-人物与格局">第二章：AI江湖-人物与格局</a><button aria-label="展开侧边栏分类 &#x27;第二章：AI江湖-人物与格局&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第四章从llm到agi">第四章：从llm到agi</a><button aria-label="展开侧边栏分类 &#x27;第四章：从llm到agi&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第三章大语言模型技术发展历史">第三章：大语言模型技术发展历史</a><button aria-label="折叠侧边栏分类 &#x27;第三章：大语言模型技术发展历史&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/前言">前言</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/从词语开始">从词语开始</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/预测的艺术">预测的艺术</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/廉价的智慧">廉价的智慧</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/序列的记忆">序列的记忆</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/Attention Is All You Need">Attention Is All You Need</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/聚焦的艺术">聚焦的艺术</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/从无到有">从无到有</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/结语：智能的新纪元">结语：智能的新纪元</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="页面路径"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主页面" class="breadcrumbs__link" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/category/第三章大语言模型技术发展历史"><span itemprop="name">第三章：大语言模型技术发展历史</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">序列的记忆</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本页总览</button></div><div class="theme-doc-markdown markdown"><header><h1>序列的记忆</h1></header><h3 class="anchor anchorWithStickyNavbar_LWe7" id="固定窗口的局限">固定窗口的局限<a href="#固定窗口的局限" class="hash-link" aria-label="固定窗口的局限的直接链接" title="固定窗口的局限的直接链接">​</a></h3>
<p>Word2Vec的成功是巨大的，但它也揭示了一个根本性的限制。所有这些漂亮的词向量，无论是通过Skip-gram、CBOW还是其他方法学到的，都有同一个致命缺陷：它们是<strong>固定的</strong>。</p>
<p>每一个词对应一个向量，不管这个词出现在什么上下文。&quot;bank&quot;在&quot;river bank&quot;（河岸）和&quot;savings bank&quot;（储蓄银行）中获得的是同一个向量。一个固定的词向量必须在这两个完全不同的含义之间进行某种妥协，最终学到的是一个模糊的、介于两者之间的表示。</p>
<p>更根本的问题是，固定的词向量无法捕捉<strong>长距离的依赖关系</strong>。考虑这样一个句子：&quot;The bank that I opened with my friend five years ago was very successful.&quot; 在这里，&quot;bank&quot;的含义不仅取决于它周围的几个词，还取决于整个句子的背景。一个固定的词向量，特别是那些只在有限窗口（比如±2个词）上训练的词向量，根本无法获得这样的全局上下文。</p>
<p>在Bengio和Mikolov的模型中，虽然他们创新的地方在于用神经网络学习词的分布式表示，但他们仍然受困于一个硬约束：上下文窗口的大小。在Bengio的模型中，输入必须是固定的N个词。这使得这些方法只能看到有限的上下文，无论N设置为多大。</p>
<p>在2010年代初期，一些研究者尝试了各种方法来突破这个限制。有人用卷积神经网络处理句子，试图用更大的感受野来捕捉更广的上下文。但这些方法都有一个共同的问题：它们最终学到的表示仍然无法有效地处理任意长的序列，无法捕捉真正的长距离依赖关系。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="记忆中的循环">记忆中的循环<a href="#记忆中的循环" class="hash-link" aria-label="记忆中的循环的直接链接" title="记忆中的循环的直接链接">​</a></h3>
<p>解决这个问题的关键想法并不新颖。事实上，它的根源可以追溯到神经网络理论的早期。</p>
<p>在1990年，Jeffrey Elman发表了一篇论文《在时间中寻找结构》（Finding Structure in Time）。这篇论文提出了一个看似简单但深刻的想法：如果一个神经网络的输出可以反馈到它自己的输入，形成一个循环，会怎样？这样的网络可以利用这些反馈连接来维持一种&quot;记忆&quot;或&quot;内部状态&quot;，这个状态可以随着时间序列的处理而改变。</p>
<p>这就是<strong>循环神经网络</strong>（Recurrent Neural Network，RNN）的起源。Elman的想法很快受到其他人的关注。在1997年，Sepp Hochreiter和Jürgen Schmidhuber发表了一篇后来被证明极其重要的论文《长短期记忆》（Long Short-Term Memory），提出了LSTM单元。</p>
<p>为什么LSTM如此重要？因为它解决了RNN的一个根本性问题：<strong>梯度消失问题</strong>（Vanishing Gradient Problem）。这是一个复杂的技术细节，但核心思想是这样的：当你用反向传播算法训练RNN时，误差信号需要从输出一路反向传播回输入。但在很长的序列中，这个误差信号会变得越来越小，最终变成接近零，就像一个极小的数不断被乘以介于0和1之间的数。经过足够多的乘法后，这个数会变得不可检测地小。这意味着，RNN很难学到对长距离的依赖关系——因为用于学习这些依赖关系的梯度已经消失了。</p>
<p>LSTM通过引入一个巧妙的结构来解决这个问题。它使用了<strong>记忆单元</strong>（Memory Cell）和<strong>门机制</strong>（Gate Mechanism）。记忆单元可以在时间步之间保持信息，而门机制则控制信息的流动。最重要的是，这个结构使得误差信号可以更有效地反向传播，梯度不会像在普通RNN中那样急速消失。</p>
<p>从1990年代末开始，RNN和LSTM就已经被开发出来了。但它们为什么在NLP社区中没有立即产生革命性的影响？最直接的原因是计算成本。训练RNN和LSTM很慢，特别是在处理长序列时。在CPU时代，这意味着训练一个中等规模的RNN模型可能需要数周。</p>
<p>但到了2010年代初，情况开始改变。GPU的计算能力不断提升，深度学习框架逐渐成熟。突然之间，训练RNN变成了可行的。Tomas Mikolov等人在2010年的论文中展示了如何用RNN来构建语言模型。Alex Graves在2013年的工作中展示了RNN可以用来做序列标注和其他任务。人们开始意识到，这些&quot;旧&quot;的循环网络在现代硬件上可以做一些非常强大的事情。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="从固定到可变序列到序列模型">从固定到可变：序列到序列模型<a href="#从固定到可变序列到序列模型" class="hash-link" aria-label="从固定到可变：序列到序列模型的直接链接" title="从固定到可变：序列到序列模型的直接链接">​</a></h3>
<p>但问题仍然存在：RNN虽然可以处理任意长的序列，但它如何处理从一个序列到另一个序列的转换呢？比如说，如何用神经网络实现机器翻译——把英文句子转换成中文句子？</p>
<p>这两个句子可能有不同的长度，不同的结构，甚至在语义上可能有很大的差异。你不能简单地用一个固定大小的向量来表示一个任意长的句子。或者说，你可以，但你需要一个巧妙的方法来做到这一点。</p>
<p>这就是<strong>序列到序列</strong>（Sequence-to-Sequence，简称Seq2Seq）模型的想法。这个想法的核心是这样的：用一个RNN（称为<strong>编码器</strong>）来读取整个输入序列，逐词逐词地处理它，同时维持一个内部状态。这个内部状态在处理完整个输入序列后，就浓缩了输入序列中的所有信息。然后，你用另一个RNN（称为<strong>解码器</strong>）来生成输出序列，这个解码器以第一个RNN的最终状态作为初始化。</p>
<p>形式上，假设输入序列X有Tx个单词，输出序列Y有Ty个单词：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><msub><mi>T</mi><mi>x</mi></msub></msub><mo stretchy="false">}</mo><mo separator="true">,</mo><mspace width="1em"></mspace><mi>Y</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>y</mi><msub><mi>T</mi><mi>y</mi></msub></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">X = \{x_1, x_2, ..., x_{T_x}\}, \quad Y = \{y_1, y_2, ..., y_{T_y}\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0001em;vertical-align:-0.2501em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em"><span></span></span></span></span></span></span><span class="mclose">}</span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.22222em">Y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0973em;vertical-align:-0.3473em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3473em"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></p>
<p>编码器RNN逐步处理输入，计算隐藏状态序列：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>W</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>W</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">hh</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mord mathnormal mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<p>这里h_t是第t时刻的隐藏状态，它包含了从第一个词到第t个词的信息。处理完整个输入序列后，最后一个隐藏状态<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><msub><mi>T</mi><mi>x</mi></msub></msub></mrow><annotation encoding="application/x-tex">h_{T_x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9445em;vertical-align:-0.2501em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em"><span></span></span></span></span></span></span></span></span></span>被作为一个<strong>上下文向量</strong>c，传递给解码器：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mo>=</mo><msub><mi>h</mi><msub><mi>T</mi><mi>x</mi></msub></msub></mrow><annotation encoding="application/x-tex">c = h_{T_x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">c</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.9445em;vertical-align:-0.2501em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em"><span></span></span></span></span></span></span></span></span></span></p>
<p>注意这个上下文向量没有时间索引——它是一个固定的向量，包含了整个输入序列的信息。这个向量就像是编码器对&quot;我看到了什么&quot;的总结。</p>
<p>然后解码器开始工作。它维持自己的隐藏状态序列，这个序列的初始化包含上下文向量c。解码器的循环关系是：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><msub><mi>f</mi><mrow><mi>d</mi><mi>e</mi><mi>c</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mi>c</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">s_t = f_{dec}(s_{t-1}, y_{t-1}, c)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">ec</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">c</span><span class="mclose">)</span></span></span></span></p>
<p>其中s_t是解码器在第t时刻的隐藏状态，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">y_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span></span></span></span>是前一时刻生成的词，c是来自编码器的固定上下文向量。解码器用这个隐藏状态来生成下一个词的概率分布，然后通过采样或贪心选择来生成下一个词。</p>
<p>整个模型通过最大化对数似然来训练：</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi>Y</mi><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>y</mi></msub></msubsup><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>y</mi><mrow><mn>1</mn><mo>:</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log p(Y) = \sum_{t=1}^{T_y} \log p(y_t | y_{1:t-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em">Y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2868em;vertical-align:-0.2663em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0205em"><span style="top:-2.4337em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2421em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2663em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<p>这正是我们之前看到的自回归目标函数，现在应用到输出序列上。整个模型（包括编码器和解码器的所有权重）通过反向传播算法端到端地训练。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="从想法到实践">从想法到实践<a href="#从想法到实践" class="hash-link" aria-label="从想法到实践的直接链接" title="从想法到实践的直接链接">​</a></h3>
<p>这个想法听起来优雅而合理，但它的实现需要时间。2013年，Kalchbrenner和Blunsom发表了《递归连续翻译模型》（Recurrent Continuous Translation Models），这是第一篇提出完整编码器-解码器架构的论文。他们的编码器使用了卷积神经网络，而解码器使用了RNN。这是一个创新，但有一个明显的改进空间：为什么编码器不也用RNN呢？</p>
<p>第二年，2014年，Cho等人在《使用RNN编码器-解码器进行统计机器翻译的短语表示学习》中提出了一个更对称的架构：两个RNN，一个作为编码器，一个作为解码器。同年，Sutskever等人在《序列到序列学习与神经网络》中发表了类似的模型，但使用了LSTM而不是普通RNN，因为LSTM对处理长距离依赖更有效。</p>
<p>Sutskever在这篇论文中直接提到了Kalchbrenner，但指出了一个关键的改进：</p>
<blockquote>
<p>&quot;我们的工作与Kalchbrenner和Blunsom密切相关，他们是第一个将输入句子映射到向量再映射回句子的人，尽管他们使用卷积神经网络映射句子到向量，这会丢失词的顺序。&quot;</p>
</blockquote>
<p>这个简洁的历史陈述清晰地展现了这个领域的进化过程。前人走过的路直接导向了更好的方向。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="序列到序列的力量">序列到序列的力量<a href="#序列到序列的力量" class="hash-link" aria-label="序列到序列的力量的直接链接" title="序列到序列的力量的直接链接">​</a></h3>
<p>Seq2Seq模型的美妙之处在于它的通用性。一旦你有了这样一个框架，可以处理来自不同长度序列的映射，你就可以解决许多不同的问题。</p>
<p>机器翻译是最直观的应用。输入是一个英文句子，输出是对应的中文翻译。但这个框架同样适用于其他许多问题。文本摘要可以被框架化为：输入一长段文本，输出一个更短的摘要。对话系统可以被框架化为：输入一个用户的问题，输出一个相关的回答。问题回答、风格转换、甚至代码生成——所有这些都可以被视为序列到序列的映射。</p>
<p>这个框架的广泛适用性使得它成为了2014年到2016年间NLP领域的主流方法。许多学术论文、许多工业应用都建立在Seq2Seq的基础上。在这个时期，深度学习真正开始在NLP中占据主导地位，取代了之前基于特征工程的方法。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="瓶颈信息的丧失">瓶颈：信息的丧失<a href="#瓶颈信息的丧失" class="hash-link" aria-label="瓶颈：信息的丧失的直接链接" title="瓶颈：信息的丧失的直接链接">​</a></h3>
<p>想象一下，你需要把一个长的英文句子翻译成中文。句子有可能有20个词、30个词，甚至更长。编码器需要把所有这些信息压缩成一个单一的上下文向量c。</p>
<p>这里有个问题：<strong>信息压缩过程中必然会丧失信息</strong>。</p>
<p>考虑一个具体的例子。假设输入是&quot;The cat sat on the mat&quot;（猫坐在垫子上）。编码器逐词处理这个句子，最后的隐藏状态包含了什么？理论上，它应该&quot;记住&quot;所有的词。但实际上，由于RNN的顺序处理方式，最后的隐藏状态更多地编码的是句子的后面部分。前面的信息虽然会通过隐藏状态的递推传递下去，但会逐步被后面的词所影响和覆盖。</p>
<p>这个现象叫做<strong>长距离依赖问题</strong>（Long-range Dependency Problem）。当解码器试图生成翻译时，它是基于这个可能已经&quot;遗忘&quot;了句子开头的上下文向量。结果，长句子的翻译质量往往比短句子差得多。</p>
<p>这个问题在机器翻译中变成了一个著名的挑战。研究者们尝试了各种方法来改进，比如使用双向编码器（Bidirectional Encoder）来同时从前向后处理输入，或者改进隐藏状态的初始化。但根本问题仍然存在：一个固定大小的向量无法完美地编码任意长的序列。</p>
<p>2015年，Bahdanau、Cho和Bengio提出了一个优雅的解决方案：<strong>注意力机制</strong>（Attention Mechanism）。这个想法很简单但深刻：与其在解码时依赖一个单一的上下文向量，不如让解码器可以&quot;看到&quot;编码器的所有隐藏状态，并学习对不同的隐藏状态赋予不同的&quot;注意力&quot;权重。</p>
<p>当解码器生成每个输出词时，它会基于当前的解码状态计算一个权重向量，这个权重向量表示它应该&quot;关注&quot;编码器的哪些隐藏状态。然后，它计算编码器隐藏状态的加权平均作为该时刻的上下文。这样，解码器就可以动态地关注输入序列的不同部分。</p>
<p>注意力机制的出现标志着NLP进入了一个新的阶段。它不仅解决了序列到序列模型的瓶颈问题，更重要的是，它暗示了一个更深刻的真理：<strong>也许完整的语言建模不需要复杂的循环结构，而只需要一个能够在序列中&quot;看到&quot;远距离关系的机制。</strong></p>
<p>这个观察最终导向了我们故事中最关键的时刻——2017年的Transformer。</p>
<p>但在我们到达那里之前，有一个重要的中间步骤需要跨越。注意力机制虽然优雅，但它仍然嵌入在RNN的循环结构中。循环处理的本质没有改变——你仍然需要一个词一个词地处理序列。这意味着，即使有了注意力，模型仍然无法充分并行化。GPU的威力无法完全释放。</p>
<p>直到有人问了一个革命性的问题：<strong>我们真的需要循环结构吗？</strong></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="从序列到下一步">从序列到下一步<a href="#从序列到下一步" class="hash-link" aria-label="从序列到下一步的直接链接" title="从序列到下一步的直接链接">​</a></h3>
<p>到2016年中期，深度学习在NLP中的成功已经无可否认。RNN、LSTM和Seq2Seq模型已经成为主流。许多研究者正在改进这些模型，添加各种机制和优化。但在这个相对的繁荣中，也有一些研究者在思考更根本的问题。</p>
<p>其中一个问题是：如果注意力机制如此有效，它是否可以成为唯一的机制？如果我们有一个完全基于注意力的架构，而不是RNN加上注意力的附加品，会怎样？</p>
<p>另一个问题是：RNN的固有的顺序处理方式是否是必要的？能否设计一个架构，允许全面的并行化，同时仍然能够捕捉序列数据中的长距离依赖？</p>
<p>这两个问题的交点，就是Transformer的诞生地。</p>
<p>在我们继续这个故事之前，让我们停下来回顾一下我们走过的路。从Bengio的固定窗口模型，到Mikolov的Word2Vec破解了廉价词向量的秘密，再到RNN/LSTM展示了如何处理任意长的序列，再到Seq2Seq证明了我们可以用神经网络做复杂的序列到序列映射，最后到注意力机制开始改变我们对序列处理的认识。每一步都解决了前一步的某个问题，但也暴露了新的问题。这就是科学进步的方式——通过不断的迭代、改进和根本性的重新思考。</p>
<p>现在我们终于准备好理解Transformer了。当你看到Transformer的设计时，你会意识到，它不是凭空出现的。它是这十多年来积累的所有洞察、解决方案和技术的一个高潮。而最关键的是，它通过一个激进的想法——&quot;注意力就是你所需的一切&quot;——改变了我们对序列建模的根本理解。</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/edit/main/docs/cp3/序列的记忆.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件选项卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/廉价的智慧"><div class="pagination-nav__sublabel">上一页</div><div class="pagination-nav__label">廉价的智慧</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/cp3/Attention Is All You Need"><div class="pagination-nav__sublabel">下一页</div><div class="pagination-nav__label">Attention Is All You Need</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#固定窗口的局限" class="table-of-contents__link toc-highlight">固定窗口的局限</a></li><li><a href="#记忆中的循环" class="table-of-contents__link toc-highlight">记忆中的循环</a></li><li><a href="#从固定到可变序列到序列模型" class="table-of-contents__link toc-highlight">从固定到可变：序列到序列模型</a></li><li><a href="#从想法到实践" class="table-of-contents__link toc-highlight">从想法到实践</a></li><li><a href="#序列到序列的力量" class="table-of-contents__link toc-highlight">序列到序列的力量</a></li><li><a href="#瓶颈信息的丧失" class="table-of-contents__link toc-highlight">瓶颈：信息的丧失</a></li><li><a href="#从序列到下一步" class="table-of-contents__link toc-highlight">从序列到下一步</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Read</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/docs/intro">Introduction</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub Discussions<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/whirling-ai-consortium/The-Awakening-of-Intelligence-A-Chronicle-of-Large-Language-Models" target="_blank" rel="noopener noreferrer" class="footer__link-item" remarkplugins="" rehypeplugins="">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Whirling AI Consortium. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>